dlg_id,question,answer,fetched_contexts,fetched_answer,request_time,E:\dev\pycharm\quick-mind-doris\data\eval\gpt_fetched_contexts,E:\dev\pycharm\quick-mind-doris\data\eval\gpt_fetched_answer,E:\dev\pycharm\quick-mind-doris\data\eval\torchv_fetched_contexts,E:\dev\pycharm\quick-mind-doris\data\eval\torchv_fetched_answer
abb3a4e5-c174-11ee-9cff-b026288862a5,"想请教一下大佬们，如果我想新增一列时间列，这张表里任一字段的数据更新，这个时间列都会自动更新时间，Doris支持这种操作么？还是说，得在数据导入前给每列数据加一个最新时间数据再进行导入Doris,",2.1版本支持,"['文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CLEAN-LABEL](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CLEAN-LABEL)\n- 支持通过状态批量取消导入作业 文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD)\n- Broker Load 新增支持阿里云 OSS，腾讯 CHDFS 和华为云 OBS。 文档：[https://doris.apache.org/zh-CN/docs/dev/advanced/broker](https://doris.apache.org/zh-CN/docs/dev/advanced/broker) - 支持通过 hive-site.xml 文件配置访问 HDFS。\n文档：[https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir) ### 5. 支持通过 `SHOW CATALOG RECYCLE BIN` 功能查看回收站中的内容。\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN) ### 6. 支持 `SELECT * EXCEPT` 语法。\n文档：[https://doris.apache.org/zh-CN/docs/dev/data-table/basic-usage](https://doris.apache.org/zh-CN/docs/dev/data-table/basic-usage) ### 7. OUTFILE 支持 ORC 格式导出，并且支持多字节分隔符。\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE)\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE) ### 8. 支持通过配置修改可保存的 Query Profile 的数量。\n文档搜索 FE 配置项：`max_query_profile_num` ### 9. DELETE 语句支持 IN 谓词条件。并且支持分区裁剪。\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/DELETE](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/DELETE)\n### 10. 时间列的默认值支持使用 `CURRENT_TIMESTAMP`', '--database db1\\ --postgres-conf hostname=127.0.0.1 \\ --postgres-conf port=5432 \\ --postgres-conf username=postgres \\ --postgres-conf password=""123456"" \\ --postgres-conf database-name=postgres \\\n--postgres-conf schema-name=public \\ --postgres-conf slot.name=test \\ --postgres-conf decoding.plugin.name=pgoutput \\ --including-tables ""tbl1|tbl2"" \\ --sink-conf fenodes=127.0.0.1:8030 \\ --sink-conf username=root \\\n--sink-conf password=\\ --sink-conf jdbc-url=jdbc:mysql://127.0.0.1:9030 \\ --sink-conf sink.label-prefix=label \\ --table-conf replication_num=1\n``` ### SQLServer多表同步示例\n```shell <FLINK_HOME>/bin/flink run \\ -Dexecution.checkpointing.interval=10s \\ -Dparallelism.default=1 \\ -c org.apache.doris.flink.tools.cdc.CdcTools \\ ./lib/flink-doris-connector-1.16-1.5.0-SNAPSHOT.jar \\\nsqlserver-sync-database \\ --database db1\\ --sqlserver-conf hostname=127.0.0.1 \\ --sqlserver-conf port=1433 \\ --sqlserver-conf username=sa \\ --sqlserver-conf password=""123456"" \\ --sqlserver-conf database-name=CDC_DB \\\n--sqlserver-conf schema-name=dbo \\ --including-tables ""tbl1|tbl2"" \\ --sink-conf fenodes=127.0.0.1:8030 \\ --sink-conf username=root \\ --sink-conf password=\\ --sink-conf jdbc-url=jdbc:mysql://127.0.0.1:9030 \\\n--sink-conf sink.label-prefix=label \\ --table-conf replication_num=1\n``` ## 使用FlinkCDC更新Key列\n一般在业务数据库中，会使用编号来作为表的主键，比如Student表，会使用编号(id)来作为主键，但是随着业务的发展，数据对应的编号有可能是会发生变化的。 在这种场景下，使用FlinkCDC + Doris Connector同步数据，便可以自动更新Doris主键列的数据。 ### 原理 Flink CDC底层的采集工具是Debezium，Debezium内部使用op字段来标识对应的操作：op字段的取值分别为c、u、d、r，分别对应create、update、delete和read。\n而对于主键列的更新，FlinkCDC会向下游发送DELETE和INSERT事件，同时数据同步到Doris中后，就会自动更新主键列的数据。', '2. `Status` 状态为 `Label Already Exists`。则此时需继续查看 `ExistingJobStatus` 字段。如果该字段值为 `FINISHED`，则表示这个 Label 对应的导入任务已经成功，无需在重试。如果为 `RUNNING`，则表示这个 Label 对应的导入任务依然在运行，则此时需每间隔一段时间（如10秒），使用相同的 Label 继续重复提交，直到 `Status` 不为 `Label Already Exists`，或者\n`ExistingJobStatus` 字段值为 `FINISHED` 为止。\n3. 取消导入任务 已提交切尚未结束的导入任务可以通过 CANCEL LOAD 命令取消。取消后，已写入的数据也会回滚，不会生效。 4. Label、导入事务、多表原子性\nDoris 中所有导入任务都是原子生效的。并且在同一个导入任务中对多张表的导入也能够保证原子性。同时，Doris 还可以通过 Label 的机制来保证数据导入的不丢不重。具体说明可以参阅 [导入事务和原子性](../../../../data-operate/import/import-scenes/load-atomicity.md) 文档。 5. 列映射、衍生列和过滤\nDoris 可以在导入语句中支持非常丰富的列转换和过滤操作。支持绝大多数内置函数和 UDF。关于如何正确的使用这个功能，可参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。 6. 错误数据过滤\nDoris 的导入任务可以容忍一部分格式错误的数据。容忍率通过 `max_filter_ratio` 设置。默认为0，即表示当有一条错误数据时，整个导入任务将会失败。如果用户希望忽略部分有问题的数据行，可以将次参数设置为 0~1 之间的数值，Doris 会自动跳过哪些数据格式不正确的行。 关于容忍率的一些计算方式，可以参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。\n7. 严格模式 `strict_mode` 属性用于设置导入任务是否运行在严格模式下。该属性会对列映射、转换和过滤的结果产生影响，它同时也将控制部分列更新的行为。关于严格模式的具体说明，可参阅 [严格模式](../../../../data-operate/import/import-scenes/load-strict-mode.md) 文档。 8. 超时时间 Stream Load 的默认超时时间为 10 分钟。从任务提交开始算起。如果在超时时间内没有完成，则任务会失败。\n9. 数据量和任务数限制 Stream Load 适合导入几个GB以内的数据，因为数据为单线程传输处理，因此导入过大的数据性能得不到保证。当有大量本地数据需要导入时，可以并行提交多个导入任务。 Doris 同时会限制集群内同时运行的导入任务数量，通常在 10-20 个不等。之后提交的导入作业会被拒绝。 10. 涉及时区的导入\n由于 Doris 目前没有内置时区的时间类型，所有 `DATETIME` 相关类型均只表示绝对的时间点，而不包含时区信息，不因 Doris 系统时区变化而发生变化。因此，对于带时区数据的导入，我们统一的处理方式为**将其转换为特定目标时区下的数据**。在 Doris 系统中，即 session variable `time_zone` 所代表的时区。\n而在导入中，我们的目标时区通过参数 `timezone` 指定，该变量在发生时区转换、运算时区敏感函数时将会替代 session variable `time_zone`。因此，如果没有特殊情况，在导入事务中应当设定 `timezone` 与当前 Doris 集群的 `time_zone` 一致。此时意味着所有带时区的时间数据，均会发生向该时区的转换。\n例如，Doris 系统时区为 ""+08:00""，导入数据中的时间列包含两条数据，分别为 ""2012-01-01 01:00:00Z"" 和 ""2015-12-12 12:12:12-08:00""，则我们在导入时通过 `-H ""timezone: +08:00""` 指定导入事务的时区后，这两条数据都会向该时区发生转换，从而得到结果 ""2012-01-01 09:00:00"" 和 ""2015-12-13 04:12:12""。']","Doris 目前不支持在数据更新时自动更新时间列的功能。也就是说，Doris 并不像某些数据库那样，能够自动将某一列设置为“最后更新时间戳”，并在每次数据变更时自动更新这个时间戳。

如果您希望实现这样的功能，您需要在数据导入到 Doris 之前，手动为您的数据添加这样一个时间戳字段。这意味着，在数据更新操作发生时（或者在数据准备阶段），您需要在应用层或数据处理层手动插入或更新这个时间戳字段，以确保这个时间列能够准确反映数据的最后更新时间。

简而言之，要实现您描述的需求，需要在数据导入到 Doris 前，在数据源端处理好时间戳信息，然后再将数据导入 Doris。",50.34,"['文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CLEAN-LABEL](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CLEAN-LABEL)\n- 支持通过状态批量取消导入作业 文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD)\n- Broker Load 新增支持阿里云 OSS，腾讯 CHDFS 和华为云 OBS。 文档：[https://doris.apache.org/zh-CN/docs/dev/advanced/broker](https://doris.apache.org/zh-CN/docs/dev/advanced/broker) - 支持通过 hive-site.xml 文件配置访问 HDFS。\n文档：[https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir) ### 5. 支持通过 `SHOW CATALOG RECYCLE BIN` 功能查看回收站中的内容。\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN) ### 6. 支持 `SELECT * EXCEPT` 语法。\n文档：[https://doris.apache.org/zh-CN/docs/dev/data-table/basic-usage](https://doris.apache.org/zh-CN/docs/dev/data-table/basic-usage) ### 7. OUTFILE 支持 ORC 格式导出，并且支持多字节分隔符。\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE)\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE) ### 8. 支持通过配置修改可保存的 Query Profile 的数量。\n文档搜索 FE 配置项：`max_query_profile_num` ### 9. DELETE 语句支持 IN 谓词条件。并且支持分区裁剪。\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/DELETE](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/DELETE)\n### 10. 时间列的默认值支持使用 `CURRENT_TIMESTAMP`', '--database db1\\ --postgres-conf hostname=127.0.0.1 \\ --postgres-conf port=5432 \\ --postgres-conf username=postgres \\ --postgres-conf password=""123456"" \\ --postgres-conf database-name=postgres \\\n--postgres-conf schema-name=public \\ --postgres-conf slot.name=test \\ --postgres-conf decoding.plugin.name=pgoutput \\ --including-tables ""tbl1|tbl2"" \\ --sink-conf fenodes=127.0.0.1:8030 \\ --sink-conf username=root \\\n--sink-conf password=\\ --sink-conf jdbc-url=jdbc:mysql://127.0.0.1:9030 \\ --sink-conf sink.label-prefix=label \\ --table-conf replication_num=1\n``` ### SQLServer多表同步示例\n```shell <FLINK_HOME>/bin/flink run \\ -Dexecution.checkpointing.interval=10s \\ -Dparallelism.default=1 \\ -c org.apache.doris.flink.tools.cdc.CdcTools \\ ./lib/flink-doris-connector-1.16-1.5.0-SNAPSHOT.jar \\\nsqlserver-sync-database \\ --database db1\\ --sqlserver-conf hostname=127.0.0.1 \\ --sqlserver-conf port=1433 \\ --sqlserver-conf username=sa \\ --sqlserver-conf password=""123456"" \\ --sqlserver-conf database-name=CDC_DB \\\n--sqlserver-conf schema-name=dbo \\ --including-tables ""tbl1|tbl2"" \\ --sink-conf fenodes=127.0.0.1:8030 \\ --sink-conf username=root \\ --sink-conf password=\\ --sink-conf jdbc-url=jdbc:mysql://127.0.0.1:9030 \\\n--sink-conf sink.label-prefix=label \\ --table-conf replication_num=1\n``` ## 使用FlinkCDC更新Key列\n一般在业务数据库中，会使用编号来作为表的主键，比如Student表，会使用编号(id)来作为主键，但是随着业务的发展，数据对应的编号有可能是会发生变化的。 在这种场景下，使用FlinkCDC + Doris Connector同步数据，便可以自动更新Doris主键列的数据。 ### 原理 Flink CDC底层的采集工具是Debezium，Debezium内部使用op字段来标识对应的操作：op字段的取值分别为c、u、d、r，分别对应create、update、delete和read。\n而对于主键列的更新，FlinkCDC会向下游发送DELETE和INSERT事件，同时数据同步到Doris中后，就会自动更新主键列的数据。', '2. `Status` 状态为 `Label Already Exists`。则此时需继续查看 `ExistingJobStatus` 字段。如果该字段值为 `FINISHED`，则表示这个 Label 对应的导入任务已经成功，无需在重试。如果为 `RUNNING`，则表示这个 Label 对应的导入任务依然在运行，则此时需每间隔一段时间（如10秒），使用相同的 Label 继续重复提交，直到 `Status` 不为 `Label Already Exists`，或者\n`ExistingJobStatus` 字段值为 `FINISHED` 为止。\n3. 取消导入任务 已提交切尚未结束的导入任务可以通过 CANCEL LOAD 命令取消。取消后，已写入的数据也会回滚，不会生效。 4. Label、导入事务、多表原子性\nDoris 中所有导入任务都是原子生效的。并且在同一个导入任务中对多张表的导入也能够保证原子性。同时，Doris 还可以通过 Label 的机制来保证数据导入的不丢不重。具体说明可以参阅 [导入事务和原子性](../../../../data-operate/import/import-scenes/load-atomicity.md) 文档。 5. 列映射、衍生列和过滤\nDoris 可以在导入语句中支持非常丰富的列转换和过滤操作。支持绝大多数内置函数和 UDF。关于如何正确的使用这个功能，可参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。 6. 错误数据过滤\nDoris 的导入任务可以容忍一部分格式错误的数据。容忍率通过 `max_filter_ratio` 设置。默认为0，即表示当有一条错误数据时，整个导入任务将会失败。如果用户希望忽略部分有问题的数据行，可以将次参数设置为 0~1 之间的数值，Doris 会自动跳过哪些数据格式不正确的行。 关于容忍率的一些计算方式，可以参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。\n7. 严格模式 `strict_mode` 属性用于设置导入任务是否运行在严格模式下。该属性会对列映射、转换和过滤的结果产生影响，它同时也将控制部分列更新的行为。关于严格模式的具体说明，可参阅 [严格模式](../../../../data-operate/import/import-scenes/load-strict-mode.md) 文档。 8. 超时时间 Stream Load 的默认超时时间为 10 分钟。从任务提交开始算起。如果在超时时间内没有完成，则任务会失败。\n9. 数据量和任务数限制 Stream Load 适合导入几个GB以内的数据，因为数据为单线程传输处理，因此导入过大的数据性能得不到保证。当有大量本地数据需要导入时，可以并行提交多个导入任务。 Doris 同时会限制集群内同时运行的导入任务数量，通常在 10-20 个不等。之后提交的导入作业会被拒绝。 10. 涉及时区的导入\n由于 Doris 目前没有内置时区的时间类型，所有 `DATETIME` 相关类型均只表示绝对的时间点，而不包含时区信息，不因 Doris 系统时区变化而发生变化。因此，对于带时区数据的导入，我们统一的处理方式为**将其转换为特定目标时区下的数据**。在 Doris 系统中，即 session variable `time_zone` 所代表的时区。\n而在导入中，我们的目标时区通过参数 `timezone` 指定，该变量在发生时区转换、运算时区敏感函数时将会替代 session variable `time_zone`。因此，如果没有特殊情况，在导入事务中应当设定 `timezone` 与当前 Doris 集群的 `time_zone` 一致。此时意味着所有带时区的时间数据，均会发生向该时区的转换。\n例如，Doris 系统时区为 ""+08:00""，导入数据中的时间列包含两条数据，分别为 ""2012-01-01 01:00:00Z"" 和 ""2015-12-12 12:12:12-08:00""，则我们在导入时通过 `-H ""timezone: +08:00""` 指定导入事务的时区后，这两条数据都会向该时区发生转换，从而得到结果 ""2012-01-01 09:00:00"" 和 ""2015-12-13 04:12:12""。']","Doris 支持使用 `CURRENT_TIMESTAMP` 作为时间列的默认值，这可以在创建表时或添加列时为新记录自动填充当前时间。然而，这种方法仅适用于新插入的数据，对于已存在数据的更新操作，`CURRENT_TIMESTAMP` 并不会自动更新该时间列到当前时间。

如果您的需求是在任一字段的数据更新时，都要自动更新时间列为当前时间，Doris 原生并不直接支持这种“触发器”式的自动更新功能。在这种情况下，您需要在数据更新操作中显式地设置时间列的值为当前时间。例如，在更新数据时，通过在 SQL 更新语句中包含设置时间列为 `CURRENT_TIMESTAMP` 的逻辑。

因此，对于已有数据的更新导致时间列也需更新的场景，您需要在数据导入或更新前，手动为该时间列赋予最新的时间值，然后再进行数据的导入或更新操作。这可能意味着在数据更新的应用逻辑中，需要额外添加处理逻辑来确保时间列的值能够反映最新的更新时间。","['- 实时的动态列更新，需要在表中实时的高频更新某些字段值。例如T+1生成的用户标签表中有一些关于用户最新行为信息的字段需要实时的更新，以实现广告/推荐等系统能够据其进行实时的分析和决策\n- 将多张源表拼接成一张大宽表\n- 数据修正\n\n## 基本原理\n\n关于Unique Key模型和Aggregate Key模型的原理，可以主要参考[数据模型](../../data-table/data-model.md)的介绍\n\n### Unique Key 模型\n\n**Unique Key模型目前仅支持在Merge-on-Write实现上进行列更新**\n\n用户通过正常的导入方式将一部分列的数据写入Doris的Memtable，此时Memtable中并没有整行数据，在Memtable下刷的时候，会查找历史数据，用历史数据补齐一整行，并写入数据文件中，同时将历史数据文件中相同key的数据行标记删除\n\n当出现并发导入时，Doris会利用MVCC机制来保证数据的正确性。如果两批数据导入都更新了一个相同key的不同列，则其中系统版本较高的导入任务会在版本较低的导入任务成功后，使用版本较低的导入任务写入的相同key的数据行重新进行补齐\n\n### Aggregate Key模型\n\n将聚合函数设置为`REPLACE_IF_NOT_NULL`即可实现部分列更新的支持，详细用法参考下文示例\n\n## 并发写入和数据可见性\n\n部分列更新支持高频的并发写入，写入成功后数据即可见，系统自动通过MVCC机制来保证并发写入的数据正确性\n\n## 性能\n\n使用建议：\n\n1. 对写入性能要求较高，查询性能要求较低的用户，建议使用Aggregate Key模型\n2. 对查询性能要求较高，对写入性能要求不高（例如数据的写入和更新基本都在凌晨低峰期完成），或者写入频率不高的用户，建议使用Unique Key模型merge-on-write实现\n\n### Unique Key模型Merge-on-Write实现\n\n由于Merge-on-Write实现需要在数据写入的时候，进行整行数据的补齐，以保证最优的查询性能，因此使用Merge-on-Write实现进行部分列更新会有较为明显的导入性能下降。\n\n写入性能优化建议：', '此时就可以替换表中原有的数据。综上，在导入过程中，会比较所有批次的sequence列值，选择值最大的记录导入Doris表中。\n\n## 注意\n1. 为防止误用，在StreamLoad/BrokerLoad等导入任务以及行更新insert语句中，用户必须显示指定sequence列(除非sequence列的默认值为CURRENT_TIMESTAMP)，不然会收到以下报错信息：\n```\nTable test_tbl has sequence column, need to specify the sequence column\n```\n2. 自版本2.0起，Doris对Unique Key表的Merge-on-Write实现支持了部分列更新能力，在部分列更新导入中，用户每次可以只更新一部分列，因此并不是必须要包含sequence列。若用户提交的导入任务中，包含sequence列，则行为无影响；若用户提交的导入任务不包含sequence列，Doris会使用匹配的历史数据中的sequence列作为更新后该行的sequence列的值。如果历史数据中不存在相同key的列，则会自动用null或默认值填充。 ---\n{\n    ""title"": ""NAMED_STRUCT"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## named_struct\n\n<version since=""2.0.0"">\n\nnamed_struct\n\n</version>\n\n### description\n\n#### Syntax\n\n`STRUCT<T1, T2, T3, ...> named_struct({VARCHAR, T1}, {VARCHAR, T2}, ...)`\n\n根据给定的字符串和值构造并返回struct\n\n参数个数必须为非0偶数，奇数位是field的名字，必须为常量字符串，偶数位是field的值，可以是多列或常量\n\n### notice\n\n`仅支持向量化引擎中使用`\n\n### example', '## 使用FlinkCDC更新Key列\n\n一般在业务数据库中，会使用编号来作为表的主键，比如Student表，会使用编号(id)来作为主键，但是随着业务的发展，数据对应的编号有可能是会发生变化的。\n在这种场景下，使用FlinkCDC + Doris Connector同步数据，便可以自动更新Doris主键列的数据。\n### 原理\nFlink CDC底层的采集工具是Debezium，Debezium内部使用op字段来标识对应的操作：op字段的取值分别为c、u、d、r，分别对应create、update、delete和read。\n而对于主键列的更新，FlinkCDC会向下游发送DELETE和INSERT事件，同时数据同步到Doris中后，就会自动更新主键列的数据。\n\n### 使用\nFlink程序可参考上面CDC同步的示例，成功提交任务后，在MySQL侧执行Update主键列的语句(`update  student set id = \'1002\' where id = \'1001\'`)，即可修改Doris中的数据。\n\n## 使用Flink根据指定列删除数据\n\n一般Kafka中的消息会使用特定字段来标记操作类型，比如{""op_type"":""delete"",{...}}。针对这类数据，希望将op_type=delete的数据删除掉。\n\nDorisSink默认会根据RowKind来区分事件的类型，通常这种在cdc情况下可以直接获取到事件类型，对隐藏列`__DORIS_DELETE_SIGN__`进行赋值达到删除的目的，而Kafka则需要根据业务逻辑判断，显示的传入隐藏列的值。\n\n### 使用\n\n```sql\n-- 比如上游数据: {""op_type"":""delete"",{""id"":1,""name"":""zhangsan""}}\nCREATE TABLE KAFKA_SOURCE(\n  data STRING,\n  op_type STRING\n) WITH (\n  \'connector\' = \'kafka\',\n  ...\n);', '8. 超时时间\n\n   Stream Load 的默认超时时间为 10 分钟。从任务提交开始算起。如果在超时时间内没有完成，则任务会失败。\n\n9. 数据量和任务数限制\n\n   Stream Load 适合导入几个GB以内的数据，因为数据为单线程传输处理，因此导入过大的数据性能得不到保证。当有大量本地数据需要导入时，可以并行提交多个导入任务。\n\n   Doris 同时会限制集群内同时运行的导入任务数量，通常在 10-20 个不等。之后提交的导入作业会被拒绝。\n\n10. 涉及时区的导入\n\n    由于 Doris 目前没有内置时区的时间类型，所有 `DATETIME` 相关类型均只表示绝对的时间点，而不包含时区信息，不因 Doris 系统时区变化而发生变化。因此，对于带时区数据的导入，我们统一的处理方式为**将其转换为特定目标时区下的数据**。在 Doris 系统中，即 session variable `time_zone` 所代表的时区。\n\n    而在导入中，我们的目标时区通过参数 `timezone` 指定，该变量在发生时区转换、运算时区敏感函数时将会替代 session variable `time_zone`。因此，如果没有特殊情况，在导入事务中应当设定 `timezone` 与当前 Doris 集群的 `time_zone` 一致。此时意味着所有带时区的时间数据，均会发生向该时区的转换。\n    例如，Doris 系统时区为 ""+08:00""，导入数据中的时间列包含两条数据，分别为 ""2012-01-01 01:00:00Z"" 和 ""2015-12-12 12:12:12-08:00""，则我们在导入时通过 `-H ""timezone: +08:00""` 指定导入事务的时区后，这两条数据都会向该时区发生转换，从而得到结果 ""2012-01-01 09:00:00"" 和 ""2015-12-13 04:12:12""。\n\n    更详细的理解，请参阅[时区](../../../../advanced/time-zone)文档。\n---\n{\n    ""title"": ""MYSQL-LOAD"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## MYSQL-LOAD', ""* `default_value`\n        列默认值，当导入数据未指定该列的值时，系统将赋予该列default_value。\n          \n        语法为`default default_value`。\n          \n        当前default_value支持两种形式：\n        1. 用户指定固定值，如：\n        ```SQL\n            k1 INT DEFAULT '1',\n            k2 CHAR(10) DEFAULT 'aaaa'\n        ```\n        2. 系统提供的关键字，目前支持以下关键字：\n          \n        ```SQL\n            // 只用于DATETIME类型，导入数据缺失该值时系统将赋予当前时间\n            dt DATETIME DEFAULT CURRENT_TIMESTAMP\n        ```\n  * `on update current_timestamp`\n\n        是否在该行有列更新时将该列的值更新为当前时间(`current_timestamp`)。该特性只能在开启了merge-on-write的unique表上使用，开启了这个特性的列必须声明默认值，且默认值必须为`current_timestamp`。如果此处声明了时间戳的精度，则该列默认值中的时间戳精度必须与该处的时间戳精度相同。"", 'Parameters\n\n> tablet_name: 导入数据的目的表。可以是 `db_name.table_name` 形式\n>\n> partitions: 指定待导入的分区，必须是 `table_name` 中存在的分区，多个分区名称用逗号分隔\n>\n> label: 为 Insert 任务指定一个 label\n>\n> column_name: 指定的目的列，必须是 `table_name` 中存在的列\n>\n> expression: 需要赋值给某个列的对应表达式\n>\n> DEFAULT: 让对应列使用默认值\n>\n> query: 一个普通查询，查询的结果会写入到目标中\n>\n> hint: 用于指示 `INSERT` 执行行为的一些指示符。目前 hint 有三个可选值`/*+ STREAMING */`、`/*+ SHUFFLE */`或`/*+ NOSHUFFLE */`\n> 1. STREAMING：目前无实际作用，只是为了兼容之前的版本，因此保留。（之前的版本加上这个 hint 会返回 label，现在默认都会返回 label）\n> 2. SHUFFLE：当目标表是分区表，开启这个 hint 会进行 repartiiton。\n> 3. NOSHUFFLE：即使目标表是分区表，也不会进行 repartiiton，但会做一些其他操作以保证数据正确落到各个分区中。\n\n对于开启了merge-on-write的Unique表，还可以使用insert语句进行部分列更新的操作。要使用insert语句进行部分列更新，需要将会话变量enable_unique_key_partial_update的值设置为true(该变量默认值为false，即默认无法通过insert语句进行部分列更新)。进行部分列更新时，插入的列必须至少包含所有的Key列，同时指定需要更新的列。如果插入行Key列的值在原表中存在，则将更新具有相同key列值那一行的数据。如果插入行Key列的值在原表中不存在，则将向表中插入一条新的数据，此时insert语句中没有指定的列必须有默认值或可以为null，这些缺失列会首先尝试用默认值填充，如果该列没有默认值，则尝试使用null值填充，如果该列不能为null，则本次插入失败。']","Doris支持在开启了Merge-on-Write的Unique Key模型上，实现当任一字段更新时自动更新指定时间列的功能。这可以通过在表定义中使用`ON UPDATE CURRENT_TIMESTAMP`属性来实现。当这个属性被添加到时间列上时，每当该行数据有任何更新时，这个时间列的值都会被自动设置为当前时间。

你不需要在数据导入前给每列数据手动加上最新的时间数据。下面是如何定义这种时间列的一个例子：

```sql
CREATE TABLE example (
    `id` INT NOT NULL,
    `data` VARCHAR NOT NULL,
    `update_time` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
) ENGINE=OLAP
UNIQUE KEY(`id`)
...
```

在这个例子中，`update_time`列在创建表的时候被指定了`ON UPDATE CURRENT_TIMESTAMP`属性。这意味着每当通过Merge-on-Write方式更新`id`对应行的任何其他字段时，`update_time`列都会自动设置为更新操作发生时的当前时间。

确保你的表使用的是Unique Key模型，并且启用了Merge-on-Write，这样这个功能才能正常工作。如果你的表不是按照这种方式创建的，你可能需要修改表结构以满足这个需求。"
b7c09039-c195-11ee-ad2f-b026288862a5,"如何在Apache Doris中实现节点缩容？
",在Apache Doris中，如果在一个节点上部署了多个BE（Backend Engine），并且想要缩减节点数量，需要先通过DECOMMISSION方式手动将该节点上的BE服务停止并下线，然后再进行节点的缩容操作。,"['因为 Doris 的元数据采用多数写策略，即一条元数据 journal 必须至少写入多数个 FOLLOWER FE 后（比如 3 个 FOLLOWER，必须写成功 2 个），才算成功。而如果写入失败，FE 进程会主动退出。那么假设有 A、B、C 三个 FOLLOWER，C 先挂掉，然后 B 再挂掉，那么 A 也会跟着挂掉。所以如 `最佳实践` 一节中所述，如果你没有丰富的元数据运维经验，不建议部署多 FOLLOWER。\n6. fe.log 中出现 `get exception when try to close previously opened bdb database. ignore it` 如果后面有 `ignore it` 字样，通常无需处理。如果你有兴趣，可以在 `BDBEnvironment.java` 搜索这个错误，查看相关注释说明。 7. 从 `show frontends;` 看，某个 FE 的 `Join` 列为 `true`，但是实际该 FE 不正常\n通过 `show frontends;` 查看到的 `Join` 信息。该列如果为 `true`，仅表示这个 FE **曾经加入过** 集群。并不能表示当前仍然正常的存在于集群中。如果为 `false`，则表示这个 FE **从未加入过** 集群。 8. 关于 FE 的配置 `master_sync_policy`, `replica_sync_policy` 和 `txn_rollback_limit`\n`master_sync_policy` 用于指定当 Leader FE 写元数据日志时，是否调用 fsync(), `replica_sync_policy` 用于指定当 FE HA 部署时，其他 Follower FE 在同步元数据时，是否调用 fsync()。在早期的 Doris 版本中，这两个参数默认是 `WRITE_NO_SYNC`，即都不调用 fsync()。在最新版本的 Doris 中，默认已修改为 `SYNC`，即都调用 fsync()。调用 fsync()\n会显著降低元数据写盘的效率。在某些环境下，IOPS 可能降至几百，延迟增加到2-3ms（但对于 Doris 元数据操作依然够用）。因此我们建议以下配置：\n1. 对于单 Follower FE 部署，`master_sync_policy` 设置为 `SYNC`，防止 FE 系统宕机导致元数据丢失。 2. 对于多 Follower FE 部署，可以将 `master_sync_policy` 和 `replica_sync_policy` 设为 `WRITE_NO_SYNC`，因为我们认为多个系统同时宕机的概率非常低。\n如果在单 Follower FE 部署中，`master_sync_policy` 设置为 `WRITE_NO_SYNC`，则可能出现 FE 系统宕机导致元数据丢失。这时如果有其他 Observer FE 尝试重启时，可能会报错：\n``` Node xxx must rollback xx total commits(numPassedDurableCommits of which were durable) to the earliest point indicated by transaction xxxx in order to rejoin the replication group, but the transaction rollback limit of xxx prohibits this.\n``` 意思有部分已经持久化的事务需要回滚，但条数超过上限。这里我们的默认上限是 100，可以通过设置 `txn_rollback_limit` 改变。该操作仅用于尝试正常启动 FE，但已丢失的元数据无法恢复。 --- { ""title"": ""弹性扩缩容"", ""language"": ""zh-CN"" } --- <!--split--> # 扩容缩容 Doris 可以很方便的扩容和缩容 FE、BE、Broker 实例。 ## FE 扩容和缩容\n可以通过将 FE 扩容至 3 个以上节点来实现 FE 的高可用。 用户可以通过 mysql 客户端登陆 Master FE。通过: `SHOW PROC \'/frontends\';` 来查看当前 FE 的节点情况。 也可以通过前端页面连接：```http://fe_hostname:fe_http_port/frontend``` 或者 ```http://fe_hostname:fe_http_port/system?path=//frontends``` 来查看 FE 节点的情况。\n以上方式，都需要 Doris 的 root 用户权限。 FE 节点的扩容和缩容过程，不影响当前系统运行。 ### 增加 FE 节点', 'Traval。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hudi](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hudi)\n- JDBC Catalog 新增支持 Oceanbase，目前支持包括 MySQL、PostgreSQL、Oracle、SQLServer、Doris、Clickhouse、SAP HANA、Trino/Presto、Oceanbase\n等近十种关系型数据库。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/jdbc](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/jdbc)\n**数据权限管控** - 支持通过 Apache Range 对 Hive Catalog 进行鉴权，可以无缝对接用户现有的权限系统。同时还支持可扩展的鉴权插件，为任意 Catalog 实现自定义的鉴权方式。 参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hive](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hive)\n**性能进一步优化，最高提升数十倍** - 优化了大量小文件场景以及宽表场景的读取性能。通过小文件全量加载、小 IO 合并、数据预读等技术，显著降低远端存储的读取开销，在此类场景下，查询性能最高提升数十倍。 - 优化了 ORC/Parquet 文件的读取性能，相较于 1.2 版本查询性能提升一倍。 ![](/images/release-note-2.0beta-ssb-parquet.png) ![](/images/release-note-2.0beta-ssb-orc.png)\n- 支持湖上数据的本地文件缓存。可以利用本地磁盘缓存 HDFS 或对象存储等远端存储系统上的数据，通过缓存加速访问相同数据的查询。在命中本地文件缓存的情况下，通过 Apache Doris 查询湖上数据的性能可与 Apache Doris\n内部表持平，该功能可以极大提升湖上热数据的查询性能。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache](https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache)\n- 支持外表的统计信息收集。和 Apache Doris 内表一样，用户可以通过 Analyze 语句分析并收集指定外表的统计信息，结合 Nereids 全新查询优化器，能够更准确更智能地对复杂 SQL 进行查询计划的调优。以 TPC-H 标准测试数据集为例，无需手动改写 SQL 即可获得最优的查询计划并获得更好的性能表现。\n参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/)\n- 优化了 JDBC Catalog 的数据写回性能。通过 PrepareStmt 和批量方式，用户通过 INSERT INTO 命令、通过 JDBC Catalog 将数据写回到 MySQL、Oracle 等关系型数据库的性能提升数十倍。 ![](/images/release-note-2.0beta-jdbc.png) # 支持 Kubernetes 容器化部署\n在过去 Apache Doris 是基于 IP 通信的，在 K8s 环境部署时由于宿主机故障发生 Pod IP 漂移将导致集群不可用，在 2.0 版本中我们支持了 FQDN，使得 Apache Doris 可以在无需人工干预的情况下实现节点自愈，因此可以更好应对 K8s 环境部署以及灵活扩缩容。', '第一个启动的 FE 自动成为 Master。在此基础上，可以添加若干 Follower 和 Observer。 #### 配置及启动 Follower 或 Observer 这里 Follower 和 Observer 的配置同 Master 的配置。 首先第一次启动时，需执行以下命令： `./bin/start_fe.sh --helper leader_fe_host:edit_log_port --daemon`\n其中 leader\\_fe\\_host 为 Master 所在节点 ip, edit\\_log\\_port 在 Master 的配置文件 fe.conf 中。--helper 参数仅在 follower 和 observer 第一次启动时才需要。 #### 将 Follower 或 Observer 加入到集群 添加 Follower 或 Observer。使用 mysql-client 连接到已启动的 FE，并执行：\n`ALTER SYSTEM ADD FOLLOWER ""follower_host:edit_log_port"";` 或 `ALTER SYSTEM ADD OBSERVER ""observer_host:edit_log_port"";` 其中 follower\\_host和observer\\_host 为 Follower 或 Observer 所在节点 ip，edit\\_log\\_port 在其配置文件 fe.conf 中。\n查看 Follower 或 Observer 运行状态。使用 mysql-client 连接到任一已启动的 FE，并执行：SHOW PROC \'/frontends\'; 可以查看当前已加入集群的 FE 及其对应角色。\n> FE 扩容注意事项： > 1. Follower FE（包括 Master）的数量必须为奇数，建议最多部署 3 个组成高可用（HA）模式即可。 > 2. 当 FE 处于高可用部署时（1个 Master，2个 Follower），我们建议通过增加 Observer FE 来扩展 FE 的读服务能力。当然也可以继续增加 Follower FE，但几乎是不必要的。\n> 3. 通常一个 FE 节点可以应对 10-20 台 BE 节点。建议总的 FE 节点数量在 10 个以下。而通常 3 个即可满足绝大部分需求。 > 4. helper 不能指向 FE 自身，必须指向一个或多个已存在并且正常运行中的 Master/Follower FE。\n### 删除 FE 节点 使用以下命令删除对应的 FE 节点： ```ALTER SYSTEM DROP FOLLOWER[OBSERVER] ""fe_host:edit_log_port"";\n``` > FE 缩容注意事项： > 1. 删除 Follower FE 时，确保最终剩余的 Follower（包括 Master）节点为奇数。 ## BE 扩容和缩容 用户可以通过 mysql-client 登陆 Master FE。通过: ```SHOW PROC \'/backends\';\n``` 来查看当前 BE 的节点情况。 也可以通过前端页面连接：```http://fe_hostname:fe_http_port/backend``` 或者 ```http://fe_hostname:fe_http_port/system?path=//backends``` 来查看 BE 节点的情况。 以上方式，都需要 Doris 的 root 用户权限。\nBE 节点的扩容和缩容过程，不影响当前系统运行以及正在执行的任务，并且不会影响当前系统的性能。数据均衡会自动进行。根据集群现有数据量的大小，集群会在几个小时到1天不等的时间内，恢复到负载均衡的状态。集群负载情况，可以参见 [Tablet 负载均衡文档](../maint-monitor/tablet-repair-and-balance.md)。 ### 增加 BE 节点\nBE 节点的增加方式同 **BE 部署** 一节中的方式，通过 `ALTER SYSTEM ADD BACKEND` 命令增加 BE 节点。 > BE 扩容注意事项： > 1. BE 扩容后，Doris 会自动根据负载情况，进行数据均衡，期间不影响使用。 ### 删除 BE 节点 删除 BE 节点有两种方式：DROP 和 DECOMMISSION DROP 语句如下：']","在Apache Doris中实现节点缩容分为FE（Frontend）和BE（Backend）节点的缩容。

### FE 节点缩容

1. 使用mysql-client登录到Master FE。
2. 执行命令来删除FE节点。对于要删除的Follower或Observer节点，使用以下命令：
   ```
   ALTER SYSTEM DROP FOLLOWER ""fe_host:edit_log_port"";
   ```
   或
   ```
   ALTER SYSTEM DROP OBSERVER ""fe_host:edit_log_port"";
   ```
   其中`fe_host:edit_log_port`是要删除的FE节点的地址和端口。

#### FE缩容注意事项：
- 确保删除Follower FE后，剩余的Follower（包括Master）节点数量为奇数，以维持集群的高可用性。
- 通常建议FE节点（包括Master和Follower）的总数保持为3个，足以满足大多数需求。

### BE 节点缩容

1. 使用mysql-client登录到Master FE。
2. BE节点的删除有两种方式：DROP和DECOMMISSION。推荐使用DECOMMISSION方式，因为它会在删除节点前将数据迁移出该BE节点，保证数据的完整性。对于要删除的BE节点，使用以下命令：
   ```
   ALTER SYSTEM DECOMMISSION BACKEND ""be_host:be_heartbeat_port"";
   ```
   如果选择直接删除，使用DROP命令：
   ```
   ALTER SYSTEM DROP BACKEND ""be_host:be_heartbeat_port"";
   ```
   其中`be_host:be_heartbeat_port`是要删除的BE节点的地址和心跳端口。

#### BE缩容注意事项：
- BE缩容后，Doris会自动进行数据均衡，以保证集群负载均衡。这个过程不会影响集群的使用，但根据数据量的大小，可能需要几小时到一天的时间来完成数据均衡。

在执行缩容操作前，建议先检查集群的状态和数据分布情况，确保缩容后的集群能继续稳定运行并保持数据的完整性和可用性。",31.52,"['因为 Doris 的元数据采用多数写策略，即一条元数据 journal 必须至少写入多数个 FOLLOWER FE 后（比如 3 个 FOLLOWER，必须写成功 2 个），才算成功。而如果写入失败，FE 进程会主动退出。那么假设有 A、B、C 三个 FOLLOWER，C 先挂掉，然后 B 再挂掉，那么 A 也会跟着挂掉。所以如 `最佳实践` 一节中所述，如果你没有丰富的元数据运维经验，不建议部署多 FOLLOWER。\n6. fe.log 中出现 `get exception when try to close previously opened bdb database. ignore it` 如果后面有 `ignore it` 字样，通常无需处理。如果你有兴趣，可以在 `BDBEnvironment.java` 搜索这个错误，查看相关注释说明。 7. 从 `show frontends;` 看，某个 FE 的 `Join` 列为 `true`，但是实际该 FE 不正常\n通过 `show frontends;` 查看到的 `Join` 信息。该列如果为 `true`，仅表示这个 FE **曾经加入过** 集群。并不能表示当前仍然正常的存在于集群中。如果为 `false`，则表示这个 FE **从未加入过** 集群。 8. 关于 FE 的配置 `master_sync_policy`, `replica_sync_policy` 和 `txn_rollback_limit`\n`master_sync_policy` 用于指定当 Leader FE 写元数据日志时，是否调用 fsync(), `replica_sync_policy` 用于指定当 FE HA 部署时，其他 Follower FE 在同步元数据时，是否调用 fsync()。在早期的 Doris 版本中，这两个参数默认是 `WRITE_NO_SYNC`，即都不调用 fsync()。在最新版本的 Doris 中，默认已修改为 `SYNC`，即都调用 fsync()。调用 fsync()\n会显著降低元数据写盘的效率。在某些环境下，IOPS 可能降至几百，延迟增加到2-3ms（但对于 Doris 元数据操作依然够用）。因此我们建议以下配置：\n1. 对于单 Follower FE 部署，`master_sync_policy` 设置为 `SYNC`，防止 FE 系统宕机导致元数据丢失。 2. 对于多 Follower FE 部署，可以将 `master_sync_policy` 和 `replica_sync_policy` 设为 `WRITE_NO_SYNC`，因为我们认为多个系统同时宕机的概率非常低。\n如果在单 Follower FE 部署中，`master_sync_policy` 设置为 `WRITE_NO_SYNC`，则可能出现 FE 系统宕机导致元数据丢失。这时如果有其他 Observer FE 尝试重启时，可能会报错：\n``` Node xxx must rollback xx total commits(numPassedDurableCommits of which were durable) to the earliest point indicated by transaction xxxx in order to rejoin the replication group, but the transaction rollback limit of xxx prohibits this.\n``` 意思有部分已经持久化的事务需要回滚，但条数超过上限。这里我们的默认上限是 100，可以通过设置 `txn_rollback_limit` 改变。该操作仅用于尝试正常启动 FE，但已丢失的元数据无法恢复。 --- { ""title"": ""弹性扩缩容"", ""language"": ""zh-CN"" } --- <!--split--> # 扩容缩容 Doris 可以很方便的扩容和缩容 FE、BE、Broker 实例。 ## FE 扩容和缩容\n可以通过将 FE 扩容至 3 个以上节点来实现 FE 的高可用。 用户可以通过 mysql 客户端登陆 Master FE。通过: `SHOW PROC \'/frontends\';` 来查看当前 FE 的节点情况。 也可以通过前端页面连接：```http://fe_hostname:fe_http_port/frontend``` 或者 ```http://fe_hostname:fe_http_port/system?path=//frontends``` 来查看 FE 节点的情况。\n以上方式，都需要 Doris 的 root 用户权限。 FE 节点的扩容和缩容过程，不影响当前系统运行。 ### 增加 FE 节点', 'Traval。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hudi](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hudi)\n- JDBC Catalog 新增支持 Oceanbase，目前支持包括 MySQL、PostgreSQL、Oracle、SQLServer、Doris、Clickhouse、SAP HANA、Trino/Presto、Oceanbase\n等近十种关系型数据库。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/jdbc](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/jdbc)\n**数据权限管控** - 支持通过 Apache Range 对 Hive Catalog 进行鉴权，可以无缝对接用户现有的权限系统。同时还支持可扩展的鉴权插件，为任意 Catalog 实现自定义的鉴权方式。 参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hive](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hive)\n**性能进一步优化，最高提升数十倍** - 优化了大量小文件场景以及宽表场景的读取性能。通过小文件全量加载、小 IO 合并、数据预读等技术，显著降低远端存储的读取开销，在此类场景下，查询性能最高提升数十倍。 - 优化了 ORC/Parquet 文件的读取性能，相较于 1.2 版本查询性能提升一倍。 ![](/images/release-note-2.0beta-ssb-parquet.png) ![](/images/release-note-2.0beta-ssb-orc.png)\n- 支持湖上数据的本地文件缓存。可以利用本地磁盘缓存 HDFS 或对象存储等远端存储系统上的数据，通过缓存加速访问相同数据的查询。在命中本地文件缓存的情况下，通过 Apache Doris 查询湖上数据的性能可与 Apache Doris\n内部表持平，该功能可以极大提升湖上热数据的查询性能。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache](https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache)\n- 支持外表的统计信息收集。和 Apache Doris 内表一样，用户可以通过 Analyze 语句分析并收集指定外表的统计信息，结合 Nereids 全新查询优化器，能够更准确更智能地对复杂 SQL 进行查询计划的调优。以 TPC-H 标准测试数据集为例，无需手动改写 SQL 即可获得最优的查询计划并获得更好的性能表现。\n参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/)\n- 优化了 JDBC Catalog 的数据写回性能。通过 PrepareStmt 和批量方式，用户通过 INSERT INTO 命令、通过 JDBC Catalog 将数据写回到 MySQL、Oracle 等关系型数据库的性能提升数十倍。 ![](/images/release-note-2.0beta-jdbc.png) # 支持 Kubernetes 容器化部署\n在过去 Apache Doris 是基于 IP 通信的，在 K8s 环境部署时由于宿主机故障发生 Pod IP 漂移将导致集群不可用，在 2.0 版本中我们支持了 FQDN，使得 Apache Doris 可以在无需人工干预的情况下实现节点自愈，因此可以更好应对 K8s 环境部署以及灵活扩缩容。', '第一个启动的 FE 自动成为 Master。在此基础上，可以添加若干 Follower 和 Observer。 #### 配置及启动 Follower 或 Observer 这里 Follower 和 Observer 的配置同 Master 的配置。 首先第一次启动时，需执行以下命令： `./bin/start_fe.sh --helper leader_fe_host:edit_log_port --daemon`\n其中 leader\\_fe\\_host 为 Master 所在节点 ip, edit\\_log\\_port 在 Master 的配置文件 fe.conf 中。--helper 参数仅在 follower 和 observer 第一次启动时才需要。 #### 将 Follower 或 Observer 加入到集群 添加 Follower 或 Observer。使用 mysql-client 连接到已启动的 FE，并执行：\n`ALTER SYSTEM ADD FOLLOWER ""follower_host:edit_log_port"";` 或 `ALTER SYSTEM ADD OBSERVER ""observer_host:edit_log_port"";` 其中 follower\\_host和observer\\_host 为 Follower 或 Observer 所在节点 ip，edit\\_log\\_port 在其配置文件 fe.conf 中。\n查看 Follower 或 Observer 运行状态。使用 mysql-client 连接到任一已启动的 FE，并执行：SHOW PROC \'/frontends\'; 可以查看当前已加入集群的 FE 及其对应角色。\n> FE 扩容注意事项： > 1. Follower FE（包括 Master）的数量必须为奇数，建议最多部署 3 个组成高可用（HA）模式即可。 > 2. 当 FE 处于高可用部署时（1个 Master，2个 Follower），我们建议通过增加 Observer FE 来扩展 FE 的读服务能力。当然也可以继续增加 Follower FE，但几乎是不必要的。\n> 3. 通常一个 FE 节点可以应对 10-20 台 BE 节点。建议总的 FE 节点数量在 10 个以下。而通常 3 个即可满足绝大部分需求。 > 4. helper 不能指向 FE 自身，必须指向一个或多个已存在并且正常运行中的 Master/Follower FE。\n### 删除 FE 节点 使用以下命令删除对应的 FE 节点： ```ALTER SYSTEM DROP FOLLOWER[OBSERVER] ""fe_host:edit_log_port"";\n``` > FE 缩容注意事项： > 1. 删除 Follower FE 时，确保最终剩余的 Follower（包括 Master）节点为奇数。 ## BE 扩容和缩容 用户可以通过 mysql-client 登陆 Master FE。通过: ```SHOW PROC \'/backends\';\n``` 来查看当前 BE 的节点情况。 也可以通过前端页面连接：```http://fe_hostname:fe_http_port/backend``` 或者 ```http://fe_hostname:fe_http_port/system?path=//backends``` 来查看 BE 节点的情况。 以上方式，都需要 Doris 的 root 用户权限。\nBE 节点的扩容和缩容过程，不影响当前系统运行以及正在执行的任务，并且不会影响当前系统的性能。数据均衡会自动进行。根据集群现有数据量的大小，集群会在几个小时到1天不等的时间内，恢复到负载均衡的状态。集群负载情况，可以参见 [Tablet 负载均衡文档](../maint-monitor/tablet-repair-and-balance.md)。 ### 增加 BE 节点\nBE 节点的增加方式同 **BE 部署** 一节中的方式，通过 `ALTER SYSTEM ADD BACKEND` 命令增加 BE 节点。 > BE 扩容注意事项： > 1. BE 扩容后，Doris 会自动根据负载情况，进行数据均衡，期间不影响使用。 ### 删除 BE 节点 删除 BE 节点有两种方式：DROP 和 DECOMMISSION DROP 语句如下：']","在Apache Doris中实现节点缩容的步骤分为FE（Frontend）和BE（Backend）两部分，具体步骤如下：

### FE节点缩容
1. 使用mysql-client登录Master FE。
2. 执行ALTER SYSTEM命令删除FE节点，如要删除Follower FE或Observer FE，使用以下命令：
   ```sql
   ALTER SYSTEM DROP FOLLOWER ""fe_host:edit_log_port"";
   ```
   或者
   ```sql
   ALTER SYSTEM DROP OBSERVER ""fe_host:edit_log_port"";
   ```
   其中`fe_host`是要删除的FE节点的IP地址，`edit_log_port`是该节点的编辑日志端口。

### BE节点缩容
1. 使用mysql-client登录Master FE。
2. 删除BE节点有两种方式：DROP和DECOMMISSION。使用DECOMMISSION方式较为安全，可以确保数据迁移完成后再删除节点，命令如下：
   ```sql
   ALTER SYSTEM DECOMMISSION BACKEND ""be_host:be_heartbeat_service_port"";
   ```
   如果确定要立即删除BE节点，不等待数据迁移，可以使用DROP命令：
   ```sql
   ALTER SYSTEM DROP BACKEND ""be_host:be_heartbeat_service_port"";
   ```
   其中`be_host`是要删除的BE节点的IP地址，`be_heartbeat_service_port`是该节点的心跳服务端口。

### 注意事项
- 删除Follower FE时，确保最终剩余的Follower（包括Master）节点为奇数，以保持高可用性。
- 在进行BE节点的缩容时，应确保数据能够在其他节点上得到正确的迁移和备份，避免数据丢失。
- 使用DECOMMISSION方式删除BE节点时，根据集群现有数据量的大小，数据迁移可能需要几个小时到1天不等的时间，期间不影响集群的使用。

通过上述步骤，可以安全地对Apache Doris集群进行节点的缩容操作。","['如果在单 Follower FE 部署中，`master_sync_policy` 设置为 `WRITE_NO_SYNC`，则可能出现 FE 系统宕机导致元数据丢失。这时如果有其他 Observer FE 尝试重启时，可能会报错：\n\n    ```\n    Node xxx must rollback xx total commits(numPassedDurableCommits of which were durable) to the earliest point indicated by transaction xxxx in order to rejoin the replication group, but the transaction rollback limit of xxx prohibits this.\n    ```\n\n    意思有部分已经持久化的事务需要回滚，但条数超过上限。这里我们的默认上限是 100，可以通过设置 `txn_rollback_limit` 改变。该操作仅用于尝试正常启动 FE，但已丢失的元数据无法恢复。\n---\n{\n    ""title"": ""弹性扩缩容"",\n    ""language"": ""zh-CN""\n}\n\n---\n\n<!--split-->\n\n# 扩容缩容\n\nDoris 可以很方便的扩容和缩容 FE、BE、Broker 实例。\n\n## FE 扩容和缩容\n\n可以通过将 FE 扩容至 3 个以上节点来实现 FE 的高可用。\n\n用户可以通过 mysql 客户端登陆 Master FE。通过:\n\n`SHOW PROC \'/frontends\';`\n\n来查看当前 FE 的节点情况。\n\n也可以通过前端页面连接：```http://fe_hostname:fe_http_port/frontend``` 或者 ```http://fe_hostname:fe_http_port/system?path=//frontends``` 来查看 FE 节点的情况。\n\n以上方式，都需要 Doris 的 root 用户权限。\n\nFE 节点的扩容和缩容过程，不影响当前系统运行。\n\n### 增加 FE 节点', '以上方式，都需要 Doris 的 root 用户权限。\n\nFE 节点的扩容和缩容过程，不影响当前系统运行。\n\n### 增加 FE 节点\n\nFE 分为 Follower 和 Observer 两种角色，其中 Follower 角色会选举出一个 Follower 节点作为 Master。 默认一个集群，只能有一个 Master 状态的 Follower 角色，可以有多个 Follower 和 Observer，同时需保证 Follower 角色为奇数个。其中所有 Follower 角色组成一个选举组，如果 Master 状态的 Follower 宕机，则剩下的 Follower 会自动选出新的 Master，保证写入高可用。Observer 同步 Master 的数据，但是不参加选举。如果只部署一个 FE，则 FE 默认就是 Master。\n\n第一个启动的 FE 自动成为 Master。在此基础上，可以添加若干 Follower 和 Observer。\n\n#### 配置及启动 Follower 或 Observer\n\n这里 Follower 和 Observer 的配置同 Master 的配置。\n\n首先第一次启动时，需执行以下命令：\n\n`./bin/start_fe.sh --helper leader_fe_host:edit_log_port --daemon`\n\n其中 leader\\_fe\\_host 为 Master 所在节点 ip, edit\\_log\\_port 在 Master 的配置文件 fe.conf 中。--helper 参数仅在 follower 和 observer 第一次启动时才需要。\n\n#### 将 Follower 或 Observer 加入到集群\n\n添加 Follower 或 Observer。使用 mysql-client 连接到已启动的 FE，并执行：\n\n`ALTER SYSTEM ADD FOLLOWER ""follower_host:edit_log_port"";`\n\n或\n\n`ALTER SYSTEM ADD OBSERVER ""observer_host:edit_log_port"";`', '## BE 扩容和缩容\n\n用户可以通过 mysql-client 登陆 Master FE。通过:\n\n```SHOW PROC \'/backends\';```\n\n来查看当前 BE 的节点情况。\n\n也可以通过前端页面连接：```http://fe_hostname:fe_http_port/backend``` 或者 ```http://fe_hostname:fe_http_port/system?path=//backends``` 来查看 BE 节点的情况。\n\n以上方式，都需要 Doris 的 root 用户权限。\n\nBE 节点的扩容和缩容过程，不影响当前系统运行以及正在执行的任务，并且不会影响当前系统的性能。数据均衡会自动进行。根据集群现有数据量的大小，集群会在几个小时到1天不等的时间内，恢复到负载均衡的状态。集群负载情况，可以参见 [Tablet 负载均衡文档](../maint-monitor/tablet-repair-and-balance.md)。\n\n### 增加 BE 节点\n\nBE 节点的增加方式同 **BE 部署** 一节中的方式，通过 `ALTER SYSTEM ADD BACKEND` 命令增加 BE 节点。\n\n> BE 扩容注意事项：\n> 1. BE 扩容后，Doris 会自动根据负载情况，进行数据均衡，期间不影响使用。\n\n### 删除 BE 节点\n\n删除 BE 节点有两种方式：DROP 和 DECOMMISSION\n\nDROP 语句如下：\n\n```ALTER SYSTEM DROP BACKEND ""be_host:be_heartbeat_service_port"";```\n\n**注意：DROP BACKEND 会直接删除该 BE，并且其上的数据将不能再恢复！！！所以我们强烈不推荐使用 DROP BACKEND 这种方式删除 BE 节点。当你使用这个语句时，会有对应的防误操作提示。**\n\nDECOMMISSION 语句如下：\n\n```ALTER SYSTEM DECOMMISSION BACKEND ""be_host:be_heartbeat_service_port"";```', '![](/images/release-note-2.0beta-ssb-parquet.png)\n![](/images/release-note-2.0beta-ssb-orc.png)\n\n- 支持湖上数据的本地文件缓存。可以利用本地磁盘缓存 HDFS 或对象存储等远端存储系统上的数据，通过缓存加速访问相同数据的查询。在命中本地文件缓存的情况下，通过 Apache Doris 查询湖上数据的性能可与 Apache Doris 内部表持平，该功能可以极大提升湖上热数据的查询性能。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache](https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache)\n\n- 支持外表的统计信息收集。和 Apache Doris 内表一样，用户可以通过 Analyze 语句分析并收集指定外表的统计信息，结合 Nereids 全新查询优化器，能够更准确更智能地对复杂 SQL 进行查询计划的调优。以 TPC-H 标准测试数据集为例，无需手动改写 SQL 即可获得最优的查询计划并获得更好的性能表现。 参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/)\n\n- 优化了 JDBC Catalog 的数据写回性能。通过 PrepareStmt 和批量方式，用户通过 INSERT INTO 命令、通过 JDBC Catalog 将数据写回到 MySQL、Oracle 等关系型数据库的性能提升数十倍。\n\n![](/images/release-note-2.0beta-jdbc.png)\n\n# 支持 Kubernetes 容器化部署\n\n在过去 Apache Doris 是基于 IP 通信的，在 K8s 环境部署时由于宿主机故障发生 Pod IP 漂移将导致集群不可用，在 2.0 版本中我们支持了 FQDN，使得 Apache Doris 可以在无需人工干预的情况下实现节点自愈，因此可以更好应对 K8s 环境部署以及灵活扩缩容。', '第二种，冷热数据分层。在存储方面，冷热数据往往面临不同频次的查询和响应速度要求，因此通常可以将冷数据存储在成本更低的存储介质中。在过去版本中 Apache Doris 支持对表分区进行生命周期管理，通过后台任务将热数据从 SSD 自动冷却到 HDD，但 HDD 上的数据是以多副本的方式存储的，并没有做到最大程度的成本节约，因此对于冷数据存储成本仍然有较大的优化空间。在 Apache Doris 2.0 版本中推出了冷热数据分层功能，冷热数据分层功能使 Apache Doris 可以将冷数据下沉到存储成本更加低廉的对象存储中，同时冷数据在对象存储上的保存方式也从多副本变为单副本，存储成本进一步降至原先的三分之一，同时也减少了因存储附加的计算资源成本和网络开销成本。通过实际测算，存储成本最高可以降低超过 70%！\n\n面对更加彻底的存储计算分离需求，飞轮科技（SelectDB）技术团队设计并实现了全新的云原生存算分离架构（SelectDB Cloud），近一年来经历了大量企业客户的大规模使用，在性能、功能成熟度、系统稳定性等方面经受了真实生产环境的考验。在 Apache Doris 2.0.0 版本发布之际，飞轮科技宣布将这一经过大规模打磨后的成熟架构贡献至 Apache Doris 社区。这一工作预计将于 2023 年 10 月前后完成，届时全部存算分离的代码都将会提交到 Apache Doris 社区主干分支中，预计在 9 月广大社区用户就可以提前体验到基于存算分离架构的预览版本。\n\n# 易用性进一步提升\n\n除了以上功能需求外，在 Apache Doris 还增加了许多面向企业级特性的体验改进：\n\n### 支持 Kubernetes 容器化部署\n\n在过去 Apache Doris 是基于 IP 通信的，在 K8s 环境部署时由于宿主机故障发生 Pod IP 漂移将导致集群不可用，在 2.0 版本中我们支持了 FQDN，使得 Apache Doris 可以在无需人工干预的情况下实现节点自愈，因此可以更好应对 K8s 环境部署以及灵活扩缩容。\n\n### 跨集群数据复制']","在Apache Doris中，你可以通过以下步骤实现节点缩容：

1. 对于BE（Backend）节点缩容：
   - 使用mysql客户端登录Master FE。
   - 执行`SHOW PROC '/backends'`命令查看当前BE节点情况。
   - 执行`ALTER SYSTEM DROP BACKEND ""be_host:be_heartbeat_service_port""`命令删除BE节点。请注意，这种方式会立即删除BE节点及其上的数据，且数据无法恢复。所以请谨慎操作，避免不必要的数据丢失。

2. 对于FE（Frontend）节点缩容：
   - 可以关闭不需要的Follower或Observer节点的服务。
   - 在Master FE中执行相应的`ALTER SYSTEM DELETE FOLLOWER/OBSERVER`命令，从集群中移除这些节点。

需要注意的是，缩容过程中不会影响当前系统运行和正在进行的任务，也不会对系统性能产生影响。数据均衡会自动进行，根据集群现有数据量的大小，可能需要几个小时到一天左右的时间才能恢复到负载均衡状态。"
9b799d38-c18a-11ee-833b-b026288862a5,"在Doris中新建了一张表，在MySQL中可以查到，但在Doris中查不到，怎么办？
",可能是因为Doris的元数据没有及时刷新导致的。可以尝试手动刷新元数据，或者检查Doris的版本是否存在元数据刷新的问题。,"['导入任务的超时时间(以秒为单位)，导入任务在设定的 timeout 时间内未完成则会被系统取消，变成 CANCELLED。\n目前 Insert Into 并不支持自定义导入的 timeout 时间，所有 Insert Into 导入的超时时间是统一的，默认的 timeout 时间为4小时。如果导入的源文件无法在规定时间内完成导入，则需要调整 FE 的参数```insert_load_default_timeout_second```。 <version since=""dev""></version>\n同时 Insert Into 语句受到 Session 变量 `insert_timeout`的限制。可以通过 `SET insert_timeout = xxx;` 来增加超时时间，单位是秒。\n### Session 变量 + enable\\_insert\\_strict Insert Into 导入本身不能控制导入可容忍的错误率。用户只能通过 `enable_insert_strict` 这个 Session 参数用来控制。 当该参数设置为 false 时，表示至少有一条数据被正确导入，则返回成功。如果有失败数据，则还会返回一个 Label。 当该参数设置为 true 时，表示如果有一条数据错误，则导入失败。\n默认为 false。可通过 `SET enable_insert_strict = true;` 来设置。 + insert\\_timeout Insert Into 本身也是一个 SQL 命令，Insert Into 语句受到 Session 变量 <version since=""dev"" type=""inline"">`insert_timeout`</version> 的限制。可以通过 `SET insert_timeout = xxx;` 来增加超时时间，单位是秒。 ## 最佳实践\n### 应用场景\n1. 用户希望仅导入几条假数据，验证一下 Doris 系统的功能。此时适合使用 [INSERT INTO VALUES](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/INSERT.md) 的语法，这里语法和MySql语法一样。 2. 用户希望将已经在 Doris 表中的数据进行 ETL 转换并导入到一个新的 Doris 表中，此时适合使用 INSERT INTO SELECT 语法。\n3. 用户可以创建一种外部表，如 MySQL 外部表映射一张 MySQL 系统中的表。或者创建 [Broker](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/BROKER-LOAD.md) 外部表来映射 HDFS 上的数据文件。然后通过 INSERT INTO SELECT 语法将外部表中的数据导入到 Doris 表中存储。\n### 数据量 Insert Into 对数据量没有限制，大数据量导入也可以支持。但 Insert Into 有默认的超时时间，用户预估的导入数据量过大，就需要修改系统的 Insert Into 导入超时时间。\n``` 导入数据量 = 36G 约≤ 3600s * 10M/s 其中 10M/s 是最大导入限速，用户需要根据当前集群情况计算出平均的导入速度来替换公式中的 10M/s\n``` ### 完整例子 用户有一张表 store\\_sales 在数据库 sales 中，用户又创建了一张表叫 bj\\_store\\_sales 也在数据库 sales 中，用户希望将 store\\_sales 中销售记录在 bj 的数据导入到这张新建的表 bj\\_store\\_sales 中。导入的数据量约为：10G。 ```sql store_sales schema： (id, total, user_id, sale_timestamp, region)', '``` Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.getFilteredObjects(AuthorizationMetaStoreFilterHook.java:78) ~[hive-exec-3.1.3-core.jar:3.1.3]\nat org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.filterDatabases(AuthorizationMetaStoreFilterHook.java:55) ~[hive-exec-3.1.3-core.jar:3.1.3]\nat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1548) ~[doris-fe.jar:3.1.3]\nat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1542) ~[doris-fe.jar:3.1.3] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]\n``` 可以尝试在 `create catalog` 语句中添加 `""metastore.filter.hook"" = ""org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl""` 解决。 3. 如果创建 Hive Catalog 后能正常`show tables`，但查询时报`java.net.UnknownHostException: xxxxx`\n可以在 CATALOG 的 PROPERTIES 中添加\n``` \'fs.defaultFS\' = \'hdfs://<your_nameservice_or_actually_HDFS_IP_and_port>\'\n``` 4. Hive 1.x 的 orc 格式的表可能会遇到底层 orc 文件 schema 中列名为 `_col0`，`_col1`，`_col2`... 这类系统列名，此时需要在 catalog 配置中添加 `hive.version` 为 1.x.x，这样就会使用 hive 表中的列名进行映射。 ```sql CREATE CATALOG hive PROPERTIES ( \'hive.version\' = \'1.x.x\' );\n``` 5. 使用Catalog查询表数据时发现与Hive Metastore相关的报错：`Invalid method name`，需要设置`hive.version`参数。 ```sql CREATE CATALOG hive PROPERTIES ( \'hive.version\' = \'2.x.x\' );\n``` 6. 查询 ORC 格式的表，FE 报错 `Could not obtain block` 或 `Caused by: java.lang.NoSuchFieldError: types` 对于 ORC 文件，在默认情况下，FE 会访问 HDFS 获取文件信息，进行文件切分。部分情况下，FE 可能无法访问到 HDFS。可以通过添加以下参数解决： `""hive.exec.orc.split.strategy"" = ""BI""` 其他选项：HYBRID（默认），ETL。\n7. 在hive上可以查到hudi表分区字段的值，但是在doris查不到。 doris和hive目前查询hudi的方式不一样，doris需要在hudi表结构的avsc文件里添加上分区字段,如果没加，就会导致doris查询partition_val为空（即使设置了hoodie.datasource.hive_sync.partition_fields=partition_val也不可以）', ""| 4 | monitor | | | 5 | stats_history | /var/lib/proxysql/proxysql_stats.db | +-----+---------------+-------------------------------------+ 5 rows in set (0.000 sec) MySQL [(none)]> use main;\nReading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MySQL [main]> show tables; +--------------------------------------------+\n| tables | +--------------------------------------------+ | global_variables | | mysql_collations | | mysql_group_replication_hostgroups |\n| mysql_query_rules | | mysql_query_rules_fast_routing | | mysql_replication_hostgroups | | mysql_servers | | mysql_users |\n| proxysql_servers | | runtime_checksums_values | | runtime_global_variables | | runtime_mysql_group_replication_hostgroups | | runtime_mysql_query_rules |\n| runtime_mysql_query_rules_fast_routing | | runtime_mysql_replication_hostgroups | | runtime_mysql_servers | | runtime_mysql_users | | runtime_proxysql_servers |\n| runtime_scheduler | | scheduler | +--------------------------------------------+ 20 rows in set (0.000 sec)\n``` #### ProxySQL 配置后端 Doris FE 使用 insert 语句添加主机到 mysql_servers 表中，其中：hostgroup_id 为10表示写组，为20表示读组，我们这里不需要读写分离，无所谓随便设置哪一个都可以。\n```sql [root@mysql-proxy ~]# mysql -uadmin -padmin -P6032 -h127.0.0.1 ............ MySQL [(none)]> insert into mysql_servers(hostgroup_id,hostname,port) values(10,'192.168.9.211',9030); Query OK, 1 row affected (0.000 sec)\nMySQL [(none)]> insert into mysql_servers(hostgroup_id,hostname,port) values(10,'192.168.9.212',9030); Query OK, 1 row affected (0.000 sec) MySQL [(none)]> insert into mysql_servers(hostgroup_id,hostname,port) values(10,'192.168.9.213',9030);\nQuery OK, 1 row affected (0.000 sec) 如果在插入过程中，出现报错： ERROR 1045 (#2800): UNIQUE constraint failed: mysql_servers.hostgroup_id, mysql_servers.hostname, mysql_servers.port 说明可能之前就已经定义了其他配置，可以清空这张表 或者 删除对应host的配置""]","在 Doris 中新建的表在 MySQL 中可以查到但在 Doris 中查不到的情况通常涉及到元数据同步或刷新的问题。以下是一些解决步骤和建议：

1. **确认表是否已成功创建**：首先确认在 Doris 中执行创建表的 SQL 语句是否成功，没有返回错误信息。

2. **元数据刷新**：Doris 中的元数据可能没有及时刷新。可以尝试手动刷新元数据，方法是在 Doris 的查询界面执行 `SHOW TABLES;` 或 `DESC <表名>;` 等命令来触发元数据的刷新。

3. **权限问题**：确认当前使用的用户是否有权限查看该表。在 Doris 中，如果用户没有足够的权限，可能会导致无法查看表。

4. **查看 FE 日志**：如果上述方法都不能解决问题，可以查看 Doris 的 FE（Frontend）日志，看是否有错误信息。日志中可能会有关于创建表过程中出现的问题的详细信息。

5. **集群状态**：确认 Doris 集群的状态是否正常，包括 FE 和 BE（Backend）的状态。如果集群状态异常，可能会影响到表的创建和查询。

6. **版本兼容性**：确保你使用的 Doris 版本支持你正在尝试进行的操作。在一些情况下，新版本的 Doris 可能引入了新特性或者不再支持旧的操作方式。

如果以上步骤都无法解决问题，建议联系 Doris 社区或查看官方文档获取更多帮助。此外，提供更多的信息，如创建表的 SQL 语句、Doris 版本等信息，可能有助于进一步定位问题。

注意：上下文中没有提供直接相关的信息来解决“在 Doris 中新建的表在 MySQL 中可以查到但在 Doris 中查不到”的具体问题。",32.48,"['导入任务的超时时间(以秒为单位)，导入任务在设定的 timeout 时间内未完成则会被系统取消，变成 CANCELLED。\n目前 Insert Into 并不支持自定义导入的 timeout 时间，所有 Insert Into 导入的超时时间是统一的，默认的 timeout 时间为4小时。如果导入的源文件无法在规定时间内完成导入，则需要调整 FE 的参数```insert_load_default_timeout_second```。 <version since=""dev""></version>\n同时 Insert Into 语句受到 Session 变量 `insert_timeout`的限制。可以通过 `SET insert_timeout = xxx;` 来增加超时时间，单位是秒。\n### Session 变量 + enable\\_insert\\_strict Insert Into 导入本身不能控制导入可容忍的错误率。用户只能通过 `enable_insert_strict` 这个 Session 参数用来控制。 当该参数设置为 false 时，表示至少有一条数据被正确导入，则返回成功。如果有失败数据，则还会返回一个 Label。 当该参数设置为 true 时，表示如果有一条数据错误，则导入失败。\n默认为 false。可通过 `SET enable_insert_strict = true;` 来设置。 + insert\\_timeout Insert Into 本身也是一个 SQL 命令，Insert Into 语句受到 Session 变量 <version since=""dev"" type=""inline"">`insert_timeout`</version> 的限制。可以通过 `SET insert_timeout = xxx;` 来增加超时时间，单位是秒。 ## 最佳实践\n### 应用场景\n1. 用户希望仅导入几条假数据，验证一下 Doris 系统的功能。此时适合使用 [INSERT INTO VALUES](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/INSERT.md) 的语法，这里语法和MySql语法一样。 2. 用户希望将已经在 Doris 表中的数据进行 ETL 转换并导入到一个新的 Doris 表中，此时适合使用 INSERT INTO SELECT 语法。\n3. 用户可以创建一种外部表，如 MySQL 外部表映射一张 MySQL 系统中的表。或者创建 [Broker](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/BROKER-LOAD.md) 外部表来映射 HDFS 上的数据文件。然后通过 INSERT INTO SELECT 语法将外部表中的数据导入到 Doris 表中存储。\n### 数据量 Insert Into 对数据量没有限制，大数据量导入也可以支持。但 Insert Into 有默认的超时时间，用户预估的导入数据量过大，就需要修改系统的 Insert Into 导入超时时间。\n``` 导入数据量 = 36G 约≤ 3600s * 10M/s 其中 10M/s 是最大导入限速，用户需要根据当前集群情况计算出平均的导入速度来替换公式中的 10M/s\n``` ### 完整例子 用户有一张表 store\\_sales 在数据库 sales 中，用户又创建了一张表叫 bj\\_store\\_sales 也在数据库 sales 中，用户希望将 store\\_sales 中销售记录在 bj 的数据导入到这张新建的表 bj\\_store\\_sales 中。导入的数据量约为：10G。 ```sql store_sales schema： (id, total, user_id, sale_timestamp, region)', '``` Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.getFilteredObjects(AuthorizationMetaStoreFilterHook.java:78) ~[hive-exec-3.1.3-core.jar:3.1.3]\nat org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.filterDatabases(AuthorizationMetaStoreFilterHook.java:55) ~[hive-exec-3.1.3-core.jar:3.1.3]\nat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1548) ~[doris-fe.jar:3.1.3]\nat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1542) ~[doris-fe.jar:3.1.3] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]\n``` 可以尝试在 `create catalog` 语句中添加 `""metastore.filter.hook"" = ""org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl""` 解决。 3. 如果创建 Hive Catalog 后能正常`show tables`，但查询时报`java.net.UnknownHostException: xxxxx`\n可以在 CATALOG 的 PROPERTIES 中添加\n``` \'fs.defaultFS\' = \'hdfs://<your_nameservice_or_actually_HDFS_IP_and_port>\'\n``` 4. Hive 1.x 的 orc 格式的表可能会遇到底层 orc 文件 schema 中列名为 `_col0`，`_col1`，`_col2`... 这类系统列名，此时需要在 catalog 配置中添加 `hive.version` 为 1.x.x，这样就会使用 hive 表中的列名进行映射。 ```sql CREATE CATALOG hive PROPERTIES ( \'hive.version\' = \'1.x.x\' );\n``` 5. 使用Catalog查询表数据时发现与Hive Metastore相关的报错：`Invalid method name`，需要设置`hive.version`参数。 ```sql CREATE CATALOG hive PROPERTIES ( \'hive.version\' = \'2.x.x\' );\n``` 6. 查询 ORC 格式的表，FE 报错 `Could not obtain block` 或 `Caused by: java.lang.NoSuchFieldError: types` 对于 ORC 文件，在默认情况下，FE 会访问 HDFS 获取文件信息，进行文件切分。部分情况下，FE 可能无法访问到 HDFS。可以通过添加以下参数解决： `""hive.exec.orc.split.strategy"" = ""BI""` 其他选项：HYBRID（默认），ETL。\n7. 在hive上可以查到hudi表分区字段的值，但是在doris查不到。 doris和hive目前查询hudi的方式不一样，doris需要在hudi表结构的avsc文件里添加上分区字段,如果没加，就会导致doris查询partition_val为空（即使设置了hoodie.datasource.hive_sync.partition_fields=partition_val也不可以）', ""| 4 | monitor | | | 5 | stats_history | /var/lib/proxysql/proxysql_stats.db | +-----+---------------+-------------------------------------+ 5 rows in set (0.000 sec) MySQL [(none)]> use main;\nReading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MySQL [main]> show tables; +--------------------------------------------+\n| tables | +--------------------------------------------+ | global_variables | | mysql_collations | | mysql_group_replication_hostgroups |\n| mysql_query_rules | | mysql_query_rules_fast_routing | | mysql_replication_hostgroups | | mysql_servers | | mysql_users |\n| proxysql_servers | | runtime_checksums_values | | runtime_global_variables | | runtime_mysql_group_replication_hostgroups | | runtime_mysql_query_rules |\n| runtime_mysql_query_rules_fast_routing | | runtime_mysql_replication_hostgroups | | runtime_mysql_servers | | runtime_mysql_users | | runtime_proxysql_servers |\n| runtime_scheduler | | scheduler | +--------------------------------------------+ 20 rows in set (0.000 sec)\n``` #### ProxySQL 配置后端 Doris FE 使用 insert 语句添加主机到 mysql_servers 表中，其中：hostgroup_id 为10表示写组，为20表示读组，我们这里不需要读写分离，无所谓随便设置哪一个都可以。\n```sql [root@mysql-proxy ~]# mysql -uadmin -padmin -P6032 -h127.0.0.1 ............ MySQL [(none)]> insert into mysql_servers(hostgroup_id,hostname,port) values(10,'192.168.9.211',9030); Query OK, 1 row affected (0.000 sec)\nMySQL [(none)]> insert into mysql_servers(hostgroup_id,hostname,port) values(10,'192.168.9.212',9030); Query OK, 1 row affected (0.000 sec) MySQL [(none)]> insert into mysql_servers(hostgroup_id,hostname,port) values(10,'192.168.9.213',9030);\nQuery OK, 1 row affected (0.000 sec) 如果在插入过程中，出现报错： ERROR 1045 (#2800): UNIQUE constraint failed: mysql_servers.hostgroup_id, mysql_servers.hostname, mysql_servers.port 说明可能之前就已经定义了其他配置，可以清空这张表 或者 删除对应host的配置""]","如果您在Doris中新建了一张表，并且在MySQL中可以查询到这张表，但在Doris中查询不到，可能的原因及解决办法如下：

1. **刷新元数据缓存**：Doris的FE（Frontend）会缓存元数据信息，包括表的元数据。如果您刚在MySQL中创建了表，可能需要在Doris中执行`REFRESH`命令来刷新元数据缓存。您可以使用`SHOW TABLES;`命令来查看当前数据库中的所有表，以确认表是否已经被Doris识别。

2. **检查数据库选择**：确保您在Doris中选择了正确的数据库。在Doris中执行SQL查询前，需要通过`USE 数据库名;`命令选择正确的数据库。如果您在不同的数据库中创建了同名的表，需要确保您选择了包含您想要查询表的数据库。

3. **检查用户权限**：确保您使用的用户在Doris中有足够的权限来查看或查询该表。在Doris中，表的可见性可能受到权限控制。您可以检查当前用户的权限，确保其有权访问该表。

4. **查看Doris集群状态**：确保Doris的FE和BE（Backend）服务都处于正常运行状态。如果Doris集群中的某些节点出现问题，可能会影响到数据的查询。可以通过Doris的管理界面或命令行工具检查集群的状态。

5. **检查表的创建语句**：在某些情况下，表的创建语句可能包含Doris不支持的特定选项或数据类型，导致表虽然在MySQL中创建成功，但在Doris中无法识别。请检查表的创建语句，确保它完全兼容Doris的要求。

如果以上方法都不能解决问题，建议查看Doris的日志文件，寻找可能的错误信息或警告，这些信息可能会提供更多关于问题的线索。此外，也可以考虑在Doris的社区论坛或官方文档中寻求帮助。

Information is missing on the specific method or command to refresh metadata cache in Doris, which could be crucial for solving the issue of a table not appearing in Doris after being created in MySQL.","['### Q7. 关于数据目录SSD和HDD的配置, 建表有时候会遇到报错`Failed to find enough host with storage medium and tag`\n\nDoris支持一个BE节点配置多个存储路径。通常情况下，每块盘配置一个存储路径即可。同时，Doris支持指定路径的存储介质属性，如SSD或HDD。SSD代表高速存储设备，HDD代表低速存储设备。\n\n如果集群只有一种介质比如都是HDD或者都是SSD，最佳实践是不用在be.conf中显式指定介质属性。如果遇到上述报错```Failed to find enough host with storage medium and tag```，一般是因为be.conf中只配置了SSD的介质，而建表阶段中显式指定了```properties {""storage_medium"" = ""hdd""}```；同理如果be.conf只配置了HDD的介质，而而建表阶段中显式指定了```properties {""storage_medium"" = ""ssd""}```也会出现上述错误。解决方案可以修改建表的properties参数与配置匹配；或者将be.conf中SSD/HDD的显式配置去掉即可。\n\n通过指定路径的存储介质属性，我们可以利用Doris的冷热数据分区存储功能，在分区级别将热数据存储在SSD中，而冷数据会自动转移到HDD中。\n\n需要注意的是，Doris并不会自动感知存储路径所在磁盘的实际存储介质类型。这个类型需要用户在路径配置中显式的表示。比如路径 ""/path/to/data1.SSD"" 即表示这个路径是SSD存储介质。而 ""data1.SSD"" 就是实际的目录名称。Doris是根据目录名称后面的 "".SSD"" 后缀来确定存储介质类型的，而不是实际的存储介质类型。也就是说，用户可以指定任意路径为SSD存储介质，而Doris仅识别目录后缀，不会去判断存储介质是否匹配。如果不写后缀，则默认为HDD。\n\n换句话说，"".HDD"" 和 "".SSD"" 只是用于标识存储目录“相对”的“低速”和“高速”之分，而并不是标识实际的存储介质类型。所以如果BE节点上的存储路径没有介质区别，则无需填写后缀。\n\n### Q8. 多个FE，在使用Nginx实现web UI负载均衡时，无法登录', '`""hive.exec.orc.split.strategy"" = ""BI""`\n\n   其他选项：HYBRID（默认），ETL。\n\n7. 在hive上可以查到hudi表分区字段的值，但是在doris查不到。\n\n    doris和hive目前查询hudi的方式不一样，doris需要在hudi表结构的avsc文件里添加上分区字段,如果没加，就会导致doris查询partition_val为空（即使设置了hoodie.datasource.hive_sync.partition_fields=partition_val也不可以）\n    ```\n    {\n        ""type"": ""record"",\n        ""name"": ""record"",\n        ""fields"": [{\n            ""name"": ""partition_val"",\n            ""type"": [\n                ""null"",\n                ""string""\n                ],\n            ""doc"": ""Preset partition field, empty string when not partitioned"",\n            ""default"": null\n            },\n            {\n            ""name"": ""name"",\n            ""type"": ""string"",\n            ""doc"": ""名称""\n            },\n            {\n            ""name"": ""create_time"",\n            ""type"": ""string"",\n            ""doc"": ""创建时间""\n            }\n        ]\n    }\n    ```\n8. 查询hive外表，遇到该报错：`java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzoCodec not found`', ""## 创建用户\n\n下载免安装的 [MySQL 客户端](https://doris-build-hk.oss-cn-hongkong.aliyuncs.com/mysql-client/mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz)。\n\n### Root用户登录与密码修改\n\nDoris 内置 root，密码默认为空。\n\n>备注：\n>\n>Doris 提供的默认 root \n>\n>root 用户默认拥有集群所有权限。同时拥有 Grant_priv 和 Node_priv 的用户，可以将该权限赋予其他用户，拥有节点变更权限，包括 FE、BE、BROKER 节点的添加、删除、下线等操作。\n>\n>关于权限这块的具体说明可以参照[权限管理](../admin-manual/privilege-ldap/user-privilege.md)\n\n启动完 Doris 程序之后，可以通过 root 或 admin 用户连接到 Doris 集群。 使用下面命令即可登录 Doris，登录后进入到Doris对应的Mysql命令行操作界面：\n\n```bash\n[root@doris ~]# mysql  -h FE_HOST -P9030 -uroot\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 41\nServer version: 5.1.0 Doris version 1.0.0-preview2-b48ee2734\n\nCopyright (c) 2000, 2022, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql> \n```"", ""对于其他数据库，仍需要在查询时指定真实的库名和表名。\n\n2. 在 Doris 2.0.3 及之后的版本，对所有的数据库都有效，在查询时，会将所有的库名和表名以及列名转换为真实的名称，再去查询，如果是从老版本升级到 2.0.3 ，需要 `Refresh <catalog_name>` 才能生效。\n\n    但是，如果库名、表名或列名只有大小写不同，例如 `Doris` 和 `doris`，则 Doris 由于歧义而无法查询它们。\n\n3. 当 FE 参数的 `lower_case_table_names` 设置为 `1` 或 `2` 时，JDBC Catalog 的 `lower_case_table_names` 参数必须设置为 `true`。如果 FE 参数的 `lower_case_table_names` 设置为 `0`，则 JDBC Catalog 的参数可以为 `true` 或 `false`，默认为 `false`。这确保了 Doris 在处理内部和外部表配置时的一致性和可预测性。\n\n### 指定同步数据库\n\n`only_specified_database`:\n在jdbc连接时可以指定链接到哪个database/schema, 如：mysql中jdbc_url中可以指定database, pg的jdbc_url中可以指定currentSchema。\n\n`include_database_list`:\n仅在`only_specified_database=true`时生效，指定需要同步的 database，以','分割，db名称是大小写敏感的。\n\n`exclude_database_list`:\n仅在`only_specified_database=true`时生效，指定不需要同步的多个database，以','分割，db名称是大小写敏感的。\n\n当 `include_database_list` 和 `exclude_database_list` 有重合的database配置时，`exclude_database_list`会优先生效。\n\n如果使用该参数时连接oracle数据库，要求使用ojdbc8.jar以上版本jar包。\n\n## 数据查询\n\n### 示例""]","如果在Doris中新建了一张表，在MySQL中可以查到，但在Doris中查不到，这可能是由几个原因造成的。以下是一些排查步骤：

1. **确认连接的是正确的Doris集群：**
   确保你连接的Doris实例是你预期中创建表的那个。可以通过执行以下命令来检查：

   ```bash
   mysql -h FE_HOST -P9030 -uroot -e ""SHOW TABLES IN your_database_name;""
   ```

2. **检查数据库和表名的大小写：**
   在Doris中，数据库和表名是大小写敏感的。确保你在Doris中查询时使用的名称与MySQL中显示的完全一致。

3. **检查权限问题：**
   确保你有权访问查询的数据库和表。如果没有，你可能需要通过root用户或其他有足够权限的用户来授权。

   ```sql
   GRANT SELECT ON your_database_name.your_table_name TO 'your_user'@'your_host';
   ```

4. **检查是否需要刷新元数据：**
   如果是刚刚创建的表，可能需要执行元数据的刷新操作。虽然通常情况下，Doris会自动刷新元数据，但在某些情况下可能需要手动执行：

   ```sql
   REFRESH TABLE your_database_name.your_table_name;
   ```

5. **检查表是否被删除或改名：**
   有可能表在Doris中被删除或者重命名了，这种情况下需要在MySQL中检查表的最新状态。

6. **检查是否是外部表问题：**
   如果表是一个外部表，确认外部数据源连接器（如Hive、JDBC等）配置正确，并且外部数据库中的表是可访问的。

7. **检查是否有命名冲突：**
   如果在Doris中有同名的表或数据库，可能会导致查询指向错误的表。

如果以上步骤都无法解决问题，可能需要查看Doris的日志文件以获取更详细的错误信息，这有助于进一步诊断问题。通常，日志文件位于Doris FE和BE节点的安装目录下的 `log` 文件夹中。"
9b771828-c18a-11ee-8a86-b026288862a5,"在使用mysql -doris整库同步时，如果在mysql中增加字段，增加表，doris会自动同步过去吗？还是需要手动在doris中操作？
测试了一下，增加字段可以同步，但增加表不行，修改字段也不同步，需要做什么操作吗？
",是的，增加字段可以自动同步，但增加表和修改字段需要手动在doris中操作。如果在mysql中新建表后，在doris中也新建了，但数据几分钟了还没有同步，可能需要手动触发同步操作。,"['这是因为同一个库并发导入超过了100，可通过调整 fe.conf的参数 `max_running_txn_num_per_db` 来解决，具体可参考 [max_running_txn_num_per_db](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/fe-config/#max_running_txn_num_per_db)。\n同时，一个任务频繁修改label重启，也可能会导致这个错误。2pc场景下(Duplicate/Aggregate模型)，每个任务的label需要唯一，并且从checkpoint重启时，flink任务才会主动abort掉之前已经precommit成功，没有commit的txn，频繁修改label重启，会导致大量precommit成功的txn无法被abort，占用事务。在Unique模型下也可关闭2pc，可以实现幂等写入。 7. **Flink写入Uniq模型时，如何保证一批数据的有序性？**\n可以添加sequence列配置来保证，具体可参考 [sequence](https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual) 8. **Flink任务没报错，但是无法同步数据？** Connector1.1.0版本以前，是攒批写入的，写入均是由数据驱动，需要判断上游是否有数据写入。1.1.0之后，依赖Checkpoint，必须开启Checkpoint才能写入。\n9. **tablet writer write failed, tablet_id=190958, txn_id=3505530, err=-235** 通常发生在Connector1.1.0之前，是由于写入频率过快，导致版本过多。可以通过设置sink.batch.size 和 sink.batch.interval参数来降低Streamload的频率。 10. **Flink导入有脏数据，如何跳过？**\nFlink在数据导入时，如果有脏数据，比如字段格式、长度等问题，会导致StreamLoad报错，此时Flink会不断的重试。如果需要跳过，可以通过禁用StreamLoad的严格模式(strict_mode=false,max_filter_ratio=1)或者在Sink算子之前对数据做过滤。\n11. **源表和Doris表应如何对应？** 使用Flink Connector导入数据时，要注意两个方面，第一是源表的列和类型跟flink sql中的列和类型要对应上；第二个是flink sql中的列和类型要跟doris表的列和类型对应上，具体可以参考上面的""Doris 和 Flink 列类型映射关系"" 12. **TApplicationException: get_next failed: out of sequence response: expected 4 but got 3**\n这是由于 Thrift 框架存在并发 bug 导致的，建议你使用尽可能新的 connector 以及与之兼容的 flink 版本。 13. **DorisRuntimeException: Fail to abort transaction 26153 with url http://192.168.0.1:8040/api/table_name/_stream_load_2pc**\n你可以在 TaskManager 中搜索日志 `abort transaction response`，根据 http 返回码确定是 client 的问题还是 server 的问题。 14. **使用doris.filter.query出现org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""xx"" at line x, column xx**\n出现这个问题主要是条件varchar/string类型，需要加引号导致的，正确写法是 xxx = \'\'xxx\'\',这样Flink SQL 解析器会将两个连续的单引号解释为一个单引号字符,而不是字符串的结束，并将拼接后的字符串作为属性的值。 15. **如果出现Failed to connect to backend: http://host:webserver_port, 并且Be还是活着的**\n可能是因为你配置的be的ip，外部的Flink集群无法访问。这主要是因为当连接fe时，会通过fe解析出be的地址。例如，当你添加的be 地址为`127.0.0.1`,那么flink通过fe获取的be地址就为`127.0.0.1:webserver_port`,\n此时Flink就会去访问这个地址。当出现这个问题时，可以通过在with属性中增加实际对应的be外部ip地`\'benodes\'=""be_ip:webserver_port,be_ip:webserver_port...""`,整库同步则可增加`--sink-conf benodes=be_ip:webserver,be_ip:webserver...`。', '- `max_concurrency` 当前 Group允许的最大查询数，超过最大并发的查询到来时会进入排队逻辑； - `max_queue_size`查询排队的长度，当队列满了之后，新来的查询会被拒绝； - `queue_timeout`查询在队列中等待的时间，如果查询等待时间超过等待时间查询将会被拒绝，时间单位为毫秒； ![](/images/release-note-2.0beta-log-queue-group.png)\n参考文档：[https://doris.apache.org/zh-CN/docs/dev/admin-manual/workload-group/](https://doris.apache.org/zh-CN/docs/dev/admin-manual/workload-group/) ## 彻底告别 OOM\n在内存充足时内存管理通常对用户是无感的，但真实场景中往往面临着各式各样的极端 Case，这些都将为内存性能和稳定性带来挑战，尤其是在面临内存资源消耗巨大的复杂计算和大规模作业时，由于内存 OOM 导致查询失败甚至可能造成 BE 进程宕机。\n因此我们逐渐统一内存数据结构、重构 MemTracker、开始支持查询内存软限，并引入进程内存超限后的 GC 机制，同时优化了高并发的查询性能等。在 2.0 版本中我们引入了全新的内存管理框架，通过有效的内存分配、统计、管控，在 Benchmark、压力测试和真实用户业务场景的反馈中，基本消除了内存热点以及 OOM 导致 BE 宕机的问题，即使发生 OOM 通常也可依据日志定位内存位置并针对性调优，从而让集群恢复稳定，对查询和导入的内存限制也更加灵活，在内存充足时让用户无需感知内存使用。\n通过以上一系列优化，Apache Doris 2.0 版本在应对复杂计算以及大规模 ETL/ELT 操作时，内存资源得以有效控制，系统稳定性表现更上一个台阶。 详细介绍：[https://mp.weixin.qq.com/s/Z5N-uZrFE3Qhn5zTyEDomQ](https://mp.weixin.qq.com/s/Z5N-uZrFE3Qhn5zTyEDomQ) # 高效稳定的数据写入 ## 更高的实时数据写入效率 ### 导入性能进一步提升\n聚焦于实时分析，我们在过去的几个版本中在不断增强实时分析能力，其中端到端的数据实时写入能力是优化的重要方向，在 Apache Doris 2.0 版本中，我们进一步强化了这一能力。通过 Memtable 不使用 Skiplist、并行下刷、单副本导入等优化，使得导入性能有了大幅提升：\n- 使用 Stream Load 对 TPC-H 144G lineitem表 原始数据进行三副本导入 48 bucket 明细表，吞吐量提升 100%。 - 使用 Stream Load 对 TPC-H 144G lineitem表 原始数据进行三副本导入 48 bucket Unique Key 表，吞吐量提升 200%。\n- 对 TPC-H 144G lineitem 表进行 insert into select 导入 48 bucket Duplicate 明细表，吞吐量提升 50%。 - 对 TPC-H 144G lineitem 表进行 insert into select 导入 48 bucket UniqueKey 表，吞吐提升 150%。\n### 数据高频写入更稳定 在高频数据写入过程中，小文件合并和写放大问题以及随之而来的磁盘 I/O和 CPU 资源开销是制约系统稳定性的关键，因此在 2.0 版本中我们引入了 Vertical Compaction 以及 Segment Compaction，用以彻底解决 Compaction 内存问题以及写入过程中的 Segment 文件过多问题，资源消耗降低 90%，速度提升 50%，**内存占用仅为原先的 10%。**\n详细介绍：[https://mp.weixin.qq.com/s/BqiMXRJ2sh4jxKdJyEgM4A](https://mp.weixin.qq.com/s/BqiMXRJ2sh4jxKdJyEgM4A) ### 数据表结构自动同步\n在过去版本中我们引入了毫秒级别的 Schema Change，而在最新版本 Flink-Doris-Connector 中，我们实现了从 MySQL 等关系型数据库到 Apache Doris 的一键整库同步。在实际测试中单个同步任务可以承载数千张表的实时并行写入，从此彻底告别过去繁琐复杂的同步流程，通过简单命令即可实现上游业务数据库的表结构及数据同步。同时当上游数据结构发生变更时，也可以自动捕获 Schema 变更并将 DDL 动态同步到 Doris 中，保证业务的无缝运行。', '2022-11-16 14:29:04.204 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] do post work. 2022-11-16 14:29:04.204 [job-0] INFO JobContainer - DataX jobId [0] completed successfully.\n2022-11-16 14:29:04.204 [job-0] INFO HookInvoker - No hook invoked, because base dir not exists or is a file: /data/datax/hook 2022-11-16 14:29:04.205 [job-0] INFO JobContainer - [total cpu info] =>\naverageCpu | maxDeltaCpu | minDeltaCpu -1.00% | -1.00% | -1.00%\n[total gc info] => NAME | totalGCCount | maxDeltaGCCount | minDeltaGCCount | totalGCTime | maxDeltaGCTime | minDeltaGCTime\nPS MarkSweep | 1 | 1 | 1 | 0.017s | 0.017s | 0.017s\nPS Scavenge | 1 | 1 | 1 | 0.007s | 0.007s | 0.007s\n2022-11-16 14:29:04.205 [job-0] INFO JobContainer - PerfTrace not enable!\n2022-11-16 14:29:04.206 [job-0] INFO StandAloneJobContainerCommunicator - Total 2 records, 214 bytes | Speed 21B/s, 0 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.000s | All Task WaitReaderTime 0.000s | Percentage 100.00%\n2022-11-16 14:29:04.206 [job-0] INFO JobContainer - 任务启动时刻 : 2022-11-16 14:28:53 任务结束时刻 : 2022-11-16 14:29:04 任务总计耗时 : 10s 任务平均流量 : 21B/s\n记录写入速度 : 0rec/s 读出记录总数 : 2 读写失败总数 : 0\n``` --- { ""title"": ""Mysql to Doris"", ""language"": ""zh-CN"" } --- <!--split--> # Mysql to Doris 这是一个通过集合了 odbc 外部表创建、内部表创建以及数据同步等功能来帮助 MySQL 用户使用 Doris 的易用工具。']","在使用 Flink-Doris-Connector 实现从 MySQL 到 Apache Doris 的整库同步时，系统能够自动捕获上游 MySQL 数据库的 Schema 变更，并将这些变更动态同步到 Doris 中。这包括字段的增加，确保业务的无缝运行。

根据您的描述，您已经测试并确认了在 MySQL 中增加字段时，Doris 能够自动同步这一变更。然而，对于增加表格和修改字段的情况，您发现这些变更没有被自动同步到 Doris。

这种情况下，可能需要检查 Flink-Doris-Connector 的版本和配置，以确保您使用的版本支持这些自动同步功能。尽管在最新版本中，Doris 通过 Flink-Doris-Connector 支持了从 MySQL 等关系型数据库到 Doris 的一键整库同步，包括自动捕获 Schema 变更，但具体到增加表和修改字段的自动同步能力，可能会受到版本或配置的限制。

如果您已经确认使用的是支持这些功能的最新版本，那么可能需要进一步检查配置，确保正确配置了相关同步任务以支持表的增加和字段的修改同步。如果问题仍然存在，考虑查阅官方文档获取更多详细信息，或者联系技术支持以寻求帮助。

总之，理论上增加字段的自动同步是支持的，但增加表和修改字段的自动同步能力可能会受到当前使用的 Flink-Doris-Connector 版本或配置的限制。需要进一步检查确保系统配置正确，或更新到支持这些功能的版本。",29.34,"['这是因为同一个库并发导入超过了100，可通过调整 fe.conf的参数 `max_running_txn_num_per_db` 来解决，具体可参考 [max_running_txn_num_per_db](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/fe-config/#max_running_txn_num_per_db)。\n同时，一个任务频繁修改label重启，也可能会导致这个错误。2pc场景下(Duplicate/Aggregate模型)，每个任务的label需要唯一，并且从checkpoint重启时，flink任务才会主动abort掉之前已经precommit成功，没有commit的txn，频繁修改label重启，会导致大量precommit成功的txn无法被abort，占用事务。在Unique模型下也可关闭2pc，可以实现幂等写入。 7. **Flink写入Uniq模型时，如何保证一批数据的有序性？**\n可以添加sequence列配置来保证，具体可参考 [sequence](https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual) 8. **Flink任务没报错，但是无法同步数据？** Connector1.1.0版本以前，是攒批写入的，写入均是由数据驱动，需要判断上游是否有数据写入。1.1.0之后，依赖Checkpoint，必须开启Checkpoint才能写入。\n9. **tablet writer write failed, tablet_id=190958, txn_id=3505530, err=-235** 通常发生在Connector1.1.0之前，是由于写入频率过快，导致版本过多。可以通过设置sink.batch.size 和 sink.batch.interval参数来降低Streamload的频率。 10. **Flink导入有脏数据，如何跳过？**\nFlink在数据导入时，如果有脏数据，比如字段格式、长度等问题，会导致StreamLoad报错，此时Flink会不断的重试。如果需要跳过，可以通过禁用StreamLoad的严格模式(strict_mode=false,max_filter_ratio=1)或者在Sink算子之前对数据做过滤。\n11. **源表和Doris表应如何对应？** 使用Flink Connector导入数据时，要注意两个方面，第一是源表的列和类型跟flink sql中的列和类型要对应上；第二个是flink sql中的列和类型要跟doris表的列和类型对应上，具体可以参考上面的""Doris 和 Flink 列类型映射关系"" 12. **TApplicationException: get_next failed: out of sequence response: expected 4 but got 3**\n这是由于 Thrift 框架存在并发 bug 导致的，建议你使用尽可能新的 connector 以及与之兼容的 flink 版本。 13. **DorisRuntimeException: Fail to abort transaction 26153 with url http://192.168.0.1:8040/api/table_name/_stream_load_2pc**\n你可以在 TaskManager 中搜索日志 `abort transaction response`，根据 http 返回码确定是 client 的问题还是 server 的问题。 14. **使用doris.filter.query出现org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""xx"" at line x, column xx**\n出现这个问题主要是条件varchar/string类型，需要加引号导致的，正确写法是 xxx = \'\'xxx\'\',这样Flink SQL 解析器会将两个连续的单引号解释为一个单引号字符,而不是字符串的结束，并将拼接后的字符串作为属性的值。 15. **如果出现Failed to connect to backend: http://host:webserver_port, 并且Be还是活着的**\n可能是因为你配置的be的ip，外部的Flink集群无法访问。这主要是因为当连接fe时，会通过fe解析出be的地址。例如，当你添加的be 地址为`127.0.0.1`,那么flink通过fe获取的be地址就为`127.0.0.1:webserver_port`,\n此时Flink就会去访问这个地址。当出现这个问题时，可以通过在with属性中增加实际对应的be外部ip地`\'benodes\'=""be_ip:webserver_port,be_ip:webserver_port...""`,整库同步则可增加`--sink-conf benodes=be_ip:webserver,be_ip:webserver...`。', '- `max_concurrency` 当前 Group允许的最大查询数，超过最大并发的查询到来时会进入排队逻辑； - `max_queue_size`查询排队的长度，当队列满了之后，新来的查询会被拒绝； - `queue_timeout`查询在队列中等待的时间，如果查询等待时间超过等待时间查询将会被拒绝，时间单位为毫秒； ![](/images/release-note-2.0beta-log-queue-group.png)\n参考文档：[https://doris.apache.org/zh-CN/docs/dev/admin-manual/workload-group/](https://doris.apache.org/zh-CN/docs/dev/admin-manual/workload-group/) ## 彻底告别 OOM\n在内存充足时内存管理通常对用户是无感的，但真实场景中往往面临着各式各样的极端 Case，这些都将为内存性能和稳定性带来挑战，尤其是在面临内存资源消耗巨大的复杂计算和大规模作业时，由于内存 OOM 导致查询失败甚至可能造成 BE 进程宕机。\n因此我们逐渐统一内存数据结构、重构 MemTracker、开始支持查询内存软限，并引入进程内存超限后的 GC 机制，同时优化了高并发的查询性能等。在 2.0 版本中我们引入了全新的内存管理框架，通过有效的内存分配、统计、管控，在 Benchmark、压力测试和真实用户业务场景的反馈中，基本消除了内存热点以及 OOM 导致 BE 宕机的问题，即使发生 OOM 通常也可依据日志定位内存位置并针对性调优，从而让集群恢复稳定，对查询和导入的内存限制也更加灵活，在内存充足时让用户无需感知内存使用。\n通过以上一系列优化，Apache Doris 2.0 版本在应对复杂计算以及大规模 ETL/ELT 操作时，内存资源得以有效控制，系统稳定性表现更上一个台阶。 详细介绍：[https://mp.weixin.qq.com/s/Z5N-uZrFE3Qhn5zTyEDomQ](https://mp.weixin.qq.com/s/Z5N-uZrFE3Qhn5zTyEDomQ) # 高效稳定的数据写入 ## 更高的实时数据写入效率 ### 导入性能进一步提升\n聚焦于实时分析，我们在过去的几个版本中在不断增强实时分析能力，其中端到端的数据实时写入能力是优化的重要方向，在 Apache Doris 2.0 版本中，我们进一步强化了这一能力。通过 Memtable 不使用 Skiplist、并行下刷、单副本导入等优化，使得导入性能有了大幅提升：\n- 使用 Stream Load 对 TPC-H 144G lineitem表 原始数据进行三副本导入 48 bucket 明细表，吞吐量提升 100%。 - 使用 Stream Load 对 TPC-H 144G lineitem表 原始数据进行三副本导入 48 bucket Unique Key 表，吞吐量提升 200%。\n- 对 TPC-H 144G lineitem 表进行 insert into select 导入 48 bucket Duplicate 明细表，吞吐量提升 50%。 - 对 TPC-H 144G lineitem 表进行 insert into select 导入 48 bucket UniqueKey 表，吞吐提升 150%。\n### 数据高频写入更稳定 在高频数据写入过程中，小文件合并和写放大问题以及随之而来的磁盘 I/O和 CPU 资源开销是制约系统稳定性的关键，因此在 2.0 版本中我们引入了 Vertical Compaction 以及 Segment Compaction，用以彻底解决 Compaction 内存问题以及写入过程中的 Segment 文件过多问题，资源消耗降低 90%，速度提升 50%，**内存占用仅为原先的 10%。**\n详细介绍：[https://mp.weixin.qq.com/s/BqiMXRJ2sh4jxKdJyEgM4A](https://mp.weixin.qq.com/s/BqiMXRJ2sh4jxKdJyEgM4A) ### 数据表结构自动同步\n在过去版本中我们引入了毫秒级别的 Schema Change，而在最新版本 Flink-Doris-Connector 中，我们实现了从 MySQL 等关系型数据库到 Apache Doris 的一键整库同步。在实际测试中单个同步任务可以承载数千张表的实时并行写入，从此彻底告别过去繁琐复杂的同步流程，通过简单命令即可实现上游业务数据库的表结构及数据同步。同时当上游数据结构发生变更时，也可以自动捕获 Schema 变更并将 DDL 动态同步到 Doris 中，保证业务的无缝运行。', '2022-11-16 14:29:04.204 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] do post work. 2022-11-16 14:29:04.204 [job-0] INFO JobContainer - DataX jobId [0] completed successfully.\n2022-11-16 14:29:04.204 [job-0] INFO HookInvoker - No hook invoked, because base dir not exists or is a file: /data/datax/hook 2022-11-16 14:29:04.205 [job-0] INFO JobContainer - [total cpu info] =>\naverageCpu | maxDeltaCpu | minDeltaCpu -1.00% | -1.00% | -1.00%\n[total gc info] => NAME | totalGCCount | maxDeltaGCCount | minDeltaGCCount | totalGCTime | maxDeltaGCTime | minDeltaGCTime\nPS MarkSweep | 1 | 1 | 1 | 0.017s | 0.017s | 0.017s\nPS Scavenge | 1 | 1 | 1 | 0.007s | 0.007s | 0.007s\n2022-11-16 14:29:04.205 [job-0] INFO JobContainer - PerfTrace not enable!\n2022-11-16 14:29:04.206 [job-0] INFO StandAloneJobContainerCommunicator - Total 2 records, 214 bytes | Speed 21B/s, 0 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.000s | All Task WaitReaderTime 0.000s | Percentage 100.00%\n2022-11-16 14:29:04.206 [job-0] INFO JobContainer - 任务启动时刻 : 2022-11-16 14:28:53 任务结束时刻 : 2022-11-16 14:29:04 任务总计耗时 : 10s 任务平均流量 : 21B/s\n记录写入速度 : 0rec/s 读出记录总数 : 2 读写失败总数 : 0\n``` --- { ""title"": ""Mysql to Doris"", ""language"": ""zh-CN"" } --- <!--split--> # Mysql to Doris 这是一个通过集合了 odbc 外部表创建、内部表创建以及数据同步等功能来帮助 MySQL 用户使用 Doris 的易用工具。']","在使用 Flink-Doris-Connector 进行 MySQL 到 Apache Doris 的整库同步时，当上游 MySQL 数据库的表结构发生变更时，比如增加字段，这种 Schema 变更可以自动捕获并将 DDL 动态同步到 Doris 中。这意味着，增加字段这类操作可以自动同步。

然而，对于增加表和修改字段这些操作，根据您的测试情况和提供的上下文信息，这些操作似乎并不会自动同步。这可能是因为当前版本 Flink-Doris-Connector 的自动同步功能主要关注在字段级别的变更上，而不包括表级别的新增或字段属性的修改。

若要处理增加表或修改字段的情况，您可能需要手动在 Doris 中进行相应的操作。这包括手动创建新表以匹配 MySQL 中新增的表，或者在 Doris 中修改现有表的字段属性以反映在 MySQL 中进行的修改。

为了确保数据同步处理过程的顺利进行和数据一致性，建议定期检查 MySQL 和 Doris 之间的 Schema 一致性，并在发现差异时手动进行调整。同时，也可以关注 Apache Doris 项目的最新更新，看是否有新的功能或者改进能够支持更广泛的自动 Schema 同步。","['### Example\n\n1. 增加两个 Broker\n\n   ```sql\n    ALTER SYSTEM ADD BROKER ""host1:port"", ""host2:port"";\n   ```\n2. fe开启fqdn([fqdn](../../../admin-manual/cluster-management/fqdn.md))时添加一个Broker\n\n   ```sql\n    ALTER SYSTEM ADD BROKER ""broker_fqdn1:port"";\n   ```\n\n\n### Keywords\n\n    ALTER, SYSTEM, ADD, FOLLOWER, ALTER SYSTEM\n\n### Best Practice\n\n---\n{\n    ""title"": ""ALTER-SYSTEM-ADD-BACKEND"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## ALTER-SYSTEM-ADD-BACKEND\n\n### Name\n\nALTER SYSTEM ADD BACKEND\n\n### Description\n\n该语句用于操作一个系统内的节点。（仅管理员使用！）\n\n语法：\n\n```sql\n1) 增加节点\n   ALTER SYSTEM ADD BACKEND ""host:heartbeat_port""[,""host:heartbeat_port""...];\n```\n\n 说明：\n\n1. host 可以是主机名或者ip地址\n2. heartbeat_port 为该节点的心跳端口\n3. 增加和删除节点为同步操作。这两种操作不考虑节点上已有的数据，节点直接从元数据中删除，请谨慎使用。\n\n### Example\n\n 1. 增加一个节点\n    \n     ```sql\n    ALTER SYSTEM ADD BACKEND ""host:port"";\n    ```\n\n### Keywords\n\n    ALTER, SYSTEM, ADD, BACKEND, ALTER SYSTEM\n\n### Best Practice\n\n---\n{\n    ""title"": ""ALTER-SYSTEM-DROP-BACKEND"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->', '-- create Doris table\nCREATE TABLE `target_test` (\n  `id` int(11) NOT NULL COMMENT """",\n  `name` int(11) NOT NULL COMMENT """"\n) ENGINE=OLAP\nUNIQUE KEY(`id`)\nCOMMENT ""OLAP""\nDISTRIBUTED BY HASH(`id`) BUCKETS 8;\n\n-- enable batch delete\nALTER TABLE target_test ENABLE FEATURE ""BATCH_DELETE"";\n```\n**！！Doris表结构和Mysql表结构字段顺序必须保持一致！！**\n\n### 创建同步作业\n\n```text\nCREATE SYNC `demo`.`job`\n(\nFROM `demo`.`source_test1` INTO `target_test`\n(id,name)\n)\nFROM BINLOG\n(\n""type"" = ""canal"",\n""canal.server.ip"" = ""127.0.0.1"",\n""canal.server.port"" = ""11111"",\n""canal.destination"" = ""xxx"",\n""canal.username"" = ""canal"",\n""canal.password"" = ""canal""\n);\n```\n\n创建数据同步作业的详细语法可以连接到 Doris 后，[CREATE SYNC JOB](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/CREATE-SYNC-JOB.md) 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。\n\n语法：\n```\nCREATE SYNC [db.]job_name\n (\n        channel_desc, \n        channel_desc\n        ...\n )\nbinlog_desc\n```\n- job_name\n\n  `job_name`是数据同步作业在当前数据库内的唯一标识，相同`job_name`的作业只能有一个在运行。\n\n- channel_desc', '- `max_bytes_sync_commit`\n\n  提交事务时的数据大小的最大值。若Fe接收到的数据大小大于它，会立即提交事务并发送已积累的数据。默认值是64MB，如果你想修改此配置，请确保此值大于canal端的`canal.instance.memory.buffer.size`和`canal.instance.memory.buffer.memunit`的乘积（默认16MB）和`min_bytes_sync_commit`。\n\n- `max_sync_task_threads_num`\n\n  数据同步作业线程池中的最大线程数量。此线程池整个FE中只有一个，用于处理FE中所有数据同步作业向BE发送数据的任务task，线程池的实现在`SyncTaskPool`类。\n\n## 常见问题\n\n1. 修改表结构是否会影响数据同步作业？\n\n   会影响。数据同步作业并不能禁止`alter table`的操作，当表结构发生了变化，如果列的映射无法匹配，可能导致作业发生错误暂停，建议通过在数据同步作业中显式指定列映射关系，或者通过增加 Nullable 列或带 Default 值的列来减少这类问题。\n\n2. 删除了数据库后数据同步作业还会继续运行吗？\n\n   不会。删除数据库后的几秒日志中可能会出现找不到元数据的错误，之后该数据同步作业会被FE的定时调度检查时\x08停止。\n\n3. 多个数据同步作业可以配置相同的`ip:port + destination`吗？\n\n   不能。创建数据同步作业时会检查`ip:port + destination`与已存在的作业是否重复，防止出现多个作业连接到同一个instance的情况。\n\n4. 为什么数据同步时浮点类型的数据精度在Mysql端和Doris端不一样？\n\n   Doris本身浮点类型的精度与Mysql不一样。可以选择用Decimal类型代替\n\n## 更多帮助\n\n关于 Binlog Load 使用的更多详细语法及最佳实践，请参阅 [Binlog Load](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/CREATE-SYNC-JOB.md) 命令手册，你也可以在 MySql 客户端命令行下输入 `HELP BINLOG` 获取更多帮助信息。', '| Key                     | Comment                                                      |\n| ----------------------- | ------------------------------------------------------------ |\n| --job-name              | Flink任务名称, 非必需                                        |\n| --database              | 同步到Doris的数据库名                                        |\n| --table-prefix          | Doris表前缀名，例如 --table-prefix ods_。                    |\n| --table-suffix          | 同上，Doris表的后缀名。                                      |\n| --including-tables      | 需要同步的MySQL表，可以使用""\\|"" 分隔多个表，并支持正则表达式。 比如--including-tables table1 |\n| --excluding-tables      | 不需要同步的表，用法同上。                                   |', '| --excluding-tables      | 不需要同步的表，用法同上。                                   |\n| --mysql-conf            | MySQL CDCSource 配置，例如--mysql-conf hostname=127.0.0.1 ，您可以在[这里](https://ververica.github.io/flink-cdc-connectors/master/content/connectors/mysql-cdc.html)查看所有配置MySQL-CDC，其中hostname/username/password/database-name 是必需的。同步的库表中含有非主键表时，必须设置 `scan.incremental.snapshot.chunk.key-column`，且只能选择非空类型的一个字段。<br/>例如：`scan.incremental.snapshot.chunk.key-column=database.table:column,database.table1:column...`，不同的库表列之间用`,`隔开。 |\n| --oracle-conf           | Oracle CDCSource 配置，例如--oracle-conf hostname=127.0.0.1，您可以在[这里](https://ververica.github.io/flink-cdc-connectors/master/content/connectors/oracle-cdc.html)查看所有配置Oracle-CDC，其中hostname/username/password/database-name/schema-name 是必需的。 |', '| --postgres-conf         | Postgres CDCSource 配置，例如--postgres-conf hostname=127.0.0.1 ，您可以在[这里](https://ververica.github.io/flink-cdc-connectors/master/content/connectors/postgres-cdc.html)查看所有配置Postgres-CDC，其中hostname/username/password/database-name/schema-name/slot.name 是必需的。 |\n| --sqlserver-conf        | SQLServer CDCSource 配置，例如--sqlserver-conf hostname=127.0.0.1 ，您可以在[这里](https://ververica.github.io/flink-cdc-connectors/master/content/connectors/sqlserver-cdc.html)查看所有配置SQLServer-CDC，其中hostname/username/password/database-name/schema-name 是必需的。 |\n| --sink-conf             | Doris Sink 的所有配置，可以在[这里](https://doris.apache.org/zh-CN/docs/dev/ecosystem/flink-doris-connector/#%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE%E9%A1%B9)查看完整的配置项。 |\n| --table-conf            | Doris表的配置项，即properties中包含的内容。 例如 --table-conf replication_num=1 |\n| --ignore-default-value  | 关闭同步mysql表结构的默认值。适用于同步mysql数据到doris时，字段有默认值，但实际插入数据为null情况。参考[#152](https://github.com/apache/doris-flink-connector/pull/152) |', '你可以在 TaskManager 中搜索日志 `abort transaction response`，根据 http 返回码确定是 client 的问题还是 server 的问题。\n\n14. **使用doris.filter.query出现org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""xx"" at line x, column xx**\n\n出现这个问题主要是条件varchar/string类型，需要加引号导致的，正确写法是 xxx = \'\'xxx\'\',这样Flink SQL 解析器会将两个连续的单引号解释为一个单引号字符,而不是字符串的结束，并将拼接后的字符串作为属性的值。\n\n15. **如果出现Failed to connect to backend: http://host:webserver_port, 并且Be还是活着的**\n\n可能是因为你配置的be的ip，外部的Flink集群无法访问。这主要是因为当连接fe时，会通过fe解析出be的地址。例如，当你添加的be 地址为`127.0.0.1`,那么flink通过fe获取的be地址就为`127.0.0.1:webserver_port`, 此时Flink就会去访问这个地址。当出现这个问题时，可以通过在with属性中增加实际对应的be外部ip地`\'benodes\'=""be_ip:webserver_port,be_ip:webserver_port...""`,整库同步则可增加`--sink-conf benodes=be_ip:webserver,be_ip:webserver...`。\n---\n{\n""title"": ""DBT Doris Adapter"",\n""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# DBT Doris Adapter', '-   使用 Stream Load 对 TPC-H 144G lineitem表 原始数据进行三副本导入 48 bucket 明细表，吞吐量提升 100%。\n-   使用 Stream Load 对 TPC-H 144G lineitem表 原始数据进行三副本导入 48 bucket Unique Key 表，吞吐量提升 200%。\n-   对 TPC-H 144G lineitem 表进行 insert into select 导入 48 bucket Duplicate 明细表，吞吐量提升 50%。\n-   对 TPC-H 144G lineitem 表进行 insert into select 导入 48 bucket UniqueKey 表，吞吐提升 150%。\n\n### 数据高频写入更稳定\n\n在高频数据写入过程中，小文件合并和写放大问题以及随之而来的磁盘 I/O和 CPU 资源开销是制约系统稳定性的关键，因此在 2.0 版本中我们引入了 Vertical Compaction 以及 Segment Compaction，用以彻底解决 Compaction 内存问题以及写入过程中的 Segment 文件过多问题，资源消耗降低 90%，速度提升 50%，**内存占用仅为原先的 10%。**\n\n详细介绍：[https://mp.weixin.qq.com/s/BqiMXRJ2sh4jxKdJyEgM4A](https://mp.weixin.qq.com/s/BqiMXRJ2sh4jxKdJyEgM4A)\n\n### 数据表结构自动同步\n\n在过去版本中我们引入了毫秒级别的 Schema Change，而在最新版本 Flink-Doris-Connector 中，我们实现了从 MySQL 等关系型数据库到 Apache Doris 的一键整库同步。在实际测试中单个同步任务可以承载数千张表的实时并行写入，从此彻底告别过去繁琐复杂的同步流程，通过简单命令即可实现上游业务数据库的表结构及数据同步。同时当上游数据结构发生变更时，也可以自动捕获 Schema 变更并将 DDL 动态同步到 Doris 中，保证业务的无缝运行。', '- 使用 Stream Load 对 TPC-H 144G lineitem 表原始数据进行三副本导入 48 buckets Duplicate 表，吞吐量提升 100%。\n- 使用 Stream Load 对 TPC-H 144G lineitem 表原始数据进行三副本导入 48 buckets Unique Key 表，吞吐量提升 200%。\n- 使用 insert into select 对 TPC-H 144G lineitem 表进行导入 48 buckets Duplicate 表，吞吐量提升 50%。\n- 使用 insert into select 对 TPC-H 144G lineitem 表进行导入 48 buckets Unique Key 表，吞吐提升 150%。\n\n\n### 数据高频写入更稳定\n\n在高频数据写入过程中，小文件合并和写放大问题以及随之而来的磁盘 I/O和 CPU 资源开销是制约系统稳定性的关键，因此在 2.0 版本中我们引入了 Vertical Compaction 以及 Segment Compaction，用以彻底解决 Compaction 内存问题以及写入过程中的 Segment 文件过多问题，资源消耗降低 90%，速度提升 50%，内存占用仅为原先的 10%。\n\n\n### 数据表结构自动同步\n\n在过去版本中我们引入了毫秒级别的 Schema Change，而在最新版本 Flink-Doris-Connector 中，我们实现了从 MySQL 等关系型数据库到 Apache Doris 的一键整库同步。在实际测试中单个同步任务可以承载数千张表的实时并行写入，从此彻底告别过去繁琐复杂的同步流程，通过简单命令即可实现上游业务数据库的表结构及数据同步。同时当上游数据结构发生变更时，也可以自动捕获 Schema 变更并将 DDL 动态同步到 Doris 中，保证业务的无缝运行。\n\n# 更加完善的多租户资源隔离', '# New Feature\n\n### 数据湖分析 \n\n- **支持自动同步 Hive Metastore 元数据信息。** 默认情况下外部数据源的元数据变更，如创建或删除表、加减列等操作不会同步给 Doris，用户需要使用 `REFRESH CATALOG` 命令手动刷新元数据。在 1.2.2 版本中支持自动刷新 Hive Metastore 元数据信息，通过让 FE 节点定时读取 HMS 的 notification event 来感知 Hive 表元数据的变更情况。\n\n参考文档：[https://doris.apache.org/docs/dev/lakehouse/multi-catalog/](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/)\n\n- **支持读取 Iceberg Snapshot 以及查询 Snapshot 历史。**  在执行 Iceberg 数据写入时，每一次写操作都会产生一个新的快照。默认情况下通过 Apache Doris 读取 Iceberg 表仅会读取最新版本的快照。在 1.2.2 版本中可以使用 `FOR TIME AS OF` 和 `FOR VERSION AS OF` 语句，根据快照 ID 或者快照产生的时间读取历史版本的数据，也可以使用 iceberg_meta 表函数查询指定表的快照信息。\n\n参考文档：[https://doris.apache.org/docs/dev/lakehouse/multi-catalog/iceberg](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/iceberg)\n\n- JDBC Catalog 支持 PostgreSQL、Clickhouse、Oracle、SQLServer。\n\n- **JDBC Catalog 支持 insert into 操作。** 在 Doris 中建立 JDBC Catalog 后，可以通过 insert into 语句直接写入数据，也可以将 Doris 执行完查询之后的结果写入 JDBC Catalog，或者是从一个 JDBC 外表将数据导入另一个 JDBC 外表。', '综上，对于插入`is_being_synced`属性有两个切入点：全量同步中的restore过程和增量同步时的getDdlStmt。  \n\n在全量同步的restore过程中，syncer会通过rpc发起对原集群中snapshot的restore，在这个过程中为会为RestoreStmt添加`is_being_synced`属性，并在最终的restoreJob中生效，执行`isBeingSynced`的相关逻辑。  \n在增量同步时的getDdlStmt中，为getDdlStmt方法添加参数`boolean getDdlForSync`，以区分是否为受控转化为目标表ddl的操作，并在创建目标表时执行`isBeingSynced`的相关逻辑。\n  \n对于失效属性的擦除无需多言，对于上述功能的失效需要进行说明：\n1. 自动分桶  \n    自动分桶会在创建表时生效，计算当前合适的bucket数量，这就可能导致源表和目的表的bucket数目不一致。因此在同步时需要获得源表的bucket数目，并且也要获得源表是否为自动分桶表的信息以便结束同步后恢复功能。当前的做法是在获取distribution信息时默认autobucket为false，在恢复表时通过检查`_auto_bucket`属性来判断源表是否为自动分桶表，如是则将目标表的autobucket字段设置为true，以此来达到跳过计算bucket数量，直接应用源表bucket数量的目的。\n2. 动态分区  \n    动态分区则是通过将`olapTable.isBeingSynced()`添加到是否执行add/drop partition的判断中来实现的，这样目标表在被同步的过程中就不会周期性的执行add/drop partition操作。\n# 注意\n在未出现异常时，`is_being_synced`属性应该完全由syncer控制开启或关闭，用户不要自行修改该属性。---\n{\n    ""title"": ""使用 HLL 近似去重"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## HLL 近似去重', ""8. 当在Catalog里使用Kerberos配置时，不能同时使用`hadoop.username`属性。\n\n## JDBC Catalog\n\n1. 通过 JDBC Catalog 连接 SQLServer 报错：`unable to find valid certification path to requested target`\n\n   请在 `jdbc_url` 中添加 `trustServerCertificate=true` 选项。\n\n2. 通过 JDBC Catalog 连接 MySQL 数据库，中文字符乱码，或中文字符条件查询不正确\n\n   请在 `jdbc_url` 中添加 `useUnicode=true&characterEncoding=utf-8`\n\n   > 注：1.2.3 版本后，使用 JDBC Catalog 连接 MySQL 数据库，会自动添加这些参数。\n\n3. 通过 JDBC Catalog 连接 MySQL 数据库报错：`Establishing SSL connection without server's identity verification is not recommended`\n\n   请在 `jdbc_url` 中添加 `useSSL=true`\n\n4. 使用JDBC Catalog将MySQL数据同步到Doris中，日期数据同步错误。需要校验下MySQL的版本是否与MySQL的驱动包是否对应，比如MySQL8以上需要使用驱动com.mysql.cj.jdbc.Driver。\n\n\n## Hive Catalog \n\n1. 通过 Hive Metastore 访问 Iceberg 表报错：`failed to get schema` 或 `Storage schema reading not supported`\n\n   在 Hive 的 lib/ 目录放上 `iceberg` 运行时有关的 jar 包。\n\n   在 `hive-site.xml` 配置：\n\n   ```\n   metastore.storage.schema.reader.impl=org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader\n   ```\n\n   配置完成后需要重启Hive Metastore。"", ""对于其他数据库，仍需要在查询时指定真实的库名和表名。\n\n2. 在 Doris 2.0.3 及之后的版本，对所有的数据库都有效，在查询时，会将所有的库名和表名以及列名转换为真实的名称，再去查询，如果是从老版本升级到 2.0.3 ，需要 `Refresh <catalog_name>` 才能生效。\n\n    但是，如果库名、表名或列名只有大小写不同，例如 `Doris` 和 `doris`，则 Doris 由于歧义而无法查询它们。\n\n3. 当 FE 参数的 `lower_case_table_names` 设置为 `1` 或 `2` 时，JDBC Catalog 的 `lower_case_table_names` 参数必须设置为 `true`。如果 FE 参数的 `lower_case_table_names` 设置为 `0`，则 JDBC Catalog 的参数可以为 `true` 或 `false`，默认为 `false`。这确保了 Doris 在处理内部和外部表配置时的一致性和可预测性。\n\n### 指定同步数据库\n\n`only_specified_database`:\n在jdbc连接时可以指定链接到哪个database/schema, 如：mysql中jdbc_url中可以指定database, pg的jdbc_url中可以指定currentSchema。\n\n`include_database_list`:\n仅在`only_specified_database=true`时生效，指定需要同步的 database，以','分割，db名称是大小写敏感的。\n\n`exclude_database_list`:\n仅在`only_specified_database=true`时生效，指定不需要同步的多个database，以','分割，db名称是大小写敏感的。\n\n当 `include_database_list` 和 `exclude_database_list` 有重合的database配置时，`exclude_database_list`会优先生效。\n\n如果使用该参数时连接oracle数据库，要求使用ojdbc8.jar以上版本jar包。\n\n## 数据查询\n\n### 示例""]","在使用 mysql - doris 整库同步时，对于 MySQL 中的字段增加和表增加，Doris 并不会自动同步过去。对于字段的增加，Doris 能够自动同步，但是对于表的增加，Doris 目前还不支持自动同步。至于字段的修改，Doris 同样不会自动同步，需要您在 Doris 中手动进行操作。

如果您希望 MySQL 中的变更能够自动同步到 Doris 中，您可以考虑使用 DataX 或其他 ETL 工具来实现这一需求。这样可以确保数据的同步性，同时也能减轻您的工作负担。"
f882ad44-c17f-11ee-8c22-b026288862a5,"部分列更新 set enable_unique_key_partial_update=true 这个参数可以设置全局吗,",",set global","['- 实时的动态列更新，需要在表中实时的高频更新某些字段值。例如T+1生成的用户标签表中有一些关于用户最新行为信息的字段需要实时的更新，以实现广告/推荐等系统能够据其进行实时的分析和决策 - 将多张源表拼接成一张大宽表 - 数据修正 ## 基本原理 关于Unique Key模型和Aggregate Key模型的原理，可以主要参考[数据模型](../../data-table/data-model.md)的介绍 ### Unique Key 模型\n**Unique Key模型目前仅支持在Merge-on-Write实现上进行列更新** 用户通过正常的导入方式将一部分列的数据写入Doris的Memtable，此时Memtable中并没有整行数据，在Memtable下刷的时候，会查找历史数据，用历史数据补齐一整行，并写入数据文件中，同时将历史数据文件中相同key的数据行标记删除\n当出现并发导入时，Doris会利用MVCC机制来保证数据的正确性。如果两批数据导入都更新了一个相同key的不同列，则其中系统版本较高的导入任务会在版本较低的导入任务成功后，使用版本较低的导入任务写入的相同key的数据行重新进行补齐 ### Aggregate Key模型 将聚合函数设置为`REPLACE_IF_NOT_NULL`即可实现部分列更新的支持，详细用法参考下文示例 ## 并发写入和数据可见性\n部分列更新支持高频的并发写入，写入成功后数据即可见，系统自动通过MVCC机制来保证并发写入的数据正确性 ## 性能 使用建议： 1. 对写入性能要求较高，查询性能要求较低的用户，建议使用Aggregate Key模型 2. 对查询性能要求较高，对写入性能要求不高（例如数据的写入和更新基本都在凌晨低峰期完成），或者写入频率不高的用户，建议使用Unique Key模型merge-on-write实现 ### Unique Key模型Merge-on-Write实现\n由于Merge-on-Write实现需要在数据写入的时候，进行整行数据的补齐，以保证最优的查询性能，因此使用Merge-on-Write实现进行部分列更新会有较为明显的导入性能下降。 写入性能优化建议： 1. 使用配备了NVMe的SSD，或者极速SSD云盘。因为补齐数据时会大量的读取历史数据，产生较高的读IOPS，以及读吞吐 2. 开启行存将能够大大减少补齐数据时产生的IOPS，导入性能提升明显，用户可以在建表时通过如下property来开启行存：\n``` ""store_row_column"" = ""true""\n``` ### Aggregate Key模型 Aggregate Key模型在写入过程中不做任何额外处理，所以写入性能不受影响，与普通的数据导入相同。但是在查询时进行聚合的代价较大，典型的聚合查询性能相比Unique Key模型的Merge-on-Write实现会有5-10倍的下降。 ## 使用方式及示例 ### Unique Key模型 #### 建表 建表时需要指定如下property，以开启Merge-on-Write实现\n``` enable_unique_key_merge_on_write = true ``` #### StreamLoad/BrokerLoad/RoutineLoad 如果使用的是StreamLoad/BrokerLoad/RoutineLoad，在导入时添加如下header ``` partial_columns:true\n``` 同时在`columns`中指定要导入的列（必须包含所有key列，不然无法更新） #### Flink Connector 如果使用Flink Connector, 需要添加如下配置： ``` \'sink.properties.partial_columns\' = \'true\',\n``` 同时在`sink.properties.column`中指定要导入的列（必须包含所有key列，不然无法更新） #### INSERT INTO 在所有的数据模型中，`INSERT INTO` 给定一部分列时默认行为都是整行写入，为了防止误用，在Merge-on-Write实现中，`INSERT INTO`默认仍然保持整行UPSERT的语意，如果需要开启部分列更新的语意，需要设置如下 session variable\n``` set enable_unique_key_partial_update=true', '<version since=""dev""></version> 控制是否在 `show roles` 的结果里显示每个用户隐式对应的角色。默认为 false。 * `use_fix_replica` <version since=""1.2.0""></version>\n使用固定replica进行查询。replica从0开始，如果use_fix_replica为0，则使用最小的，如果use_fix_replica为1，则使用第二个最小的，依此类推。默认值为-1，表示未启用。 * `dry_run_query` <version since=""dev""></version> 如果设置为true，对于查询请求，将不再返回实际结果集，而仅返回行数。对于导入和insert，Sink 丢掉了数据，不会有实际的写发生。额默认为 false。\n该参数可以用于测试返回大量数据集时，规避结果集传输的耗时，重点关注底层查询执行的耗时。\n``` mysql> select * from bigtable; +--------------+ | ReturnedRows | +--------------+ | 10000000 | +--------------+\n``` * `enable_parquet_lazy_materialization` 控制 parquet reader 是否启用延迟物化技术。默认为 true。 * `enable_orc_lazy_materialization` 控制 orc reader 是否启用延迟物化技术。默认为 true。 * `enable_strong_consistency_read`\n用以开启强一致读。Doris 默认支持同一个会话内的强一致性，即同一个会话内对数据的变更操作是实时可见的。如需要会话间的强一致读，则需将此变量设置为true。 * `truncate_char_or_varchar_columns` 是否按照表的 schema 来截断 char 或者 varchar 列。默认为 false。\n因为外表会存在表的 schema 中 char 或者 varchar 列的最大长度和底层 parquet 或者 orc 文件中的 schema 不一致的情况。此时开启改选项，会按照表的 schema 中的最大长度进行截断。 * `jdbc_clickhouse_query_final` 是否在使用 JDBC Catalog 功能查询 ClickHouse 时增加 final 关键字，默认为 false 用于 ClickHouse 的 ReplacingMergeTree 表引擎查询去重\n* `enable_memtable_on_sink_node` <version since=""2.1.0""> 是否在数据导入中启用 MemTable 前移，默认为 false </version> 在 DataSink 节点上构建 MemTable，并通过 brpc streaming 发送 segment 到其他 BE。 该方法减少了多副本之间的重复工作，并且节省了数据序列化和反序列化的时间。 * `enable_unique_key_partial_update`\n<version since=""2.0.2"">\n是否在对insert into语句启用部分列更新的语义，默认为\nfalse。需要注意的是，控制insert语句是否开启严格模式的会话变量`enable_insert_strict`的默认值为true，即insert语句默认开启严格模式，而在严格模式下进行部分列更新不允许更新不存在的key。所以，在使用insert语句进行部分列更新的时候如果希望能插入不存在的key，需要在`enable_unique_key_partial_update`设置为true的基础上同时将`enable_insert_strict`设置为false。', '例如以下导入，需要先将列b14和列a13先cast成`array<string>`类型，再运用`array_union`函数。\n```sql LOAD LABEL label_03_14_49_34_898986_19090452100 ( DATA INFILE(""hdfs://test.hdfs.com:9000/user/test/data/sys/load/array_test.data"") INTO TABLE `test_array_table`\nCOLUMNS TERMINATED BY ""|"" (`k1`, `a1`, `a2`, `a3`, `a4`, `a5`, `a6`, `a7`, `a8`, `a9`, `a10`, `a11`, `a12`, `a13`, `b14`) SET(a14=array_union(cast(b14 as array<string>), cast(a13 as array<string>))) WHERE size(a2) > 270)\nWITH BROKER ""hdfs"" (""username""=""test_array"", ""password""="""") PROPERTIES( ""max_filter_ratio""=""0.8"" );\n``` --- { ""title"": ""INSERT"", ""language"": ""zh-CN"" } --- <!--split--> ## INSERT ### Name INSERT ### Description 该语句是完成数据插入操作。\n```sql INSERT INTO table_name [ PARTITION (p1, ...) ] [ WITH LABEL label] [ (column [, ...]) ] [ [ hint [, ...] ] ] { VALUES ( { expression | DEFAULT } [, ...] ) [, ...] | query }\n``` Parameters\n> tablet_name: 导入数据的目的表。可以是 `db_name.table_name` 形式 > > partitions: 指定待导入的分区，必须是 `table_name` 中存在的分区，多个分区名称用逗号分隔 > > label: 为 Insert 任务指定一个 label > > column_name: 指定的目的列，必须是 `table_name` 中存在的列 > > expression: 需要赋值给某个列的对应表达式 > > DEFAULT: 让对应列使用默认值 >\n> query: 一个普通查询，查询的结果会写入到目标中 > > hint: 用于指示 `INSERT` 执行行为的一些指示符。目前 hint 有三个可选值`/*+ STREAMING */`、`/*+ SHUFFLE */`或`/*+ NOSHUFFLE */` > 1. STREAMING：目前无实际作用，只是为了兼容之前的版本，因此保留。（之前的版本加上这个 hint 会返回 label，现在默认都会返回 label）\n> 2. SHUFFLE：当目标表是分区表，开启这个 hint 会进行 repartiiton。 > 3. NOSHUFFLE：即使目标表是分区表，也不会进行 repartiiton，但会做一些其他操作以保证数据正确落到各个分区中。\n对于开启了merge-on-write的Unique表，还可以使用insert语句进行部分列更新的操作。要使用insert语句进行部分列更新，需要将会话变量enable_unique_key_partial_update的值设置为true(该变量默认值为false，即默认无法通过insert语句进行部分列更新)。进行部分列更新时，插入的列必须至少包含所有的Key列，同时指定需要更新的列。如果插入行Key列的值在原表中存在，则将更新具有相同key列值那一行的数据。如果插入行Key列的值在原表中不存在，则将向']",`set enable_unique_key_partial_update=true`这个参数是一个会话（session）变量，它用于控制是否启用部分列更新的语义。根据提供的上下文，这个设置是在会话级别进行的，意味着它只对当前会话有效。并没有提及这个参数可以设置为全局变量。因此，如果需要在不同的会话中都启用部分列更新的特性，需要在每个会话开始时单独设置该参数。信息是缺失关于是否可以将`enable_unique_key_partial_update`设置为全局变量的明确说明。,13.51,"['- 实时的动态列更新，需要在表中实时的高频更新某些字段值。例如T+1生成的用户标签表中有一些关于用户最新行为信息的字段需要实时的更新，以实现广告/推荐等系统能够据其进行实时的分析和决策 - 将多张源表拼接成一张大宽表 - 数据修正 ## 基本原理 关于Unique Key模型和Aggregate Key模型的原理，可以主要参考[数据模型](../../data-table/data-model.md)的介绍 ### Unique Key 模型\n**Unique Key模型目前仅支持在Merge-on-Write实现上进行列更新** 用户通过正常的导入方式将一部分列的数据写入Doris的Memtable，此时Memtable中并没有整行数据，在Memtable下刷的时候，会查找历史数据，用历史数据补齐一整行，并写入数据文件中，同时将历史数据文件中相同key的数据行标记删除\n当出现并发导入时，Doris会利用MVCC机制来保证数据的正确性。如果两批数据导入都更新了一个相同key的不同列，则其中系统版本较高的导入任务会在版本较低的导入任务成功后，使用版本较低的导入任务写入的相同key的数据行重新进行补齐 ### Aggregate Key模型 将聚合函数设置为`REPLACE_IF_NOT_NULL`即可实现部分列更新的支持，详细用法参考下文示例 ## 并发写入和数据可见性\n部分列更新支持高频的并发写入，写入成功后数据即可见，系统自动通过MVCC机制来保证并发写入的数据正确性 ## 性能 使用建议： 1. 对写入性能要求较高，查询性能要求较低的用户，建议使用Aggregate Key模型 2. 对查询性能要求较高，对写入性能要求不高（例如数据的写入和更新基本都在凌晨低峰期完成），或者写入频率不高的用户，建议使用Unique Key模型merge-on-write实现 ### Unique Key模型Merge-on-Write实现\n由于Merge-on-Write实现需要在数据写入的时候，进行整行数据的补齐，以保证最优的查询性能，因此使用Merge-on-Write实现进行部分列更新会有较为明显的导入性能下降。 写入性能优化建议： 1. 使用配备了NVMe的SSD，或者极速SSD云盘。因为补齐数据时会大量的读取历史数据，产生较高的读IOPS，以及读吞吐 2. 开启行存将能够大大减少补齐数据时产生的IOPS，导入性能提升明显，用户可以在建表时通过如下property来开启行存：\n``` ""store_row_column"" = ""true""\n``` ### Aggregate Key模型 Aggregate Key模型在写入过程中不做任何额外处理，所以写入性能不受影响，与普通的数据导入相同。但是在查询时进行聚合的代价较大，典型的聚合查询性能相比Unique Key模型的Merge-on-Write实现会有5-10倍的下降。 ## 使用方式及示例 ### Unique Key模型 #### 建表 建表时需要指定如下property，以开启Merge-on-Write实现\n``` enable_unique_key_merge_on_write = true ``` #### StreamLoad/BrokerLoad/RoutineLoad 如果使用的是StreamLoad/BrokerLoad/RoutineLoad，在导入时添加如下header ``` partial_columns:true\n``` 同时在`columns`中指定要导入的列（必须包含所有key列，不然无法更新） #### Flink Connector 如果使用Flink Connector, 需要添加如下配置： ``` \'sink.properties.partial_columns\' = \'true\',\n``` 同时在`sink.properties.column`中指定要导入的列（必须包含所有key列，不然无法更新） #### INSERT INTO 在所有的数据模型中，`INSERT INTO` 给定一部分列时默认行为都是整行写入，为了防止误用，在Merge-on-Write实现中，`INSERT INTO`默认仍然保持整行UPSERT的语意，如果需要开启部分列更新的语意，需要设置如下 session variable\n``` set enable_unique_key_partial_update=true', '<version since=""dev""></version> 控制是否在 `show roles` 的结果里显示每个用户隐式对应的角色。默认为 false。 * `use_fix_replica` <version since=""1.2.0""></version>\n使用固定replica进行查询。replica从0开始，如果use_fix_replica为0，则使用最小的，如果use_fix_replica为1，则使用第二个最小的，依此类推。默认值为-1，表示未启用。 * `dry_run_query` <version since=""dev""></version> 如果设置为true，对于查询请求，将不再返回实际结果集，而仅返回行数。对于导入和insert，Sink 丢掉了数据，不会有实际的写发生。额默认为 false。\n该参数可以用于测试返回大量数据集时，规避结果集传输的耗时，重点关注底层查询执行的耗时。\n``` mysql> select * from bigtable; +--------------+ | ReturnedRows | +--------------+ | 10000000 | +--------------+\n``` * `enable_parquet_lazy_materialization` 控制 parquet reader 是否启用延迟物化技术。默认为 true。 * `enable_orc_lazy_materialization` 控制 orc reader 是否启用延迟物化技术。默认为 true。 * `enable_strong_consistency_read`\n用以开启强一致读。Doris 默认支持同一个会话内的强一致性，即同一个会话内对数据的变更操作是实时可见的。如需要会话间的强一致读，则需将此变量设置为true。 * `truncate_char_or_varchar_columns` 是否按照表的 schema 来截断 char 或者 varchar 列。默认为 false。\n因为外表会存在表的 schema 中 char 或者 varchar 列的最大长度和底层 parquet 或者 orc 文件中的 schema 不一致的情况。此时开启改选项，会按照表的 schema 中的最大长度进行截断。 * `jdbc_clickhouse_query_final` 是否在使用 JDBC Catalog 功能查询 ClickHouse 时增加 final 关键字，默认为 false 用于 ClickHouse 的 ReplacingMergeTree 表引擎查询去重\n* `enable_memtable_on_sink_node` <version since=""2.1.0""> 是否在数据导入中启用 MemTable 前移，默认为 false </version> 在 DataSink 节点上构建 MemTable，并通过 brpc streaming 发送 segment 到其他 BE。 该方法减少了多副本之间的重复工作，并且节省了数据序列化和反序列化的时间。 * `enable_unique_key_partial_update`\n<version since=""2.0.2"">\n是否在对insert into语句启用部分列更新的语义，默认为\nfalse。需要注意的是，控制insert语句是否开启严格模式的会话变量`enable_insert_strict`的默认值为true，即insert语句默认开启严格模式，而在严格模式下进行部分列更新不允许更新不存在的key。所以，在使用insert语句进行部分列更新的时候如果希望能插入不存在的key，需要在`enable_unique_key_partial_update`设置为true的基础上同时将`enable_insert_strict`设置为false。', '例如以下导入，需要先将列b14和列a13先cast成`array<string>`类型，再运用`array_union`函数。\n```sql LOAD LABEL label_03_14_49_34_898986_19090452100 ( DATA INFILE(""hdfs://test.hdfs.com:9000/user/test/data/sys/load/array_test.data"") INTO TABLE `test_array_table`\nCOLUMNS TERMINATED BY ""|"" (`k1`, `a1`, `a2`, `a3`, `a4`, `a5`, `a6`, `a7`, `a8`, `a9`, `a10`, `a11`, `a12`, `a13`, `b14`) SET(a14=array_union(cast(b14 as array<string>), cast(a13 as array<string>))) WHERE size(a2) > 270)\nWITH BROKER ""hdfs"" (""username""=""test_array"", ""password""="""") PROPERTIES( ""max_filter_ratio""=""0.8"" );\n``` --- { ""title"": ""INSERT"", ""language"": ""zh-CN"" } --- <!--split--> ## INSERT ### Name INSERT ### Description 该语句是完成数据插入操作。\n```sql INSERT INTO table_name [ PARTITION (p1, ...) ] [ WITH LABEL label] [ (column [, ...]) ] [ [ hint [, ...] ] ] { VALUES ( { expression | DEFAULT } [, ...] ) [, ...] | query }\n``` Parameters\n> tablet_name: 导入数据的目的表。可以是 `db_name.table_name` 形式 > > partitions: 指定待导入的分区，必须是 `table_name` 中存在的分区，多个分区名称用逗号分隔 > > label: 为 Insert 任务指定一个 label > > column_name: 指定的目的列，必须是 `table_name` 中存在的列 > > expression: 需要赋值给某个列的对应表达式 > > DEFAULT: 让对应列使用默认值 >\n> query: 一个普通查询，查询的结果会写入到目标中 > > hint: 用于指示 `INSERT` 执行行为的一些指示符。目前 hint 有三个可选值`/*+ STREAMING */`、`/*+ SHUFFLE */`或`/*+ NOSHUFFLE */` > 1. STREAMING：目前无实际作用，只是为了兼容之前的版本，因此保留。（之前的版本加上这个 hint 会返回 label，现在默认都会返回 label）\n> 2. SHUFFLE：当目标表是分区表，开启这个 hint 会进行 repartiiton。 > 3. NOSHUFFLE：即使目标表是分区表，也不会进行 repartiiton，但会做一些其他操作以保证数据正确落到各个分区中。\n对于开启了merge-on-write的Unique表，还可以使用insert语句进行部分列更新的操作。要使用insert语句进行部分列更新，需要将会话变量enable_unique_key_partial_update的值设置为true(该变量默认值为false，即默认无法通过insert语句进行部分列更新)。进行部分列更新时，插入的列必须至少包含所有的Key列，同时指定需要更新的列。如果插入行Key列的值在原表中存在，则将更新具有相同key列值那一行的数据。如果插入行Key列的值在原表中不存在，则将向']",`set enable_unique_key_partial_update=true`是一个会话级别（session-level）的设置，意味着它只对当前数据库会话生效。这个设置并不是全局参数，因此不能全局设置。如果需要在不同的会话中使用部分列更新的功能，你需要在每个会话中分别设置此参数。全局参数通常在数据库的配置文件中设置或通过数据库管理命令设置，而`enable_unique_key_partial_update`并不支持这样的全局配置方式。,"['<version since=""1.2.7"">当 `enable_profile` 为 true 时，Stream Load profile 将会被打印到 be.INFO 日志中。</version>\n\n- memtable_on_sink_node\n\n  <version since=""2.1.0"">\n  是否在数据导入中启用 MemTable 前移，默认为 false\n  </version>\n\n  在 DataSink 节点上构建 MemTable，并通过 brpc streaming 发送 segment 到其他 BE。\n  该方法减少了多副本之间的重复工作，并且节省了数据序列化和反序列化的时间。\n\n- partial_columns\n\n  <version since=""2.0"">\n\n  是否启用部分列更新，布尔类型，为 true 表示使用部分列更新，默认值为 false，该参数只允许在表模型为 Unique 且采用 Merge on Write 时设置。\n  \n  eg: `curl  --location-trusted -u root: -H ""partial_columns:true"" -H ""column_separator:,"" -H ""columns:id,balance,last_access_time"" -T /tmp/test.csv http://127.0.0.1:48037/api/db1/user_profile/_stream_load`\n\n  </version>\n\n### 使用SQL表达Stream Load的参数\n\n可以在Header中添加一个`sql`的参数，去替代之前参数中的`column_separator`、`line_delimiter`、`where`、`columns`等参数，方便使用。\n\n```\ncurl --location-trusted -u user:passwd [-H ""sql: ${load_sql}""...] -T data.file -XPUT http://fe_host:http_port/api/_http_stream', '```\nset enable_unique_key_partial_update=true\n```\n\n需要注意的是，控制insert语句是否开启严格模式的会话变量`enable_insert_strict`的默认值为true，即insert语句默认开启严格模式，而在严格模式下进行部分列更新不允许更新不存在的key。所以，在使用insert语句进行部分列更新的时候如果希望能插入不存在的key，需要在`enable_unique_key_partial_update`设置为true的基础上同时将`enable_insert_strict`设置为false。\n\n#### 示例\n\n假设 Doris 中存在一张订单表order_tbl，其中 订单id 是 Key 列，订单状态，订单金额是 Value 列。数据状态如下：\n\n| 订单id | 订单金额 | 订单状态 |\n| ------ | -------- | -------- |\n| 1      | 100      | 待付款   |\n\n```sql\n+----------+--------------+--------------+\n| order_id | order_amount | order_status |\n+----------+--------------+--------------+\n| 1        |          100 | 待付款        |\n+----------+--------------+--------------+\n1 row in set (0.01 sec)\n```\n\n这时候，用户点击付款后，Doris 系统需要将订单id 为 \'1\' 的订单状态变更为 \'待发货\'。\n\n若使用StreamLoad可以通过如下方式进行更新：\n\n```sql\n$cat update.csv\n1,待发货\n\n$ curl  --location-trusted -u root: -H ""partial_columns:true"" -H ""column_separator:,"" -H ""columns:order_id,order_status"" -T /tmp/update.csv http://127.0.0.1:48037/api/db1/order_tbl/_stream_load\n```', '若使用`INSRT INTO`可以通过如下方式进行更新：\n\n```\nset enable_unique_key_partial_update=true;\nINSERT INTO order_tbl (order_id, order_status) values (1,\'待发货\');\n```\n\n更新后结果如下\n\n```sql\n+----------+--------------+--------------+\n| order_id | order_amount | order_status |\n+----------+--------------+--------------+\n| 1        |          100 | 待发货        |\n+----------+--------------+--------------+\n1 row in set (0.01 sec)\n```\n\n### Aggregate Key模型\n\n#### 建表\n\n将需要进行列更新的字段对应的聚合函数设置为`REPLACE_IF_NOT_NULL`\n\n```\nCREATE TABLE `order_tbl` (\n  `order_id` int(11) NULL,\n  `order_amount` int(11) REPLACE_IF_NOT_NULL NULL,\n  `order_status` varchar(100) REPLACE_IF_NOT_NULL NULL\n) ENGINE=OLAP\nAGGREGATE KEY(`order_id`)\nCOMMENT \'OLAP\'\nDISTRIBUTED BY HASH(`order_id`) BUCKETS 1\nPROPERTIES (\n""replication_allocation"" = ""tag.location.default: 1""\n);\n```\n\n#### 数据写入\n\n无论是导入任务还是`INSERT INTO`, 直接写入要更新的字段的数据即可\n\n#### 示例\n\n与前面例子相同，对应的Stream Load命令为（不需要额外的header）：', '7. `jsonpaths`\n\n     当导入数据格式为 json 时，可以通过 jsonpaths 指定抽取 Json 数据中的字段。\n\n     `-H ""jsonpaths: [\\""$.k2\\"", \\""$.k1\\""]""`\n\n  8. `strip_outer_array`\n\n     当导入数据格式为 json 时，strip_outer_array 为 true 表示 Json 数据以数组的形式展现，数据中的每一个元素将被视为一行数据。默认值是 false。\n\n     `-H ""strip_outer_array: true""`\n\n  9. `json_root`\n\n     当导入数据格式为 json 时，可以通过 json_root 指定 Json 数据的根节点。Doris 将通过 json_root 抽取根节点的元素进行解析。默认为空。\n\n     `-H ""json_root: $.RECORDS""`\n  \n  10. `send_batch_parallelism`\n\n      整型，用于设置发送批处理数据的并行度，如果并行度的值超过 BE 配置中的 `max_send_batch_parallelism_per_job`，那么作为协调点的 BE 将使用 `max_send_batch_parallelism_per_job` 的值。 \n\n  11. `load_to_single_tablet`\n\n      布尔类型，为 true 表示支持一个任务只导入数据到对应分区的一个 tablet，默认值为 false，该参数只允许在对带有 random 分桶的 olap 表导数的时候设置。\n\n  12. `partial_columns`\n      布尔类型，为 true 表示使用部分列更新，默认值为 false，该参数只允许在表模型为 Unique 且采用 Merge on Write 时设置。一流多表不支持此参数。\n\n  13. `max_filter_ratio`\n\n      采样窗口内，允许的最大过滤率。必须在大于等于0到小于等于1之间。默认值是 1.0。\n\n      采样窗口为 `max_batch_rows * 10`。即如果在采样窗口内，错误行数/总行数大于 `max_filter_ratio`，则会导致例行作业被暂停，需要人工介入检查数据质量问题。', '* `enable_unique_key_partial_update`\n\n  <version since=""2.0.2"">\n  是否在对insert into语句启用部分列更新的语义，默认为 false。需要注意的是，控制insert语句是否开启严格模式的会话变量`enable_insert_strict`的默认值为true，即insert语句默认开启严格模式，而在严格模式下进行部分列更新不允许更新不存在的key。所以，在使用insert语句进行部分列更新的时候如果希望能插入不存在的key，需要在`enable_unique_key_partial_update`设置为true的基础上同时将`enable_insert_strict`设置为false。\n  </version>\n\n* `describe_extend_variant_column`\n\n  是否展示 variant 的拆解列。默认为 false。\n\n***\n\n#### 关于语句执行超时控制的补充说明\n\n* 控制手段\n\n    目前doris支持通过`variable`和`user property`两种体系来进行超时控制。其中均包含`qeury_timeout`和`insert_timeout`。\n\n* 优先次序\n\n    超时生效的优先级次序是：`session variable` > `user property` > `global variable` > `default value`\n\n    较高优先级的变量未设置时，会自动采用下一个优先级的数值。\n\n* 相关语义\n\n    `query_timeout`用于控制所有语句的超时，`insert_timeout`特定用于控制 INSERT 语句的超时，在执行 INSERT 语句时，超时时间会取\n    \n    `query_timeout`和`insert_timeout`中的最大值。\n\n    `user property`中的`query_timeout`和`insert_timeout`只能由 ADMIN 用户对目标用户予以指定，其语义在于改变被指定用户的默认超时时间，\n    \n    并且不具备`quota`语义。\n\n* 注意事项', 'Parameters\n\n> tablet_name: 导入数据的目的表。可以是 `db_name.table_name` 形式\n>\n> partitions: 指定待导入的分区，必须是 `table_name` 中存在的分区，多个分区名称用逗号分隔\n>\n> label: 为 Insert 任务指定一个 label\n>\n> column_name: 指定的目的列，必须是 `table_name` 中存在的列\n>\n> expression: 需要赋值给某个列的对应表达式\n>\n> DEFAULT: 让对应列使用默认值\n>\n> query: 一个普通查询，查询的结果会写入到目标中\n>\n> hint: 用于指示 `INSERT` 执行行为的一些指示符。目前 hint 有三个可选值`/*+ STREAMING */`、`/*+ SHUFFLE */`或`/*+ NOSHUFFLE */`\n> 1. STREAMING：目前无实际作用，只是为了兼容之前的版本，因此保留。（之前的版本加上这个 hint 会返回 label，现在默认都会返回 label）\n> 2. SHUFFLE：当目标表是分区表，开启这个 hint 会进行 repartiiton。\n> 3. NOSHUFFLE：即使目标表是分区表，也不会进行 repartiiton，但会做一些其他操作以保证数据正确落到各个分区中。\n\n对于开启了merge-on-write的Unique表，还可以使用insert语句进行部分列更新的操作。要使用insert语句进行部分列更新，需要将会话变量enable_unique_key_partial_update的值设置为true(该变量默认值为false，即默认无法通过insert语句进行部分列更新)。进行部分列更新时，插入的列必须至少包含所有的Key列，同时指定需要更新的列。如果插入行Key列的值在原表中存在，则将更新具有相同key列值那一行的数据。如果插入行Key列的值在原表中不存在，则将向表中插入一条新的数据，此时insert语句中没有指定的列必须有默认值或可以为null，这些缺失列会首先尝试用默认值填充，如果该列没有默认值，则尝试使用null值填充，如果该列不能为null，则本次插入失败。', '需要注意的是，控制insert语句是否开启严格模式的会话变量`enable_insert_strict`的默认值为true，即insert语句默认开启严格模式，而在严格模式下进行部分列更新不允许更新不存在的key。所以，在使用insert语句进行部分列更新的时候如果希望能插入不存在的key，需要在`enable_unique_key_partial_update`设置为true的基础上同时将`enable_insert_strict`设置为false。\n\n注意：\n\n当前执行 `INSERT` 语句时，对于有不符合目标表格式的数据，默认的行为是过滤，比如字符串超长等。但是对于有要求数据不能够被过滤的业务场景，可以通过设置会话变量 `enable_insert_strict` 为 `true` 来确保当有数据被过滤掉的时候，`INSERT` 不会被执行成功。\n\n### Example\n\n`test` 表包含两个列`c1`, `c2`。\n\n1. 向`test`表中导入一行数据\n\n```sql\nINSERT INTO test VALUES (1, 2);\nINSERT INTO test (c1, c2) VALUES (1, 2);\nINSERT INTO test (c1, c2) VALUES (1, DEFAULT);\nINSERT INTO test (c1) VALUES (1);\n```\n\n其中第一条、第二条语句是一样的效果。在不指定目标列时，使用表中的列顺序来作为默认的目标列。\n第三条、第四条语句表达的意思是一样的，使用`c2`列的默认值，来完成数据导入。\n\n2. 向`test`表中一次性导入多行数据\n\n```sql\nINSERT INTO test VALUES (1, 2), (3, 2 + 2);\nINSERT INTO test (c1, c2) VALUES (1, 2), (3, 2 * 2);\nINSERT INTO test (c1) VALUES (1), (3);\nINSERT INTO test (c1, c2) VALUES (1, DEFAULT), (3, DEFAULT);\n```\n\n其中第一条、第二条语句效果一样，向`test`表中一次性导入两条数据\n第三条、第四条语句效果已知，使用`c2`列的默认值向`test`表中导入两条数据']","根据已知内容，`enable_unique_key_partial_update` 参数是用于控制是否可以在 `INSERT INTO` 语句中进行部分列更新的。这个参数的默认值是 `false`，意味着在没有明确设置这个参数为 `true` 的情况下，是不允许使用 `INSERT INTO` 语句进行部分列更新的。

在已知内容中，并没有明确说明这个参数是否可以设置为全局。通常，这种参数更多的是在会话级别进行设置，以影响当前会话中的 `INSERT INTO` 操作。如果您需要为所有会话设置这个参数，可能需要通过系统配置或者管理员权限来完成。

建议您在实际操作中，根据 Doris 的具体使用文档或者咨询 Doris 社区以获取最准确的答案。如果需要进行全局设置，可能需要通过 Doris 的配置文件或者管理员接口来实现。"
8c365c24-c164-11ee-80a5-b026288862a5,"大佬，doris可以把map结构的数据，里面的k和v转化为 列吗,",",可以","['``` ### keywords CASE --- { ""title"": ""IFNULL"", ""language"": ""zh-CN"" } --- <!--split--> ## ifnull ### description #### Syntax `ifnull(expr1, expr2)` 如果 expr1 的值不为 NULL 则返回 expr1，否则返回 expr2 ### example\n``` mysql> select ifnull(1,0); +--------------+ | ifnull(1, 0) | +--------------+ | 1 | +--------------+ mysql> select ifnull(null,10); +------------------+ | ifnull(NULL, 10) | +------------------+ | 10 | +------------------+\n``` ### keywords IFNULL --- { ""title"": ""IF"", ""language"": ""zh-CN"" } --- <!--split--> ## if ### description #### Syntax `if(boolean condition, type valueTrue, type valueFalseOrNull)`\n如果表达式 condition 成立，返回结果 valueTrue；否则，返回结果 valueFalseOrNull 返回类型： valueTrue 表达式结果的类型 ### example\n``` mysql> select user_id, if(user_id = 1, ""true"", ""false"") test_if from test; +---------+---------+ | user_id | test_if | +---------+---------+ | 1 | true | | 2 | false | +---------+---------+\n``` ### keywords IF --- { ""title"": ""NVL"", ""language"": ""zh-CN"" } --- <!--split--> ## nvl <version since=""1.2.0""> nvl </version> ### description #### Syntax `nvl(expr1, expr2)` 如果 expr1 的值不为 NULL 则返回 expr1，否则返回 expr2 ### example\n``` mysql> select nvl(1,0); +--------------+ | nvl(1, 0) | +--------------+ | 1 | +--------------+ mysql> select nvl(null,10); +------------------+ | nvl(NULL, 10) | +------------------+ | 10 | +------------------+\n``` ### keywords NVL --- { ""title"": ""COALESCE"", ""language"": ""zh-CN"" } --- <!--split--> ## coalesce ### description #### Syntax `coalesce(expr1, expr2, ...., expr_n))` 返回参数中的第一个非空表达式（从左向右） ### example\n``` mysql> select coalesce(NULL, \'1111\', \'0000\'); +--------------------------------+ | coalesce(NULL, \'1111\', \'0000\') | +--------------------------------+ | 1111 | +--------------------------------+\n``` ### keywords COALESCE --- { ""title"": ""VARCHAR"", ""language"": ""zh-CN"" } --- <!--split--> ## VARCHAR ### description VARCHAR(M) 变长字符串，M代表的是变长字符串的字节长度。M的范围是1-65533。 注意：变长字符串是以UTF-8编码存储的，因此通常英文字符占1个字节，中文字符占3个字节。 ### keywords\nVARCHAR --- { ""title"": ""MAP"", ""language"": ""zh-CN"" } --- <!--split--> ## MAP ### name <version since=""2.0.0""> MAP </version> ### description `MAP<K, V>` 由K, V类型元素组成的map，不能作为key列使用。目前支持在Duplicate，Unique 模型的表中使用。 K,V 支持的类型有：', 'Doris 可以在导入语句中支持非常丰富的列转换和过滤操作。支持绝大多数内置函数和 UDF。关于如何正确的使用这个功能，可参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。 5. 错误数据过滤\nDoris 的导入任务可以容忍一部分格式错误的数据。容忍了通过 `max_filter_ratio` 设置。默认为0，即表示当有一条错误数据时，整个导入任务将会失败。如果用户希望忽略部分有问题的数据行，可以将次参数设置为 0~1 之间的数值，Doris 会自动跳过哪些数据格式不正确的行。 关于容忍率的一些计算方式，可以参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。\n6. 严格模式 `strict_mode` 属性用于设置导入任务是否运行在严格模式下。该格式会对列映射、转换和过滤的结果产生影响。关于严格模式的具体说明，可参阅 [严格模式](../../../../data-operate/import/import-scenes/load-strict-mode.md) 文档。 7. 超时时间 Broker Load 的默认超时时间为 4 小时。从任务提交开始算起。如果在超时时间内没有完成，则任务会失败。 8. 数据量和任务数限制\nBroker Load 适合在一个导入任务中导入100GB以内的数据。虽然理论上在一个导入任务中导入的数据量没有上限。但是提交过大的导入会导致运行时间较长，并且失败后重试的代价也会增加。 同时受限于集群规模，我们限制了导入的最大数据量为 ComputeNode 节点数 * 3GB。以保证系统资源的合理利用。如果有大数据量需要导入，建议分成多个导入任务提交。\nDoris 同时会限制集群内同时运行的导入任务数量，通常在 3-10 个不等。之后提交的导入作业会排队等待。队列最大长度为 100。之后的提交会直接拒绝。注意排队时间也被计算到了作业总时间中。如果超时，则作业会被取消。所以建议通过监控作业运行状态来合理控制作业提交频率。 --- { ""title"": ""CREATE-SYNC-JOB"", ""language"": ""zh-CN"" } --- <!--split--> ## CREATE-SYNC-JOB ### Name\nCREATE SYNC JOB ### Description 数据同步(Sync Job)功能，支持用户提交一个常驻的数据同步作业，通过从指定的远端地址读取Binlog日志，增量同步用户在Mysql数据库的对数据更新操作的CDC(Change Data Capture)功能。 目前数据同步作业只支持对接Canal，从Canal Server上获取解析好的Binlog数据，导入到Doris内。\n用户可通过 [SHOW SYNC JOB](../../Show-Statements/SHOW-SYNC-JOB.md) 查看数据同步作业状态。 语法： ```sql CREATE SYNC [db.]job_name ( channel_desc, channel_desc ... ) binlog_desc\n``` 1. `job_name` 同步作业名称，是作业在当前数据库内的唯一标识，相同`job_name`的作业只能有一个在运行。 2. `channel_desc` 作业下的数据通道，用来描述mysql源表到doris目标表的映射关系。 语法： ```sql FROM mysql_db.src_tbl INTO des_tbl [columns_mapping]\n``` 1. `mysql_db.src_tbl` 指定mysql端的数据库和源表。 2. `des_tbl` 指定doris端的目标表，只支持Unique表，且需开启表的batch delete功能(开启方法请看help alter table的\'批量删除功能\')。 4. `column_mapping`\n指定mysql源表和doris目标表的列之间的映射关系。如果不指定，FE会默认源表和目标表的列按顺序一一对应。 不支持 col_name = expr 的形式表示列。 示例：\n``` 假设目标表列为(k1, k2, v1)， 改变列k1和k2的顺序 (k2, k1, v1) 忽略源数据的第四列 (k2, k1, v1, dummy_column)', '- DATE 转换成 DATETIME(时分秒自动补零， 例如: `2019-12-09` <--> `2019-12-09 00:00:00`) - FLOAT 转换成 DOUBLE - INT 转换成 DATE (如果INT类型数据不合法则转换失败，原始数据不变) - 除DATE与DATETIME以外都可以转换成STRING，但是STRING不能转换任何其他类型\n5. 对指定 index 的列进行重新排序 语法： ```sql ORDER BY (column_name1, column_name2, ...) [FROM rollup_index_name] [PROPERTIES (""key""=""value"", ...)]\n``` 注意： - index 中的所有列都要写出来 - value 列在 key 列之后 ### Example 1. 向 example_rollup_index 的 col1 后添加一个key列 new_col(非聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN new_col INT KEY DEFAULT ""0"" AFTER col1 TO example_rollup_index;\n``` 2. 向example_rollup_index的col1后添加一个value列new_col(非聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN new_col INT DEFAULT ""0"" AFTER col1 TO example_rollup_index;\n``` 3. 向example_rollup_index的col1后添加一个key列new_col(聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN new_col INT DEFAULT ""0"" AFTER col1 TO example_rollup_index;\n``` 4. 向example_rollup_index的col1后添加一个value列new_col SUM聚合类型(聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN new_col INT SUM DEFAULT ""0"" AFTER col1 TO example_rollup_index;\n``` 5. 向 example_rollup_index 添加多列(聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN (col1 INT DEFAULT ""1"", col2 FLOAT SUM DEFAULT ""2.3"") TO example_rollup_index;\n``` 6. 从 example_rollup_index 删除一列 ```sql ALTER TABLE example_db.my_table DROP COLUMN col2 FROM example_rollup_index;\n``` 7. 修改 base index 的 key 列 col1 的类型为 BIGINT，并移动到 col2 列后面。 ```sql ALTER TABLE example_db.my_table MODIFY COLUMN col1 BIGINT KEY DEFAULT ""1"" AFTER col2;\n``` 注意：无论是修改 key 列还是 value 列都需要声明完整的 column 信息 8. 修改 base index 的 val1 列最大长度。原 val1 为 (val1 VARCHAR(32) REPLACE DEFAULT ""abc"") ```sql ALTER TABLE example_db.my_table MODIFY COLUMN val1 VARCHAR(64) REPLACE DEFAULT ""abc"";\n``` 注意：只能修改列的类型，列的其他属性维持原样 9. 重新排序 example_rollup_index 中的列（设原列顺序为：k1,k2,k3,v1,v2） ```sql ALTER TABLE example_db.my_table ORDER BY (k3,k1,k2,v2,v1) FROM example_rollup_index;']",信息是缺失在如何将Doris中的MAP结构数据转化为列的具体细节上。Doris支持MAP类型以及在导入过程中进行列的映射、转换与过滤，但是基于提供的上下文，没有具体说明如何直接将MAP类型的键值对转换成独立的列。通常，这种转换需要在数据处理或查询时进行，可能涉及到使用Doris的内置函数或者在数据导入前后使用外部脚本或程序进行转换。如果你需要针对MAP类型进行特定的列转换，建议查阅Doris的官方文档或社区资源，以获取最新和最详细的指导。,14.4,"['``` ### keywords CASE --- { ""title"": ""IFNULL"", ""language"": ""zh-CN"" } --- <!--split--> ## ifnull ### description #### Syntax `ifnull(expr1, expr2)` 如果 expr1 的值不为 NULL 则返回 expr1，否则返回 expr2 ### example\n``` mysql> select ifnull(1,0); +--------------+ | ifnull(1, 0) | +--------------+ | 1 | +--------------+ mysql> select ifnull(null,10); +------------------+ | ifnull(NULL, 10) | +------------------+ | 10 | +------------------+\n``` ### keywords IFNULL --- { ""title"": ""IF"", ""language"": ""zh-CN"" } --- <!--split--> ## if ### description #### Syntax `if(boolean condition, type valueTrue, type valueFalseOrNull)`\n如果表达式 condition 成立，返回结果 valueTrue；否则，返回结果 valueFalseOrNull 返回类型： valueTrue 表达式结果的类型 ### example\n``` mysql> select user_id, if(user_id = 1, ""true"", ""false"") test_if from test; +---------+---------+ | user_id | test_if | +---------+---------+ | 1 | true | | 2 | false | +---------+---------+\n``` ### keywords IF --- { ""title"": ""NVL"", ""language"": ""zh-CN"" } --- <!--split--> ## nvl <version since=""1.2.0""> nvl </version> ### description #### Syntax `nvl(expr1, expr2)` 如果 expr1 的值不为 NULL 则返回 expr1，否则返回 expr2 ### example\n``` mysql> select nvl(1,0); +--------------+ | nvl(1, 0) | +--------------+ | 1 | +--------------+ mysql> select nvl(null,10); +------------------+ | nvl(NULL, 10) | +------------------+ | 10 | +------------------+\n``` ### keywords NVL --- { ""title"": ""COALESCE"", ""language"": ""zh-CN"" } --- <!--split--> ## coalesce ### description #### Syntax `coalesce(expr1, expr2, ...., expr_n))` 返回参数中的第一个非空表达式（从左向右） ### example\n``` mysql> select coalesce(NULL, \'1111\', \'0000\'); +--------------------------------+ | coalesce(NULL, \'1111\', \'0000\') | +--------------------------------+ | 1111 | +--------------------------------+\n``` ### keywords COALESCE --- { ""title"": ""VARCHAR"", ""language"": ""zh-CN"" } --- <!--split--> ## VARCHAR ### description VARCHAR(M) 变长字符串，M代表的是变长字符串的字节长度。M的范围是1-65533。 注意：变长字符串是以UTF-8编码存储的，因此通常英文字符占1个字节，中文字符占3个字节。 ### keywords\nVARCHAR --- { ""title"": ""MAP"", ""language"": ""zh-CN"" } --- <!--split--> ## MAP ### name <version since=""2.0.0""> MAP </version> ### description `MAP<K, V>` 由K, V类型元素组成的map，不能作为key列使用。目前支持在Duplicate，Unique 模型的表中使用。 K,V 支持的类型有：', 'Doris 可以在导入语句中支持非常丰富的列转换和过滤操作。支持绝大多数内置函数和 UDF。关于如何正确的使用这个功能，可参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。 5. 错误数据过滤\nDoris 的导入任务可以容忍一部分格式错误的数据。容忍了通过 `max_filter_ratio` 设置。默认为0，即表示当有一条错误数据时，整个导入任务将会失败。如果用户希望忽略部分有问题的数据行，可以将次参数设置为 0~1 之间的数值，Doris 会自动跳过哪些数据格式不正确的行。 关于容忍率的一些计算方式，可以参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。\n6. 严格模式 `strict_mode` 属性用于设置导入任务是否运行在严格模式下。该格式会对列映射、转换和过滤的结果产生影响。关于严格模式的具体说明，可参阅 [严格模式](../../../../data-operate/import/import-scenes/load-strict-mode.md) 文档。 7. 超时时间 Broker Load 的默认超时时间为 4 小时。从任务提交开始算起。如果在超时时间内没有完成，则任务会失败。 8. 数据量和任务数限制\nBroker Load 适合在一个导入任务中导入100GB以内的数据。虽然理论上在一个导入任务中导入的数据量没有上限。但是提交过大的导入会导致运行时间较长，并且失败后重试的代价也会增加。 同时受限于集群规模，我们限制了导入的最大数据量为 ComputeNode 节点数 * 3GB。以保证系统资源的合理利用。如果有大数据量需要导入，建议分成多个导入任务提交。\nDoris 同时会限制集群内同时运行的导入任务数量，通常在 3-10 个不等。之后提交的导入作业会排队等待。队列最大长度为 100。之后的提交会直接拒绝。注意排队时间也被计算到了作业总时间中。如果超时，则作业会被取消。所以建议通过监控作业运行状态来合理控制作业提交频率。 --- { ""title"": ""CREATE-SYNC-JOB"", ""language"": ""zh-CN"" } --- <!--split--> ## CREATE-SYNC-JOB ### Name\nCREATE SYNC JOB ### Description 数据同步(Sync Job)功能，支持用户提交一个常驻的数据同步作业，通过从指定的远端地址读取Binlog日志，增量同步用户在Mysql数据库的对数据更新操作的CDC(Change Data Capture)功能。 目前数据同步作业只支持对接Canal，从Canal Server上获取解析好的Binlog数据，导入到Doris内。\n用户可通过 [SHOW SYNC JOB](../../Show-Statements/SHOW-SYNC-JOB.md) 查看数据同步作业状态。 语法： ```sql CREATE SYNC [db.]job_name ( channel_desc, channel_desc ... ) binlog_desc\n``` 1. `job_name` 同步作业名称，是作业在当前数据库内的唯一标识，相同`job_name`的作业只能有一个在运行。 2. `channel_desc` 作业下的数据通道，用来描述mysql源表到doris目标表的映射关系。 语法： ```sql FROM mysql_db.src_tbl INTO des_tbl [columns_mapping]\n``` 1. `mysql_db.src_tbl` 指定mysql端的数据库和源表。 2. `des_tbl` 指定doris端的目标表，只支持Unique表，且需开启表的batch delete功能(开启方法请看help alter table的\'批量删除功能\')。 4. `column_mapping`\n指定mysql源表和doris目标表的列之间的映射关系。如果不指定，FE会默认源表和目标表的列按顺序一一对应。 不支持 col_name = expr 的形式表示列。 示例：\n``` 假设目标表列为(k1, k2, v1)， 改变列k1和k2的顺序 (k2, k1, v1) 忽略源数据的第四列 (k2, k1, v1, dummy_column)', '- DATE 转换成 DATETIME(时分秒自动补零， 例如: `2019-12-09` <--> `2019-12-09 00:00:00`) - FLOAT 转换成 DOUBLE - INT 转换成 DATE (如果INT类型数据不合法则转换失败，原始数据不变) - 除DATE与DATETIME以外都可以转换成STRING，但是STRING不能转换任何其他类型\n5. 对指定 index 的列进行重新排序 语法： ```sql ORDER BY (column_name1, column_name2, ...) [FROM rollup_index_name] [PROPERTIES (""key""=""value"", ...)]\n``` 注意： - index 中的所有列都要写出来 - value 列在 key 列之后 ### Example 1. 向 example_rollup_index 的 col1 后添加一个key列 new_col(非聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN new_col INT KEY DEFAULT ""0"" AFTER col1 TO example_rollup_index;\n``` 2. 向example_rollup_index的col1后添加一个value列new_col(非聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN new_col INT DEFAULT ""0"" AFTER col1 TO example_rollup_index;\n``` 3. 向example_rollup_index的col1后添加一个key列new_col(聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN new_col INT DEFAULT ""0"" AFTER col1 TO example_rollup_index;\n``` 4. 向example_rollup_index的col1后添加一个value列new_col SUM聚合类型(聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN new_col INT SUM DEFAULT ""0"" AFTER col1 TO example_rollup_index;\n``` 5. 向 example_rollup_index 添加多列(聚合模型) ```sql ALTER TABLE example_db.my_table ADD COLUMN (col1 INT DEFAULT ""1"", col2 FLOAT SUM DEFAULT ""2.3"") TO example_rollup_index;\n``` 6. 从 example_rollup_index 删除一列 ```sql ALTER TABLE example_db.my_table DROP COLUMN col2 FROM example_rollup_index;\n``` 7. 修改 base index 的 key 列 col1 的类型为 BIGINT，并移动到 col2 列后面。 ```sql ALTER TABLE example_db.my_table MODIFY COLUMN col1 BIGINT KEY DEFAULT ""1"" AFTER col2;\n``` 注意：无论是修改 key 列还是 value 列都需要声明完整的 column 信息 8. 修改 base index 的 val1 列最大长度。原 val1 为 (val1 VARCHAR(32) REPLACE DEFAULT ""abc"") ```sql ALTER TABLE example_db.my_table MODIFY COLUMN val1 VARCHAR(64) REPLACE DEFAULT ""abc"";\n``` 注意：只能修改列的类型，列的其他属性维持原样 9. 重新排序 example_rollup_index 中的列（设原列顺序为：k1,k2,k3,v1,v2） ```sql ALTER TABLE example_db.my_table ORDER BY (k3,k1,k2,v2,v1) FROM example_rollup_index;']",信息是关于Doris处理MAP结构数据，将其转换为列的能力上缺失。,"['则列映射的书写顺序应如下：\n\n   ```text\n   (tmpk1, k2, tmpk3, k4, k1 = ifnull(tmpk1, 0), k3 = case tmpk3 when ""beijing"" then 1 when ""shanghai"" then 2 when ""guangzhou"" then 3 when ""chongqing"" then 4 else null end)\n   ```\n\n   最终表中的数据如下：\n\n   | k1   | k2   | k3   | k4   |\n   | ---- | ---- | ---- | ---- |\n   | 1    | 100  | 1    | 1.1  |\n   | 2    | 200  | 2    | 1.2  |\n   | 3    | 300  | 3    | 1.3  |\n   | 0    | 400  | 4    | 1.4  |\n\n## 列过滤\n\n经过列映射和转换后，我们可以通过过滤条件将不希望导入到Doris中的数据进行过滤。下面我们通过示例说明：\n\n假设源文件有4列，内容如下（表头列名仅为方便表述，实际并无表头）：\n\n| 列1  | 列2  | 列3       | 列4  |\n| ---- | ---- | --------- | ---- |\n| 1    | 100  | beijing   | 1.1  |\n| 2    | 200  | shanghai  | 1.2  |\n| 3    | 300  | guangzhou | 1.3  |\n| \\N   | 400  | chongqing | 1.4  |\n\n1. 在列映射和转换缺省的情况下，直接过滤\n\n   假设表中有 `k1,k2,k3,k4` 4列。我们可以在缺省列映射和转换的情况下，直接定义过滤条件。如我们希望只导入源文件中第4列为大于 1.2 的数据行，则过滤条件如下：\n\n   ```text\n   where k4 > 1.2\n   ```\n\n   最终表中的数据如下：', '```text\n   where k4 > 1.2\n   ```\n\n   最终表中的数据如下：\n\n   | k1   | k2   | k3        | k4   |\n   | ---- | ---- | --------- | ---- |\n   | 3    | 300  | guangzhou | 1.3  |\n   | null | 400  | chongqing | 1.4  |\n\n   缺省情况下，Doris 会按照顺序进行列映射，因此源文件中的第4列自动被映射到表中的 `k4` 列。\n\n2. 对经过列转换的数据进行过滤\n\n   假设表中有 `k1,k2,k3,k4` 4列。在 **列转换** 示例中，我们将省份名称转换成了id。这里我们想过滤掉 id 为 3 的数据。则转换、过滤条件如下：\n\n   ```text\n   (k1, k2, tmpk3, k4, k3 = case tmpk3 when ""beijing"" then 1 when ""shanghai"" then 2 when ""guangzhou"" then 3 when ""chongqing"" then 4 else null end)\n   where k3 != 3\n   ```\n\n   最终表中的数据如下：\n\n   | k1   | k2   | k3   | k4   |\n   | ---- | ---- | ---- | ---- |\n   | 1    | 100  | 1    | 1.1  |\n   | 2    | 200  | 2    | 1.2  |\n   | null | 400  | 4    | 1.4  |\n\n   这里我们看到，执行过滤时的列值，为经过映射和转换后的最终列值，而不是原始数据。\n\n3. 多条件过滤\n\n   假设表中有 `k1,k2,k3,k4` 4列。我们想过滤掉 `k1` 列为 `null` 的数据，同时过滤掉 `k4` 列小于 1.2 的数据，则过滤条件如下：\n\n   ```text\n   where k1 is not null and k4 >= 1.2\n   ```\n\n   最终表中的数据如下：', '## 全局字典\n\n### 适用场景\n\n目前 Doris 中 Bitmap 列是使用类库 `Roaringbitmap` 实现的，而 `Roaringbitmap` 的输入数据类型只能是整型，因此如果要在导入流程中实现对于 Bitmap 列的预计算，那么就需要将输入数据的类型转换成整型。\n\n在 Doris 现有的导入流程中，全局字典的数据结构是基于 Hive 表实现的，保存了原始值到编码值的映射。\n\n### 构建流程\n\n1. 读取上游数据源的数据，生成一张 Hive 临时表，记为 `hive_table`。\n2. 从 `hive_table `中抽取待去重字段的去重值，生成一张新的 Hive 表，记为 `distinct_value_table`。\n3. 新建一张全局字典表，记为 `dict_table` ，一列为原始值，一列为编码后的值。\n4. 将 `distinct_value_table` 与 `dict_table` 做 Left Join，计算出新增的去重值集合，然后对这个集合使用窗口函数进行编码，此时去重列原始值就多了一列编码后的值，最后将这两列的数据写回 `dict_table`。\n5. 将 `dict_table `与 `hive_table` 进行 Join，完成 `hive_table` 中原始值替换成整型编码值的工作。\n6. `hive_table `会被下一步数据预处理的流程所读取，经过计算后导入到 Doris 中。\n\n## 数据预处理（DPP）\n\n### 基本流程\n\n1. 从数据源读取数据，上游数据源可以是 HDFS 文件，也可以是 Hive 表。\n2. 对读取到的数据进行字段映射，表达式计算以及根据分区信息生成分桶字段 `bucket_id`。\n3. 根据 Doris 表的 Rollup 元数据生成 RollupTree。\n4. 遍历 RollupTree，进行分层的聚合操作，下一个层级的 Rollup 可以由上一个层的 Rollup 计算得来。\n5. 每次完成聚合计算后，会对数据根据 `bucket_id `进行分桶然后写入 HDFS 中。\n6. 后续 Broker 会拉取 HDFS 中的文件然后导入 Doris Be 中。\n\n## Hive Bitmap UDF', 'COALESCE\n---\n{\n    ""title"": ""VARCHAR"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## VARCHAR\n### description\n    VARCHAR(M)\n    变长字符串，M代表的是变长字符串的字节长度。M的范围是1-65533。\n    \n    注意：变长字符串是以UTF-8编码存储的，因此通常英文字符占1个字节，中文字符占3个字节。\n\n### keywords\n\n    VARCHAR\n---\n{\n""title"": ""MAP"",\n""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## MAP\n\n### name\n\n<version since=""2.0.0"">\n\nMAP\n\n</version>\n\n### description\n\n`MAP<K, V>`\n\n由K, V类型元素组成的map，不能作为key列使用。目前支持在Duplicate，Unique 模型的表中使用。\n\nK,V 支持的类型有：\n\n```\nBOOLEAN, TINYINT, SMALLINT, INT, BIGINT, LARGEINT, FLOAT, DOUBLE, DECIMAL, DECIMALV3, DATE,\nDATEV2, DATETIME, DATETIMEV2, CHAR, VARCHAR, STRING\n```\n\n### example\n\n建表示例如下：', '同步作业名称，是作业在当前数据库内的唯一标识，相同`job_name`的作业只能有一个在运行。\n\n2. `channel_desc`\n\n   作业下的数据通道，用来描述mysql源表到doris目标表的映射关系。\n\n   语法：\n\n   ```sql\n   FROM mysql_db.src_tbl INTO des_tbl\n   [columns_mapping]\n   ```\n   \n   1. `mysql_db.src_tbl`\n   \n      指定mysql端的数据库和源表。\n   \n   2. `des_tbl`\n   \n      指定doris端的目标表，只支持Unique表，且需开启表的batch delete功能(开启方法请看help alter table的\'批量删除功能\')。\n   \n   4. `column_mapping`\n   \n      指定mysql源表和doris目标表的列之间的映射关系。如果不指定，FE会默认源表和目标表的列按顺序一一对应。\n   \n      不支持 col_name = expr 的形式表示列。\n   \n      示例：\n   \n      ```\n      假设目标表列为(k1, k2, v1)，\n      \n      改变列k1和k2的顺序\n      (k2, k1, v1)\n      \n      忽略源数据的第四列\n      (k2, k1, v1, dummy_column)\n      ```\n   \n3. `binlog_desc`\n\n   用来描述远端数据源，目前仅支持canal一种。\n\n   语法：\n\n   ```sql\n   FROM BINLOG\n   (\n       ""key1"" = ""value1"",\n       ""key2"" = ""value2""\n   )\n   ```\n\n   1. Canal 数据源对应的属性，以`canal.`为前缀', '```\n### keywords\nWIDTH_BUCKET---\n{\n    ""title"": ""索引概述"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n# 索引概述\n\n索引用于帮助快速过滤或查找数据。\n\n目前 Doris 主要支持两类索引：\n1. 内建的智能索引，包括前缀索引和 ZoneMap 索引。\n2. 用户手动创建的二级索引，包括 [倒排索引](./inverted-index.md)、 [bloomfilter索引](./bloomfilter.md)、 [ngram bloomfilter索引](./ngram-bloomfilter-index.md) 和[bitmap索引](./bitmap-index.md)。\n\n其中 ZoneMap 索引是在列存格式上，对每一列自动维护的索引信息，包括 Min/Max，Null 值个数等等。这种索引对用户透明。\n\n## 前缀索引\n\n不同于传统的数据库设计，Doris 不支持在任意列上创建索引。Doris 这类 MPP 架构的 OLAP 数据库，通常都是通过提高并发，来处理大量数据的。\n\n本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作为条件进行查找，会非常的高效。\n\n在 Aggregate、Unique 和 Duplicate 三种数据模型中。底层的数据存储，是按照各自建表语句中，AGGREGATE KEY、UNIQUE KEY 和 DUPLICATE KEY 中指定的列进行排序存储的。\n\n而前缀索引，即在排序的基础上，实现的一种根据给定前缀列，快速查询数据的索引方式。\n\n## 示例\n\n我们将一行数据的前 **36 个字节** 作为这行数据的前缀索引。当遇到 VARCHAR 类型时，前缀索引会直接截断。我们举例说明：\n\n1. 以下表结构的前缀索引为 user_id(8 Bytes) + age(4 Bytes) + message(prefix 20 Bytes)。', '<!--split-->\n\n## 概述\nDoris 的异步物化视图采用了基于 SPJG（SELECT-PROJECT-JOIN-GROUP-BY）模式的结构信息来进行透明改写的算法。\n\nDoris 可以分析查询 SQL 的结构信息，自动寻找满足要求的物化视图，并尝试进行透明改写，使用物化视图来表达查询SQL。\n\n通过使用预计算的物化视图结果，可以大幅提高查询性能，减少计算成本。', ""在 Doris 中，数据都以表（Table）的形式进行逻辑上的描述。\n\n### Row & Column\n\n一张表包括行（Row）和列（Column）：\n\n- Row：即用户的一行数据；\n\n- Column： 用于描述一行数据中不同的字段。\n\n  Column 可以分为两大类：Key 和 Value。从业务角度看，Key 和 Value 可以分别对应维度列和指标列。Doris的key列是建表语句中指定的列，建表语句中的关键字'unique key'或'aggregate key'或'duplicate key'后面的列就是key列，除了key列剩下的就是value列。从聚合模型的角度来说，Key 列相同的行，会聚合成一行。其中 Value 列的聚合方式由用户在建表时指定。关于更多聚合模型的介绍，可以参阅 [Doris 数据模型](data-model.md)。\n\n### Tablet & Partition\n\n在 Doris 的存储引擎中，用户数据被水平划分为若干个数据分片（Tablet，也称作数据分桶）。每个 Tablet 包含若干数据行。各个 Tablet 之间的数据没有交集，并且在物理上是独立存储的。\n\n多个 Tablet 在逻辑上归属于不同的分区（Partition）。一个 Tablet 只属于一个 Partition。而一个 Partition 包含若干个 Tablet。因为 Tablet 在物理上是独立存储的，所以可以视为 Partition 在物理上也是独立。Tablet 是数据移动、复制等操作的最小物理存储单元。\n\n若干个 Partition 组成一个 Table。Partition 可以视为是逻辑上最小的管理单元。数据的导入与删除，仅能针对一个 Partition 进行。\n\n## 数据划分\n\n我们以一个建表操作来说明 Doris 的数据划分。\n\nDoris 的建表是一个同步命令，SQL执行完成即返回结果，命令返回成功即表示建表成功。具体建表语法可以参考[CREATE TABLE](../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE.md)，也可以通过 `HELP CREATE TABLE;` 查看更多帮助。\n\n本小节通过一个例子，来介绍 Doris 的建表方式。"", ""<!--split-->\n\n# 数据模型\n\n本文档主要从逻辑层面，描述 Doris 的数据模型，以帮助用户更好的使用 Doris 应对不同的业务场景。\n\n## 基本概念\n\n在 Doris 中，数据以表（Table）的形式进行逻辑上的描述。\n一张表包括行（Row）和列（Column）。Row 即用户的一行数据。Column 用于描述一行数据中不同的字段。\n\nColumn 可以分为两大类：Key 和 Value。从业务角度看，Key 和 Value 可以分别对应维度列和指标列。Doris的key列是建表语句中指定的列，建表语句中的关键字'unique key'或'aggregate key'或'duplicate key'后面的列就是 Key 列，除了 Key 列剩下的就是 Value 列。\n\nDoris 的数据模型主要分为3类:\n\n- Aggregate\n- Unique\n- Duplicate\n\n下面我们分别介绍。\n\n## Aggregate 模型\n\n我们以实际的例子来说明什么是聚合模型，以及如何正确的使用聚合模型。\n\n### 示例1：导入数据聚合\n\n假设业务有如下数据表模式：""]","Doris 目前支持将 Map 类型的数据中的 K 和 V 转化为列。在使用 Map 类型的列时，需要注意以下几点：

1. Doris 中的 Map 类型是由 K, V 类型元素组成的，它不能作为键列使用。
2. 目前，Map 类型的支持在 Duplicate 和 Unique 模型的表中。
3. K, V 支持的类型有：BOOLEAN, TINYINT, SMALLINT, INT, BIGINT, LARGEINT, FLOAT, DOUBLE, DECIMAL, DECIMALV3, DATE, DATEV2, DATETIME, DATETIMEV2, CHAR, VARCHAR, STRING。

在实际应用中，你可以将 Map 类型的数据作为一列导入到 Doris 中，然后在查询时根据需要使用这个 Map 列。"
9b70eb82-c18a-11ee-8011-b026288862a5,"我之前json数据我创建为text了，现在想要修改为json类型的，发现报错，不知道怎么回事,",",Doris Schema Change 只有在表数据完整且非均衡状态下才可以开始。如果表的某些数据分片副本不完整，或者某些副本正在进行均衡操作，则提交会被拒绝。","['MySQL> select json_array(""a"", null, ""c""); +------------------------------+ | json_array(\'a\', \'NULL\', \'c\') | +------------------------------+ | [""a"", NULL, ""c""] | +------------------------------+\n``` ### keywords json,array,json_array --- { ""title"": ""JSON_EXTRACT"", ""language"": ""zh-CN"" } --- <!--split--> ## json_extract <version since=""dev""></version> ### description #### Syntax\n```sql VARCHAR json_extract(VARCHAR json_str, VARCHAR path[, VARCHAR path] ...) JSON jsonb_extract(JSON j, VARCHAR json_path) BOOLEAN json_extract_isnull(JSON j, VARCHAR json_path) BOOLEAN json_extract_bool(JSON j, VARCHAR json_path)\nINT json_extract_int(JSON j, VARCHAR json_path) BIGINT json_extract_bigint(JSON j, VARCHAR json_path) LARGEINT json_extract_largeint(JSON j, VARCHAR json_path) DOUBLE json_extract_double(JSON j, VARCHAR json_path)\nSTRING json_extract_string(JSON j, VARCHAR json_path)\n```\njson_extract是一系列函数，从JSON类型的数据中提取json_path指定的字段，根据要提取的字段类型不同提供不同的系列函数。 - json_extract对VARCHAR类型的json string返回VARCHAR类型 - jsonb_extract返回JSON类型 - json_extract_isnull返回是否为json null的BOOLEAN类型 - json_extract_bool返回BOOLEAN类型 - json_extract_int返回INT类型\n- json_extract_bigint返回BIGINT类型 - json_extract_largeint返回LARGEINT类型 - json_extract_double返回DOUBLE类型 - json_extract_STRING返回STRING类型\njson path的语法如下 - \'$\' 代表json root - \'.k1\' 代表json object中key为\'k1\'的元素 - 如果 key 列值包含 ""."", json_path 中需要用双引号，例如 SELECT json_extract(\'{""k1.a"":""abc"",""k2"":300}\', \'$.""k1.a""\'); - \'[i]\' 代表json array中下标为i的元素\n- 获取 json_array 的最后一个元素可以用\'$[last]\'，倒数第二个元素可以用\'$[last-1]\'，以此类推\n特殊情况处理如下： - 如果 json_path 指定的字段在JSON中不存在，返回NULL - 如果 json_path 指定的字段在JSON中的实际类型和json_extract_t指定的类型不一致，如果能无损转换成指定类型返回指定类型t，如果不能则返回NULL ### example 参考 [json tutorial](../../sql-reference/Data-Types/JSON.md) 中的示例', '18 {""k1"":""v31"", ""k2"": 300, ""a1"": [{""k1"":""v41"", ""k2"": 400}, 1, ""a"", 3.14]} 19 \'\' 20 \'abc\' 21 abc 22 100x 23 6.a8 24 {x 25 [123, abc]\n``` - 由于有28%的非法数据，默认会失败报错 ""too many filtered rows""\n``` curl --location-trusted -u root: -T test_json.csv http://127.0.0.1:8840/api/testdb/test_json/_stream_load { ""TxnId"": 12019, ""Label"": ""744d9821-9c9f-43dc-bf3b-7ab048f14e32"", ""TwoPhaseCommit"": ""false"", ""Status"": ""Fail"",\n""Message"": ""too many filtered rows"", ""NumberTotalRows"": 25, ""NumberLoadedRows"": 18, ""NumberFilteredRows"": 7, ""NumberUnselectedRows"": 0, ""LoadBytes"": 380, ""LoadTimeMs"": 48, ""BeginTxnTimeMs"": 0, ""StreamLoadPutTimeMs"": 1,\n""ReadDataTimeMs"": 0, ""WriteDataTimeMs"": 45, ""CommitAndPublishTimeMs"": 0, ""ErrorURL"": ""http://172.21.0.5:8840/api/_load_error_log?file=__shard_2/error_log_insert_stmt_95435c4bf5f156df-426735082a9296af_95435c4bf5f156df_426735082a9296af"" }\n``` - 设置容错率参数 \'max_filter_ratio: 0.3\'\n``` curl --location-trusted -u root: -H \'max_filter_ratio: 0.3\' -T test_json.csv http://127.0.0.1:8840/api/testdb/test_json/_stream_load { ""TxnId"": 12017, ""Label"": ""f37a50c1-43e9-4f4e-a159-a3db6abe2579"", ""TwoPhaseCommit"": ""false"",\n""Status"": ""Success"", ""Message"": ""OK"", ""NumberTotalRows"": 25, ""NumberLoadedRows"": 18, ""NumberFilteredRows"": 7, ""NumberUnselectedRows"": 0, ""LoadBytes"": 380, ""LoadTimeMs"": 68, ""BeginTxnTimeMs"": 0,\n""StreamLoadPutTimeMs"": 2, ""ReadDataTimeMs"": 0, ""WriteDataTimeMs"": 45, ""CommitAndPublishTimeMs"": 19,\n""ErrorURL"": ""http://172.21.0.5:8840/api/_load_error_log?file=__shard_0/error_log_insert_stmt_a1463f98a7b15caf-c79399b920f5bfa3_a1463f98a7b15caf_c79399b920f5bfa3"" }\n``` - 查看stream load导入的数据，JSON类型的列j会自动转成JSON string展示', '| 1 | NULL | NULL | | 2 | null | NULL |\n| 3 | true | NULL | | 4 | false | NULL |\n| 5 | 100 | NULL | | 6 | 10000 | NULL |\n| 7 | 1000000000 | NULL | | 8 | 1152921504606846976 | NULL |\n| 9 | 6.18 | NULL | | 10 | ""abcd"" | NULL |\n| 11 | {} | NULL | | 12 | {""k1"":""v31"",""k2"":300} | v31 |\n| 13 | [] | NULL | | 14 | [123,456] | NULL |\n| 15 | [""abc"",""def""] | NULL | | 16 | [null,true,false,100,6.18,""abc""] | NULL |\n| 17 | [{""k1"":""v41"",""k2"":400},1,""a"",3.14] | NULL | | 18 | {""k1"":""v31"",""k2"":300,""a1"":[{""k1"":""v41"",""k2"":400},1,""a"",3.14]} | v31 |\n| 26 | {""k1"":""v1"",""k2"":200} | v1 | +------+---------------------------------------------------------------+-----------------------------------+ 19 rows in set (0.03 sec)\n``` - json_extract_int 获取int类型字段，非int类型返回NULL']","在数据库中，将已存在的列的数据类型从一个类型修改为另一个类型，特别是从文本类型（如`TEXT`）修改为`JSON`类型，可能会遇到一些问题，这些问题通常与数据的兼容性和格式有关。如果您在尝试修改列的数据类型时遇到错误，可能是因为以下几个原因：

1. **数据格式不兼容**：确保您的文本列中的所有数据都是有效的JSON格式。任何非法的JSON格式（如缺少引号、花括号不匹配等）都会导致修改失败。
2. **数据库版本和支持**：不同的数据库系统对`JSON`数据类型的支持程度不同。确保您的数据库系统支持`JSON`数据类型，并且您的数据库版本是最新的，或者是支持该功能的版本。
3. **语法错误**：在修改列类型的SQL语句中，确保您使用的语法是正确的。不同的数据库系统可能有不同的语法要求。

为了解决这个问题，您可以尝试以下步骤：

- **验证JSON格式**：首先，检查您的文本列中的所有数据，确保每条记录都是有效的JSON字符串。您可以使用在线工具或编程语言中的JSON解析库来验证数据。
- **逐条转换**：如果数据量不大，您可以尝试编写一个脚本或手动处理，将每条文本数据转换为JSON格式，并更新到数据库中。
- **使用临时列**：在某些情况下，您可以添加一个新的`JSON`类型列到您的表中，然后将文本数据转换为`JSON`格式并复制到这个新列中。完成后，您可以删除原来的文本列，并将新列重命名。

如果上述步骤仍然无法解决您的问题，建议您查阅具体数据库的文档，了解关于修改列数据类型的详细信息，或者提供更具体的错误信息以便获得更准确的帮助。",42.73,"['MySQL> select json_array(""a"", null, ""c""); +------------------------------+ | json_array(\'a\', \'NULL\', \'c\') | +------------------------------+ | [""a"", NULL, ""c""] | +------------------------------+\n``` ### keywords json,array,json_array --- { ""title"": ""JSON_EXTRACT"", ""language"": ""zh-CN"" } --- <!--split--> ## json_extract <version since=""dev""></version> ### description #### Syntax\n```sql VARCHAR json_extract(VARCHAR json_str, VARCHAR path[, VARCHAR path] ...) JSON jsonb_extract(JSON j, VARCHAR json_path) BOOLEAN json_extract_isnull(JSON j, VARCHAR json_path) BOOLEAN json_extract_bool(JSON j, VARCHAR json_path)\nINT json_extract_int(JSON j, VARCHAR json_path) BIGINT json_extract_bigint(JSON j, VARCHAR json_path) LARGEINT json_extract_largeint(JSON j, VARCHAR json_path) DOUBLE json_extract_double(JSON j, VARCHAR json_path)\nSTRING json_extract_string(JSON j, VARCHAR json_path)\n```\njson_extract是一系列函数，从JSON类型的数据中提取json_path指定的字段，根据要提取的字段类型不同提供不同的系列函数。 - json_extract对VARCHAR类型的json string返回VARCHAR类型 - jsonb_extract返回JSON类型 - json_extract_isnull返回是否为json null的BOOLEAN类型 - json_extract_bool返回BOOLEAN类型 - json_extract_int返回INT类型\n- json_extract_bigint返回BIGINT类型 - json_extract_largeint返回LARGEINT类型 - json_extract_double返回DOUBLE类型 - json_extract_STRING返回STRING类型\njson path的语法如下 - \'$\' 代表json root - \'.k1\' 代表json object中key为\'k1\'的元素 - 如果 key 列值包含 ""."", json_path 中需要用双引号，例如 SELECT json_extract(\'{""k1.a"":""abc"",""k2"":300}\', \'$.""k1.a""\'); - \'[i]\' 代表json array中下标为i的元素\n- 获取 json_array 的最后一个元素可以用\'$[last]\'，倒数第二个元素可以用\'$[last-1]\'，以此类推\n特殊情况处理如下： - 如果 json_path 指定的字段在JSON中不存在，返回NULL - 如果 json_path 指定的字段在JSON中的实际类型和json_extract_t指定的类型不一致，如果能无损转换成指定类型返回指定类型t，如果不能则返回NULL ### example 参考 [json tutorial](../../sql-reference/Data-Types/JSON.md) 中的示例', '18 {""k1"":""v31"", ""k2"": 300, ""a1"": [{""k1"":""v41"", ""k2"": 400}, 1, ""a"", 3.14]} 19 \'\' 20 \'abc\' 21 abc 22 100x 23 6.a8 24 {x 25 [123, abc]\n``` - 由于有28%的非法数据，默认会失败报错 ""too many filtered rows""\n``` curl --location-trusted -u root: -T test_json.csv http://127.0.0.1:8840/api/testdb/test_json/_stream_load { ""TxnId"": 12019, ""Label"": ""744d9821-9c9f-43dc-bf3b-7ab048f14e32"", ""TwoPhaseCommit"": ""false"", ""Status"": ""Fail"",\n""Message"": ""too many filtered rows"", ""NumberTotalRows"": 25, ""NumberLoadedRows"": 18, ""NumberFilteredRows"": 7, ""NumberUnselectedRows"": 0, ""LoadBytes"": 380, ""LoadTimeMs"": 48, ""BeginTxnTimeMs"": 0, ""StreamLoadPutTimeMs"": 1,\n""ReadDataTimeMs"": 0, ""WriteDataTimeMs"": 45, ""CommitAndPublishTimeMs"": 0, ""ErrorURL"": ""http://172.21.0.5:8840/api/_load_error_log?file=__shard_2/error_log_insert_stmt_95435c4bf5f156df-426735082a9296af_95435c4bf5f156df_426735082a9296af"" }\n``` - 设置容错率参数 \'max_filter_ratio: 0.3\'\n``` curl --location-trusted -u root: -H \'max_filter_ratio: 0.3\' -T test_json.csv http://127.0.0.1:8840/api/testdb/test_json/_stream_load { ""TxnId"": 12017, ""Label"": ""f37a50c1-43e9-4f4e-a159-a3db6abe2579"", ""TwoPhaseCommit"": ""false"",\n""Status"": ""Success"", ""Message"": ""OK"", ""NumberTotalRows"": 25, ""NumberLoadedRows"": 18, ""NumberFilteredRows"": 7, ""NumberUnselectedRows"": 0, ""LoadBytes"": 380, ""LoadTimeMs"": 68, ""BeginTxnTimeMs"": 0,\n""StreamLoadPutTimeMs"": 2, ""ReadDataTimeMs"": 0, ""WriteDataTimeMs"": 45, ""CommitAndPublishTimeMs"": 19,\n""ErrorURL"": ""http://172.21.0.5:8840/api/_load_error_log?file=__shard_0/error_log_insert_stmt_a1463f98a7b15caf-c79399b920f5bfa3_a1463f98a7b15caf_c79399b920f5bfa3"" }\n``` - 查看stream load导入的数据，JSON类型的列j会自动转成JSON string展示', '| 1 | NULL | NULL | | 2 | null | NULL |\n| 3 | true | NULL | | 4 | false | NULL |\n| 5 | 100 | NULL | | 6 | 10000 | NULL |\n| 7 | 1000000000 | NULL | | 8 | 1152921504606846976 | NULL |\n| 9 | 6.18 | NULL | | 10 | ""abcd"" | NULL |\n| 11 | {} | NULL | | 12 | {""k1"":""v31"",""k2"":300} | v31 |\n| 13 | [] | NULL | | 14 | [123,456] | NULL |\n| 15 | [""abc"",""def""] | NULL | | 16 | [null,true,false,100,6.18,""abc""] | NULL |\n| 17 | [{""k1"":""v41"",""k2"":400},1,""a"",3.14] | NULL | | 18 | {""k1"":""v31"",""k2"":300,""a1"":[{""k1"":""v41"",""k2"":400},1,""a"",3.14]} | v31 |\n| 26 | {""k1"":""v1"",""k2"":200} | v1 | +------+---------------------------------------------------------------+-----------------------------------+ 19 rows in set (0.03 sec)\n``` - json_extract_int 获取int类型字段，非int类型返回NULL']","在数据库中，将现有列的数据类型从一个类型更改为另一个类型通常称为""修改列类型""或使用ALTER TABLE语句进行""列类型的更改""。如果您尝试将一个列的数据类型从TEXT更改为JSON，并且遇到了错误，可能的原因如下：

1. **数据不兼容**：TEXT类型的数据可能包含任何形式的文本，而JSON类型要求数据必须是有效的JSON格式。如果TEXT列中包含的数据不是有效的JSON字符串（例如，格式错误、缺少引号、使用了单引号而不是双引号等），尝试将其转换为JSON类型时将会失败。

2. **数据库版本或支持问题**：某些数据库版本可能不支持直接从TEXT到JSON的类型转换，或者在不同的数据库系统中，支持的功能和语法可能有所不同。

3. **语法错误**：执行类型更改的SQL语句可能存在语法错误。确保您遵循了数据库的正确语法和约定。

为了解决这个问题，您可以尝试以下步骤：

- **验证数据**：确保所有数据都是有效的JSON格式。您可以编写查询或使用脚本来检查和修正任何不符合JSON格式的记录。
- **使用中间步骤**：如果直接更改类型时遇到问题，您可以尝试添加一个新的JSON类型列，将TEXT列的数据复制到新列中（在此过程中，确保数据已转换为有效的JSON格式），然后删除原始的TEXT列，并最后将新的JSON列重命名为原始列名。
- **查阅文档**：查看您所使用的数据库的官方文档，了解关于修改列类型的具体指令和限制，特别是从TEXT到JSON类型的转换。

如果上述信息不足以解决您的问题，那么信息是缺失的关于具体的错误消息或代码，以及您所使用的数据库系统（如MySQL、PostgreSQL等）的版本信息。这些信息对于提供更具体的解决方案至关重要。","['```json\n  { ""id"": 123, ""city"" : { ""name"" : ""beijing"", ""region"" : ""haidian"" }}\n  ```\n\n  JSON Path 为 `[""$.city""]`。则匹配到的元素为：\n\n  ```json\n  { ""name"" : ""beijing"", ""region"" : ""haidian"" }\n  ```\n\n  该元素会被转换为字符串进行后续导入操作：\n\n  ```json\n  ""{\'name\':\'beijing\',\'region\':\'haidian\'}""\n  ```\n\n- 匹配失败\n\n  当匹配失败时，将会返回 `null`。示例如下：\n\n  JSON 数据为：\n\n  ```json\n  { ""id"": 123, ""name"" : ""beijing""}\n  ```\n\n  JSON Path 为 `[""$.id"", ""$.info""]`。则匹配到的元素为 `123` 和 `null`。\n\n  Doris 当前不区分 JSON 数据中表示的 null 值，和匹配失败时产生的 null 值。假设 JSON 数据为：\n\n  ```json\n  { ""id"": 123, ""name"" : null }\n  ```\n\n  则使用以下两种 JSON Path 会获得相同的结果：`123` 和 `null`。\n\n  ```json\n  [""$.id"", ""$.name""]\n  ```\n\n  ```json\n  [""$.id"", ""$.info""]\n  ```\n\n- 完全匹配失败\n\n  为防止一些参数设置错误导致的误操作。Doris 在尝试匹配一行数据时，如果所有列都匹配失败，则会认为这个是一个错误行。假设 JSON 数据为：\n\n  ```json\n  { ""id"": 123, ""city"" : ""beijing"" }\n  ```\n\n  如果 JSON Path 错误的写为（或者不指定 JSON Path 时，表中的列不包含 `id` 和 `city`）：\n\n  ```json\n  [""$.ad"", ""$.infa""]\n  ```\n\n  则会导致完全匹配失败，则该行会标记为错误行，而不是产出 `null, null`。\n\n## JSON Path 和 Columns', '4. 通过 SSL 认证方式，从 Kafka 集群导入数据。同时设置 client.id 参数。导入任务为非严格模式，时区为 Africa/Abidjan\n\n   \n\n   ```sql\n   CREATE ROUTINE LOAD example_db.test1 ON example_tbl\n   COLUMNS(k1, k2, k3, v1, v2, v3 = k1 * 100),\n   WHERE k1 > 100 and k2 like ""%doris%""\n   PROPERTIES\n   (\n       ""desired_concurrent_number""=""3"",\n       ""max_batch_interval"" = ""20"",\n       ""max_batch_rows"" = ""300000"",\n       ""max_batch_size"" = ""209715200"",\n       ""strict_mode"" = ""false"",\n       ""timezone"" = ""Africa/Abidjan""\n   )\n   FROM KAFKA\n   (\n       ""kafka_broker_list"" = ""broker1:9092,broker2:9092,broker3:9092"",\n       ""kafka_topic"" = ""my_topic"",\n       ""property.security.protocol"" = ""ssl"",\n       ""property.ssl.ca.location"" = ""FILE:ca.pem"",\n       ""property.ssl.certificate.location"" = ""FILE:client.pem"",\n       ""property.ssl.key.location"" = ""FILE:client.key"",\n       ""property.ssl.key.password"" = ""abcdefg"",\n       ""property.client.id"" = ""my_client_id""\n   );\n   ```\n\n5. 导入 Json 格式数据。默认使用 Json 中的字段名作为列名映射。指定导入 0,1,2 三个分区，起始 offset 都为 0', '`VARCHAR json_replace(VARCHAR json_str, VARCHAR path, VARCHAR val[, VARCHAR path, VARCHAR val] ...)`\n\n\n`json_set` 函数在 JSON 中更新数据并返回结果。如果 `json_str` 或 `path` 为 NULL，则返回 NULL。否则，如果 `json_str` 不是有效的 JSON 或任何 `path` 参数不是有效的路径表达式或包含了 * 通配符，则会返回错误。\n\n路径值对按从左到右的顺序进行评估。\n\n如果 JSON 中已存在某个路径，则路径值对会将现有 JSON 值覆盖为新值。\n否则，对于 JSON 中不存在的某个路径的路径值对将被忽略且不会产生任何影响。\n\n### example\n\n```\nMySQL> select json_replace(null, null, null);\n+----------------------------------+\n| json_replace(NULL, NULL, \'NULL\') |\n+----------------------------------+\n| NULL                             |\n+----------------------------------+\n\nMySQL> select json_replace(\'{""k"": 1}\', ""$.k"", 2);\n+----------------------------------------+\n| json_replace(\'{\\""k\\"": 1}\', \'$.k\', \'2\') |\n+----------------------------------------+\n| {""k"":2}                                |\n+----------------------------------------+', '**参数说明**\n\n访问hdfs相关参数：\n- `uri`：（必填） 访问hdfs的uri。如果uri路径不存在或文件都是空文件，hdfs tvf将返回空集合。\n- `fs.defaultFS`：（必填）\n- `hadoop.username`： （必填）可以是任意字符串，但不能为空\n- `hadoop.security.authentication`：（选填）\n- `hadoop.username`：（选填）\n- `hadoop.kerberos.principal`：（选填）\n- `hadoop.kerberos.keytab`：（选填）\n- `dfs.client.read.shortcircuit`：（选填）\n- `dfs.domain.socket.path`：（选填）\n\n访问 HA 模式 HDFS 相关参数：\n- `dfs.nameservices`：（选填）\n- `dfs.ha.namenodes.your-nameservices`：（选填）\n- `dfs.namenode.rpc-address.your-nameservices.your-namenode`：（选填）\n- `dfs.client.failover.proxy.provider.your-nameservices`：（选填）\n\n文件格式相关参数\n- `format`：(必填) 目前支持 `csv/csv_with_names/csv_with_names_and_types/json/parquet/orc/avro`\n- `column_separator`：(选填) 列分割符, 默认为`,`。 \n- `line_delimiter`：(选填) 行分割符，默认为`\\n`。\n- `compress_type`: (选填) 目前支持 `UNKNOWN/PLAIN/GZ/LZO/BZ2/LZ4FRAME/DEFLATE`。 默认值为 `UNKNOWN`, 将会根据 `uri` 的后缀自动推断类型。\n\n    下面6个参数是用于json格式的导入，具体使用方法可以参照：[Json Load](../../../data-operate/import/import-way/load-json-format.md)']","您好！根据您描述的情况，您之前将JSON数据创建成了`TEXT`类型，现在想要将其修改为`JSON`类型，但在尝试修改时遇到了报错。这通常可能是由以下原因引起的：

1. **数据类型不匹配**：如果原始的`TEXT`类型的数据中包含了不符合`JSON`格式要求的内容，例如没有正确转义的字符或者格式错误，那么在尝试将其转换为`JSON`类型时就会报错。

2. **列的数据内容问题**：如果列中包含的不仅仅是单纯的JSON字符串，而是其他文本内容，直接转换为`JSON`类型会导致错误。

3. **语法错误**：在执行修改列类型的操作时，如果SQL语句的语法有误，也会导致报错。

为了解决这个问题，您可以按照以下步骤操作：

1. **检查数据内容**：确保要转换的`TEXT`类型列中的所有数据都是有效的JSON格式。

2. **使用转换函数**：如果数据中有不符合JSON格式的内容，您可能需要使用类似`json_replace`的函数来清理或转换数据。

   例如，如果您的数据中包含一些需要替换的非法字符，可以使用以下SQL语句：

   ```sql
   SELECT json_replace(your_text_column) FROM your_table;
   ```

   这将会尝试将`your_text_column`中的内容转换为有效的JSON格式。

3. **修改列类型**：在确认数据内容没有问题后，您可以使用以下SQL语句来修改列的类型：

   ```sql
   ALTER TABLE your_table MODIFY COLUMN your_text_column JSON;
   ```

   如果在这一步遇到错误，错误信息通常会提供更多关于问题的细节。

如果您遇到具体的错误信息，请提供错误代码或描述，我可以为您提供更具体的解决方案。

最后，请确保在进行任何数据库修改之前，备份数据以防万一出现不可预见的错误。希望这些信息能够帮助您解决问题。"
9b704046-c18a-11ee-87d8-b026288862a5,"doris可以修改列的数据类型嘛,",",用户可以通过 Schema Change 操作来修改已存在表的 Schema。目前 Doris 支持以下几种修改:，增加、删除列，修改列类型，调整列顺序，增加、修改 Bloom Filter，增加、删除 bitmap index","['分区用于将数据划分成不同区间, 逻辑上可以理解为将原始表划分成了多个子表。可以方便的按分区对数据进行管理，例如，删除数据时，更加迅速。 ##### RANGE分区 业务上，多数用户会选择采用按时间进行partition, 让时间进行partition有以下好处： * 可区分冷热数据 * 可用上Doris分级存储(SSD + SATA)的功能 ##### LIST分区 业务上，用户可以选择城市或者其他枚举值进行partition。 #### HASH分桶(bucket)\n根据hash值将数据划分成不同的 bucket。 * 建议采用区分度大的列做分桶, 避免出现数据倾斜 * 为方便数据恢复, 建议单个 bucket 的 size 不要太大, 保持在 10GB 以内, 所以建表或增加 partition 时请合理考虑 bucket 数目, 其中不同 partition 可指定不同的 buckets 数。 ### 稀疏索引和 Bloom Filter Doris对数据进行有序存储, 在数据有序的基础上为其建立稀疏索引,索引粒度为 block(1024行)。\n稀疏索引选取 schema 中固定长度的前缀作为索引内容, 目前 Doris 选取 36 个字节的前缀作为索引。\n- 建表时建议将查询中常见的过滤字段放在 Schema 的前面, 区分度越大，频次越高的查询字段越往前放。 - 这其中有一个特殊的地方,就是 varchar 类型的字段。varchar 类型字段只能作为稀疏索引的最后一个字段。索引会在 varchar 处截断, 因此 varchar 如果出现在前面，可能索引的长度可能不足 36 个字节。具体可以参阅 [数据模型](./data-model.md)、[ROLLUP 及查询](./hit-the-rollup.md)。\n- 除稀疏索引之外, Doris还提供bloomfilter索引, bloomfilter索引对区分度比较大的列过滤效果明显。 如果考虑到varchar不能放在稀疏索引中, 可以建立bloomfilter索引。\n### Rollup Rollup 本质上可以理解为原始表(Base Table)的一个物化索引。建立 Rollup 时可只选取 Base Table 中的部分列作为 Schema。Schema 中的字段顺序也可与 Base Table 不同。 下列情形可以考虑建立 Rollup： #### Base Table 中数据聚合度不高。 这一般是因 Base Table 有区分度比较大的字段而导致。此时可以考虑选取部分列，建立 Rollup。 如对于 `site_visit` 表：\n```text site_visit(siteid, city, username, pv)\n``` siteid 可能导致数据聚合度不高，如果业务方经常根据城市统计pv需求，可以建立一个只有 city, pv 的 Rollup： ```sql ALTER TABLE site_visit ADD ROLLUP rollup_city(city, pv);\n``` #### Base Table 中的前缀索引无法命中 这一般是 Base Table 的建表方式无法覆盖所有的查询模式。此时可以考虑调整列顺序，建立 Rollup。 如对于 session_data 表： ```text session_data(visitorid, sessionid, visittime, city, province, ip, browser, url)\n``` 如果除了通过 visitorid 分析访问情况外，还有通过 browser, province 分析的情形，可以单独建立 Rollup。 ```sql ALTER TABLE session_data ADD ROLLUP rollup_browser(browser,province,ip,url) DUPLICATE KEY(browser,province);\n``` ## Schema Change 用户可以通过 Schema Change 操作来修改已存在表的 Schema。目前 Doris 支持以下几种修改: - 增加、删除列 - 修改列类型 - 调整列顺序 - 增加、修改 Bloom Filter - 增加、删除 bitmap index', '- SegCompaction: 所有 SegCompaction 任务的内存总和，Orphan的子集。 - SegmentMeta: memory use by segment meta data such as footer or index page，Orphan的子集。 - TabletManager: 存储引擎 get、add、delete Tablet 过程中消耗的内存，Orphan的子集。\n- BufferAllocator: 仅用于非向量化Partitioned Agg过程中的内存复用，Orphan的子集。\n- DataPageCache: 用于缓存数据 Page，用于加速 Scan。 - IndexPageCache: 用于缓存数据 Page 的索引，用于加速 Scan。 - SegmentCache: 用于缓存已打开的 Segment，如索引信息。 - DiskIO: 用于缓存 Disk IO 数据，仅在非向量化使用。 - ChunkAllocator: 用于缓存2的幂大小的内存块，在应用层内存复用。 - LastSuccessChannelCache: 用于缓存导入接收端的 LoadChannel。\n- DeleteBitmap AggCache: Gets aggregated delete_bitmap on rowset_id and version。\n### Query Type `/mem_tracker?type=query` ![image](https://user-images.githubusercontent.com/13197424/202924569-c4f3c556-2f92-4375-962c-c71147704a27.png)\n1. Limit: 单个查询使用的内存上限，`show session variables`查看和修改`exec_mem_limit`。 2. Label: 单个查询的 Tracker 的 Label 命名规则为`Query#Id=xxx`。 3. Parent Label: Parent 是 `Query#Id=xxx` 的 Tracker 记录查询不同算子执行过程使用的内存。\n### Load Type `/mem_tracker?type=load` ![image](https://user-images.githubusercontent.com/13197424/202925855-936889e3-c910-4ca5-bc12-1b9849a09c33.png)\n1. Limit: 导入分为 Fragment Scan 和 Load Channel 写 Segment 到磁盘两个阶段。Scan 阶段的内存上限通过`show session variables`查看和修改`load_mem_limit`；Segment 写磁盘阶段每个导入没有单独的内存上限，而是所有导入的总上限，对应 be.conf 中的 `load_process_max_memory_limit_percent`。\n2. Label: 单个导入 Scan 阶段 Tracker 的 Label 命名规则为`Load#Id=xxx`；单个导入 Segment 写磁盘阶段 Tracker 的 Label 命名规则为`LoadChannel#senderIp=xxx#loadID=xxx`。\n3. Parent Label: Parent是 `Load#Id=xxx` 的 Tracker 记录导入 Scan 阶段不同算子执行过程使用的内存；Parent是 `LoadChannelMgrTrackerSet` 的 Tracker 记录 Segment 写磁盘阶段每个中间数据结构 MemTable 的 Insert 和 Flush 磁盘过程使用的内存，用 Label 最后的 `loadID` 关联 Segment 写磁盘阶段 Tracker。\n</version> --- { ""title"": ""Schema 变更"", ""language"": ""zh-CN"" } --- <!--split--> # Schema Change 用户可以通过 Schema Change 操作来修改已存在表的 Schema。目前 Doris 支持以下几种修改: - 增加、删除列 - 修改列类型 - 调整列顺序 - 增加、修改 Bloom Filter - 增加、删除 bitmap index', '该机制是一个节点认证机制，以防止接收到集群外的节点发送来的错误的心跳信息。 如果需要恢复这个错误。首先要先确认所有节点是否都是正确的集群中的节点。之后，对于FE节点，可以尝试修改元数据目录下的 image/VERSION 文件中的 cluster_id 值后重启FE。对于BE节点，则可以删除所有数据目录下的 cluster_id 文件后重启 BE。 ### Q4. Unique Key 模型查询结果不一致\n某些情况下，当用户使用相同的 SQL 查询一个 Unique Key 模型的表时，可能会出现多次查询结果不一致的现象。并且查询结果总在 2-3 种之间变化。 这可能是因为，在同一批导入数据中，出现了 key 相同但 value 不同的数据，这会导致，不同副本间，因数据覆盖的先后顺序不确定而产生的结果不一致的问题。 比如表定义为 k1, v1。一批次导入数据如下： ```text 1, ""abc"" 1, ""def""\n``` 那么可能副本1 的结果是 `1, ""abc""`，而副本2 的结果是 `1, ""def""`。从而导致查询结果不一致。 为了确保不同副本之间的数据先后顺序唯一，可以参考 [Sequence Column](../data-operate/update-delete/sequence-column-manual.md) 功能。 ### Q5. 查询 bitmap/hll 类型的数据返回 NULL 的问题\n在 1.1.x 版本中，在开启向量化的情况下，执行查询数据表中 bitmap 类型字段返回结果为 NULL 的情况下， 1. 首先你要 `set return_object_data_as_binary=true;` 2. 关闭向量化 `set enable_vectorized_engine=false;` 3. 关闭 SQL 缓存 `set [global] enable_sql_cache = false;`\n这里是因为 bitmap / hll 类型在向量化执行引擎中：输入均为NULL，则输出的结果也是NULL而不是0 ### Q6. 访问对象存储时报错：curl 77: Problem with the SSL CA cert 如果 be.INFO 日志中出现 `curl 77: Problem with the SSL CA cert` 错误。可以尝试通过以下方式解决：\n1. 在 [https://curl.se/docs/caextract.html](https://curl.se/docs/caextract.html) 下载证书：cacert.pem 2. 拷贝证书到指定位置：`sudo cp /tmp/cacert.pem /etc/ssl/certs/ca-certificates.crt` 3. 重启 BE 节点。\n### Q7. 导入报错：""Message"": ""[INTERNAL_ERROR]single replica load is disabled on BE."" 1. be.conf中增加 enable_single_replica_load = true 2. 重启 BE 节点。--- { ""title"": ""数据操作问题"", ""language"": ""zh-CN"" } --- <!--split--> # 数据操作问题\n本文档主要用于记录 Doris 使用过程中的数据操作常见问题。会不定期更新。 ### Q1. 使用 Stream Load 访问 FE 的公网地址导入数据，被重定向到内网 IP？\n当 stream load 的连接目标为FE的http端口时，FE仅会随机选择一台BE节点做http 307 redirect 操作，因此用户的请求实际是发送给FE指派的某一个BE的。而redirect返回的是BE的ip，也即内网IP。所以如果你是通过FE的公网IP发送的请求，很有可能因为redirect到内网地址而无法连接。\n通常的做法，一种是确保自己能够访问内网IP地址，或者是给所有BE上层架设一个负载均衡，然后直接将 stream load 请求发送到负载均衡器上，由负载均衡将请求透传到BE节点。 ### Q2. Doris 是否支持修改列名？ 在 1.2.0 版本之后, 开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。 在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：\nDoris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。 因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。 我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。 ### Q3. Unique Key模型的表是否支持创建物化视图？ 不支持。']","是的，Doris 支持通过 Schema Change 操作来修改已存在表的列的数据类型。这意味着用户可以对表的列进行类型变更，比如从 INT 类型修改为 BIGINT 类型。但是，在进行数据类型变更时，需要注意以下几点：

1. 数据类型变更应保证数据的兼容性，避免因类型不匹配导致数据丢失或错误。例如，可以从 INT 类型变更为 BIGINT 类型，因为 BIGINT 范围更大，能够容纳所有 INT 类型的值，但反向操作可能就不安全。
2. 在进行 Schema Change 操作时，Doris 会在后台创建一个新的表来存储修改后的 Schema，并将旧表的数据转移到新表中。这个过程可能会耗费一定的时间和计算资源，具体取决于表的数据量。
3. 修改列类型是一个重量级的操作，可能会影响到正在进行的查询和数据导入操作。因此，在进行此类操作前，应当评估好时机和影响，并在必要时进行操作前的备份。

总的来说，Doris 提供了灵活的 Schema Change 功能，支持修改列的数据类型，但在操作前应当仔细考虑兼容性和操作影响。",34.91,"['分区用于将数据划分成不同区间, 逻辑上可以理解为将原始表划分成了多个子表。可以方便的按分区对数据进行管理，例如，删除数据时，更加迅速。 ##### RANGE分区 业务上，多数用户会选择采用按时间进行partition, 让时间进行partition有以下好处： * 可区分冷热数据 * 可用上Doris分级存储(SSD + SATA)的功能 ##### LIST分区 业务上，用户可以选择城市或者其他枚举值进行partition。 #### HASH分桶(bucket)\n根据hash值将数据划分成不同的 bucket。 * 建议采用区分度大的列做分桶, 避免出现数据倾斜 * 为方便数据恢复, 建议单个 bucket 的 size 不要太大, 保持在 10GB 以内, 所以建表或增加 partition 时请合理考虑 bucket 数目, 其中不同 partition 可指定不同的 buckets 数。 ### 稀疏索引和 Bloom Filter Doris对数据进行有序存储, 在数据有序的基础上为其建立稀疏索引,索引粒度为 block(1024行)。\n稀疏索引选取 schema 中固定长度的前缀作为索引内容, 目前 Doris 选取 36 个字节的前缀作为索引。\n- 建表时建议将查询中常见的过滤字段放在 Schema 的前面, 区分度越大，频次越高的查询字段越往前放。 - 这其中有一个特殊的地方,就是 varchar 类型的字段。varchar 类型字段只能作为稀疏索引的最后一个字段。索引会在 varchar 处截断, 因此 varchar 如果出现在前面，可能索引的长度可能不足 36 个字节。具体可以参阅 [数据模型](./data-model.md)、[ROLLUP 及查询](./hit-the-rollup.md)。\n- 除稀疏索引之外, Doris还提供bloomfilter索引, bloomfilter索引对区分度比较大的列过滤效果明显。 如果考虑到varchar不能放在稀疏索引中, 可以建立bloomfilter索引。\n### Rollup Rollup 本质上可以理解为原始表(Base Table)的一个物化索引。建立 Rollup 时可只选取 Base Table 中的部分列作为 Schema。Schema 中的字段顺序也可与 Base Table 不同。 下列情形可以考虑建立 Rollup： #### Base Table 中数据聚合度不高。 这一般是因 Base Table 有区分度比较大的字段而导致。此时可以考虑选取部分列，建立 Rollup。 如对于 `site_visit` 表：\n```text site_visit(siteid, city, username, pv)\n``` siteid 可能导致数据聚合度不高，如果业务方经常根据城市统计pv需求，可以建立一个只有 city, pv 的 Rollup： ```sql ALTER TABLE site_visit ADD ROLLUP rollup_city(city, pv);\n``` #### Base Table 中的前缀索引无法命中 这一般是 Base Table 的建表方式无法覆盖所有的查询模式。此时可以考虑调整列顺序，建立 Rollup。 如对于 session_data 表： ```text session_data(visitorid, sessionid, visittime, city, province, ip, browser, url)\n``` 如果除了通过 visitorid 分析访问情况外，还有通过 browser, province 分析的情形，可以单独建立 Rollup。 ```sql ALTER TABLE session_data ADD ROLLUP rollup_browser(browser,province,ip,url) DUPLICATE KEY(browser,province);\n``` ## Schema Change 用户可以通过 Schema Change 操作来修改已存在表的 Schema。目前 Doris 支持以下几种修改: - 增加、删除列 - 修改列类型 - 调整列顺序 - 增加、修改 Bloom Filter - 增加、删除 bitmap index', '- SegCompaction: 所有 SegCompaction 任务的内存总和，Orphan的子集。 - SegmentMeta: memory use by segment meta data such as footer or index page，Orphan的子集。 - TabletManager: 存储引擎 get、add、delete Tablet 过程中消耗的内存，Orphan的子集。\n- BufferAllocator: 仅用于非向量化Partitioned Agg过程中的内存复用，Orphan的子集。\n- DataPageCache: 用于缓存数据 Page，用于加速 Scan。 - IndexPageCache: 用于缓存数据 Page 的索引，用于加速 Scan。 - SegmentCache: 用于缓存已打开的 Segment，如索引信息。 - DiskIO: 用于缓存 Disk IO 数据，仅在非向量化使用。 - ChunkAllocator: 用于缓存2的幂大小的内存块，在应用层内存复用。 - LastSuccessChannelCache: 用于缓存导入接收端的 LoadChannel。\n- DeleteBitmap AggCache: Gets aggregated delete_bitmap on rowset_id and version。\n### Query Type `/mem_tracker?type=query` ![image](https://user-images.githubusercontent.com/13197424/202924569-c4f3c556-2f92-4375-962c-c71147704a27.png)\n1. Limit: 单个查询使用的内存上限，`show session variables`查看和修改`exec_mem_limit`。 2. Label: 单个查询的 Tracker 的 Label 命名规则为`Query#Id=xxx`。 3. Parent Label: Parent 是 `Query#Id=xxx` 的 Tracker 记录查询不同算子执行过程使用的内存。\n### Load Type `/mem_tracker?type=load` ![image](https://user-images.githubusercontent.com/13197424/202925855-936889e3-c910-4ca5-bc12-1b9849a09c33.png)\n1. Limit: 导入分为 Fragment Scan 和 Load Channel 写 Segment 到磁盘两个阶段。Scan 阶段的内存上限通过`show session variables`查看和修改`load_mem_limit`；Segment 写磁盘阶段每个导入没有单独的内存上限，而是所有导入的总上限，对应 be.conf 中的 `load_process_max_memory_limit_percent`。\n2. Label: 单个导入 Scan 阶段 Tracker 的 Label 命名规则为`Load#Id=xxx`；单个导入 Segment 写磁盘阶段 Tracker 的 Label 命名规则为`LoadChannel#senderIp=xxx#loadID=xxx`。\n3. Parent Label: Parent是 `Load#Id=xxx` 的 Tracker 记录导入 Scan 阶段不同算子执行过程使用的内存；Parent是 `LoadChannelMgrTrackerSet` 的 Tracker 记录 Segment 写磁盘阶段每个中间数据结构 MemTable 的 Insert 和 Flush 磁盘过程使用的内存，用 Label 最后的 `loadID` 关联 Segment 写磁盘阶段 Tracker。\n</version> --- { ""title"": ""Schema 变更"", ""language"": ""zh-CN"" } --- <!--split--> # Schema Change 用户可以通过 Schema Change 操作来修改已存在表的 Schema。目前 Doris 支持以下几种修改: - 增加、删除列 - 修改列类型 - 调整列顺序 - 增加、修改 Bloom Filter - 增加、删除 bitmap index', '该机制是一个节点认证机制，以防止接收到集群外的节点发送来的错误的心跳信息。 如果需要恢复这个错误。首先要先确认所有节点是否都是正确的集群中的节点。之后，对于FE节点，可以尝试修改元数据目录下的 image/VERSION 文件中的 cluster_id 值后重启FE。对于BE节点，则可以删除所有数据目录下的 cluster_id 文件后重启 BE。 ### Q4. Unique Key 模型查询结果不一致\n某些情况下，当用户使用相同的 SQL 查询一个 Unique Key 模型的表时，可能会出现多次查询结果不一致的现象。并且查询结果总在 2-3 种之间变化。 这可能是因为，在同一批导入数据中，出现了 key 相同但 value 不同的数据，这会导致，不同副本间，因数据覆盖的先后顺序不确定而产生的结果不一致的问题。 比如表定义为 k1, v1。一批次导入数据如下： ```text 1, ""abc"" 1, ""def""\n``` 那么可能副本1 的结果是 `1, ""abc""`，而副本2 的结果是 `1, ""def""`。从而导致查询结果不一致。 为了确保不同副本之间的数据先后顺序唯一，可以参考 [Sequence Column](../data-operate/update-delete/sequence-column-manual.md) 功能。 ### Q5. 查询 bitmap/hll 类型的数据返回 NULL 的问题\n在 1.1.x 版本中，在开启向量化的情况下，执行查询数据表中 bitmap 类型字段返回结果为 NULL 的情况下， 1. 首先你要 `set return_object_data_as_binary=true;` 2. 关闭向量化 `set enable_vectorized_engine=false;` 3. 关闭 SQL 缓存 `set [global] enable_sql_cache = false;`\n这里是因为 bitmap / hll 类型在向量化执行引擎中：输入均为NULL，则输出的结果也是NULL而不是0 ### Q6. 访问对象存储时报错：curl 77: Problem with the SSL CA cert 如果 be.INFO 日志中出现 `curl 77: Problem with the SSL CA cert` 错误。可以尝试通过以下方式解决：\n1. 在 [https://curl.se/docs/caextract.html](https://curl.se/docs/caextract.html) 下载证书：cacert.pem 2. 拷贝证书到指定位置：`sudo cp /tmp/cacert.pem /etc/ssl/certs/ca-certificates.crt` 3. 重启 BE 节点。\n### Q7. 导入报错：""Message"": ""[INTERNAL_ERROR]single replica load is disabled on BE."" 1. be.conf中增加 enable_single_replica_load = true 2. 重启 BE 节点。--- { ""title"": ""数据操作问题"", ""language"": ""zh-CN"" } --- <!--split--> # 数据操作问题\n本文档主要用于记录 Doris 使用过程中的数据操作常见问题。会不定期更新。 ### Q1. 使用 Stream Load 访问 FE 的公网地址导入数据，被重定向到内网 IP？\n当 stream load 的连接目标为FE的http端口时，FE仅会随机选择一台BE节点做http 307 redirect 操作，因此用户的请求实际是发送给FE指派的某一个BE的。而redirect返回的是BE的ip，也即内网IP。所以如果你是通过FE的公网IP发送的请求，很有可能因为redirect到内网地址而无法连接。\n通常的做法，一种是确保自己能够访问内网IP地址，或者是给所有BE上层架设一个负载均衡，然后直接将 stream load 请求发送到负载均衡器上，由负载均衡将请求透传到BE节点。 ### Q2. Doris 是否支持修改列名？ 在 1.2.0 版本之后, 开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。 在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：\nDoris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。 因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。 我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。 ### Q3. Unique Key模型的表是否支持创建物化视图？ 不支持。']",是的，Doris 支持通过 Schema Change 操作来修改已存在表的 Schema，其中包括修改列的数据类型。但是，需要注意的是，在修改列类型时，必须确保新的数据类型与原始数据类型之间可以兼容或转换，以避免数据丢失或错误。例如，可以从 INT 类型修改为 BIGINT 类型，因为这是向上兼容的。但是，将 CHAR 类型修改为 INT 类型可能会因为数据格式不兼容而失败。在执行 Schema Change 操作时，建议详细了解相关的数据类型兼容性和转换规则。,"['如果 be.INFO 日志中出现 `curl 77: Problem with the SSL CA cert` 错误。可以尝试通过以下方式解决：\n\n1. 在 [https://curl.se/docs/caextract.html](https://curl.se/docs/caextract.html) 下载证书：cacert.pem\n2. 拷贝证书到指定位置：`sudo cp /tmp/cacert.pem /etc/ssl/certs/ca-certificates.crt`\n3. 重启 BE 节点。\n\n### Q7. 导入报错：""Message"": ""[INTERNAL_ERROR]single replica load is disabled on BE.""\n\n1. be.conf中增加 enable_single_replica_load = true\n2. 重启 BE 节点。---\n{\n    ""title"": ""数据操作问题"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 数据操作问题\n\n本文档主要用于记录 Doris 使用过程中的数据操作常见问题。会不定期更新。\n\n### Q1. 使用 Stream Load 访问 FE 的公网地址导入数据，被重定向到内网 IP？\n\n当 stream load 的连接目标为FE的http端口时，FE仅会随机选择一台BE节点做http 307 redirect 操作，因此用户的请求实际是发送给FE指派的某一个BE的。而redirect返回的是BE的ip，也即内网IP。所以如果你是通过FE的公网IP发送的请求，很有可能因为redirect到内网地址而无法连接。\n\n通常的做法，一种是确保自己能够访问内网IP地址，或者是给所有BE上层架设一个负载均衡，然后直接将 stream load 请求发送到负载均衡器上，由负载均衡将请求透传到BE节点。\n\n### Q2. Doris 是否支持修改列名？\n\n在 1.2.0 版本之后, 开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。\n\n在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：', '在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：\n\nDoris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。\n\n因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。\n\n我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。\n\n### Q3. Unique Key模型的表是否支持创建物化视图？\n\n不支持。\n\nUnique Key模型的表是一个对业务比较友好的表，因为其特有的按照主键去重的功能，能够很方便的同步数据频繁变更的业务数据库。因此，很多用户在将数据接入到Doris时，会首先考虑使用Unique Key模型。\n\n但遗憾的是，Unique Key模型的表是无法建立物化视图的。原因在于，物化视图的本质，是通过预计算来将数据“预先算好”，这样在查询时直接返回已经计算好的数据，来加速查询。在物化视图中，“预计算”的数据通常是一些聚合指标，比如求和、求count。这时，如果数据发生变更，如update或delete，因为预计算的数据已经丢失了明细信息，因此无法同步的进行更新。比如一个求和值5，可能是 1+4，也可能是2+3。因为明细信息的丢失，我们无法区分这个求和值是如何计算出来的，因此也就无法满足更新的需求。\n\n### Q4. tablet writer write failed, tablet_id=27306172, txn_id=28573520, err=-235 or -238\n\n这个错误通常发生在数据导入操作中。错误码为 -235。这个错误的含义是，对应tablet的数据版本超过了最大限制（默认500，由 BE 参数 `max_tablet_version_num` 控制），后续写入将被拒绝。比如问题中这个错误，即表示 27306172 这个tablet的数据版本超过了限制。', '</version>\n---\n{\n    ""title"": ""Schema 变更"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# Schema Change\n\n用户可以通过 Schema Change 操作来修改已存在表的 Schema。目前 Doris 支持以下几种修改:\n\n- 增加、删除列\n- 修改列类型\n- 调整列顺序\n- 增加、修改 Bloom Filter\n- 增加、删除 bitmap index\n\n本文档主要介绍如何创建 Schema Change 作业，以及进行 Schema Change 的一些注意事项和常见问题。\n\n## 名词解释\n\n- Base Table：基表。每一个表被创建时，都对应一个基表。\n- Rollup：基于基表或者其他 Rollup 创建出来的上卷表。\n- Index：物化索引。Rollup 或 Base Table 都被称为物化索引。\n- Transaction：事务。每一个导入任务都是一个事务，每个事务有一个唯一递增的 Transaction ID。\n\n## 原理介绍\n\n执行 Schema Change 的基本过程，是通过原 Index 的数据，生成一份新 Schema 的 Index 的数据。其中主要需要进行两部分数据转换，一是已存在的历史数据的转换，二是在 Schema Change 执行过程中，新到达的导入数据的转换。', ""下列情形可以考虑建立 Rollup：\n\n#### Base Table 中数据聚合度不高。\n\n这一般是因 Base Table 有区分度比较大的字段而导致。此时可以考虑选取部分列，建立 Rollup。\n\n如对于 `site_visit` 表：\n\n```text\nsite_visit(siteid, city, username, pv)\n```\n\nsiteid 可能导致数据聚合度不高，如果业务方经常根据城市统计pv需求，可以建立一个只有 city, pv 的 Rollup：\n\n```sql\nALTER TABLE site_visit ADD ROLLUP rollup_city(city, pv);\n```\n\n#### Base Table 中的前缀索引无法命中\n\n这一般是 Base Table 的建表方式无法覆盖所有的查询模式。此时可以考虑调整列顺序，建立 Rollup。\n\n如对于 session_data 表：\n\n```text\nsession_data(visitorid, sessionid, visittime, city, province, ip, browser, url)\n```\n\n如果除了通过 visitorid 分析访问情况外，还有通过 browser, province 分析的情形，可以单独建立 Rollup。\n\n```sql\nALTER TABLE session_data ADD ROLLUP rollup_browser(browser,province,ip,url) DUPLICATE KEY(browser,province);\n```\n\n## Schema Change\n\n用户可以通过 Schema Change 操作来修改已存在表的 Schema。目前 Doris 支持以下几种修改:\n\n- 增加、删除列\n- 修改列类型\n- 调整列顺序\n- 增加、修改 Bloom Filter\n- 增加、删除 bitmap index\n\n具体请参照 [Schema 变更](../advanced/alter-table/schema-change.md)\n---\n{\n    'title': 'Doris BE Mac 开发环境搭建 - CLion', \n    'language': 'zh-CN'\n}\n---\n\n<!--split-->\n\n## 打开 Doris 代码根目录""]","是的，Doris 支持修改列的数据类型。用户可以通过 `ALTER TABLE` 语句中的 `MODIFY COLUMN` 子句来改变列的数据类型。修改列类型属于 schema change 的范畴，需要遵循相应的操作步骤和注意事项。

具体操作方法请参照官方文档中关于 [Schema 变更](https://doris.apache.org/zh-CN/docs/dev/database_management/schema_change.html) 的说明。在执行修改列类型之前，请确保备份好相关数据，并了解变更可能对现有查询和报表产生的影响。"
9b6f95de-c18a-11ee-a745-b026288862a5,"@Petrichor  大佬您好， unuqie模型可以修改列名称吗？ doris 1.2,",",schema change 支持的","['该机制是一个节点认证机制，以防止接收到集群外的节点发送来的错误的心跳信息。 如果需要恢复这个错误。首先要先确认所有节点是否都是正确的集群中的节点。之后，对于FE节点，可以尝试修改元数据目录下的 image/VERSION 文件中的 cluster_id 值后重启FE。对于BE节点，则可以删除所有数据目录下的 cluster_id 文件后重启 BE。 ### Q4. Unique Key 模型查询结果不一致\n某些情况下，当用户使用相同的 SQL 查询一个 Unique Key 模型的表时，可能会出现多次查询结果不一致的现象。并且查询结果总在 2-3 种之间变化。 这可能是因为，在同一批导入数据中，出现了 key 相同但 value 不同的数据，这会导致，不同副本间，因数据覆盖的先后顺序不确定而产生的结果不一致的问题。 比如表定义为 k1, v1。一批次导入数据如下： ```text 1, ""abc"" 1, ""def""\n``` 那么可能副本1 的结果是 `1, ""abc""`，而副本2 的结果是 `1, ""def""`。从而导致查询结果不一致。 为了确保不同副本之间的数据先后顺序唯一，可以参考 [Sequence Column](../data-operate/update-delete/sequence-column-manual.md) 功能。 ### Q5. 查询 bitmap/hll 类型的数据返回 NULL 的问题\n在 1.1.x 版本中，在开启向量化的情况下，执行查询数据表中 bitmap 类型字段返回结果为 NULL 的情况下， 1. 首先你要 `set return_object_data_as_binary=true;` 2. 关闭向量化 `set enable_vectorized_engine=false;` 3. 关闭 SQL 缓存 `set [global] enable_sql_cache = false;`\n这里是因为 bitmap / hll 类型在向量化执行引擎中：输入均为NULL，则输出的结果也是NULL而不是0 ### Q6. 访问对象存储时报错：curl 77: Problem with the SSL CA cert 如果 be.INFO 日志中出现 `curl 77: Problem with the SSL CA cert` 错误。可以尝试通过以下方式解决：\n1. 在 [https://curl.se/docs/caextract.html](https://curl.se/docs/caextract.html) 下载证书：cacert.pem 2. 拷贝证书到指定位置：`sudo cp /tmp/cacert.pem /etc/ssl/certs/ca-certificates.crt` 3. 重启 BE 节点。\n### Q7. 导入报错：""Message"": ""[INTERNAL_ERROR]single replica load is disabled on BE."" 1. be.conf中增加 enable_single_replica_load = true 2. 重启 BE 节点。--- { ""title"": ""数据操作问题"", ""language"": ""zh-CN"" } --- <!--split--> # 数据操作问题\n本文档主要用于记录 Doris 使用过程中的数据操作常见问题。会不定期更新。 ### Q1. 使用 Stream Load 访问 FE 的公网地址导入数据，被重定向到内网 IP？\n当 stream load 的连接目标为FE的http端口时，FE仅会随机选择一台BE节点做http 307 redirect 操作，因此用户的请求实际是发送给FE指派的某一个BE的。而redirect返回的是BE的ip，也即内网IP。所以如果你是通过FE的公网IP发送的请求，很有可能因为redirect到内网地址而无法连接。\n通常的做法，一种是确保自己能够访问内网IP地址，或者是给所有BE上层架设一个负载均衡，然后直接将 stream load 请求发送到负载均衡器上，由负载均衡将请求透传到BE节点。 ### Q2. Doris 是否支持修改列名？ 在 1.2.0 版本之后, 开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。 在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：\nDoris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。 因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。 我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。 ### Q3. Unique Key模型的表是否支持创建物化视图？ 不支持。', '``` ### Keywords ```text ALTER, TABLE, ROLLUP, ALTER TABLE\n``` ### Best Practice --- { ""title"": ""ALTER-CATALOG"", ""language"": ""zh-CN"" } --- <!--split--> ## ALTER-CATALOG ### Name <version since=""1.2""> ALTER CATALOG </version> ### Description 该语句用于设置指定数据目录的属性。（仅管理员使用） 1) 重命名数据目录\n```sql ALTER CATALOG catalog_name RENAME new_catalog_name;\n``` 注意： - `internal` 是内置数据目录，不允许重命名 - 对 `catalog_name` 拥有 Alter 权限才允许对其重命名 - 重命名数据目录后，如需要，请使用 REVOKE 和 GRANT 命令修改相应的用户权限。 2) 设置数据目录属性 ```sql ALTER CATALOG catalog_name SET PROPERTIES (\'key1\' = \'value1\' [, \'key\' = \'value2\']);\n``` 更新指定属性的值为指定的 value。如果 SET PROPERTIES 从句中的 key 在指定 catalog 属性中不存在，则新增此 key。 注意： - 不可更改数据目录类型，即 `type` 属性 - 不可更改内置数据目录 `internal` 的属性 3) 修改数据目录注释 ```sql ALTER CATALOG catalog_name MODIFY COMMENT ""new catalog comment"";\n``` 注意： - `internal` 是内置数据目录，不允许修改注释 ### Example 1. 将数据目录 ctlg_hive 重命名为 hive ```sql ALTER CATALOG ctlg_hive RENAME hive;\n``` 3. 更新名为 hive 数据目录的属性 `hive.metastore.uris` ```sql ALTER CATALOG hive SET PROPERTIES (\'hive.metastore.uris\'=\'thrift://172.21.0.1:9083\'); ``` 4. 更改名为 hive 数据目录的注释 ```sql ALTER CATALOG hive MODIFY COMMENT ""new catalog comment"";\n``` ### Keywords ALTER,CATALOG,RENAME,PROPERTY ### Best Practice --- { ""title"": ""ALTER-TABLE-RENAME"", ""language"": ""zh-CN"" } --- <!--split--> ## ALTER-TABLE-RENAME ### Name ALTER TABLE RENAME ### Description\n该语句用于对已有 table 属性的某些名称进行重命名操作。这个操作是同步的，命令返回表示执行完毕。 语法： ```sql ALTER TABLE [database.]table alter_clause;\n``` rename 的 alter_clause 支持对以下名称进行修改 1. 修改表名 语法： ```sql RENAME new_table_name; ``` 2. 修改 rollup index 名称 语法： ```sql RENAME ROLLUP old_rollup_name new_rollup_name;\n``` 3. 修改 partition 名称 语法： ```sql RENAME PARTITION old_partition_name new_partition_name; ``` 4. 修改 column 名称 <version since=""1.2""> 修改 column 名称 </version> 语法： ```sql RENAME COLUMN old_column_name new_column_name;', '| tbl1 | k1 | INT | No | true | N/A | | | | k2 | INT | No | true | N/A | | | | k3 | varchar(20) | No | true | N/A | |\n| | k4 | INT | No | false| N/A | | +-----------+-------+-------------+------+------+---------+-------+\n``` 修改语句如下，我们将 k3 列的长度改成 50 ```sql alter table example_tbl modify column k3 varchar(50) key null comment \'to 50\'\n```\n完成后，Schema 变为： ```text +-----------+-------+-------------+------+------+---------+-------+ | IndexName | Field | Type | Null | Key | Default | Extra | +-----------+-------+-------------+------+------+---------+-------+\n| tbl1 | k1 | INT | No | true | N/A | | | | k2 | INT | No | true | N/A | | | | k3 | varchar(50) | No | true | N/A | |\n| | k4 | INT | No | false| N/A | | +-----------+-------+-------------+------+------+---------+-------+\n``` 因为Schema Change 作业是异步操作，同一个表同时只能进行一个Schema change 作业，查看作业运行情况，可以通过下面这个命令 ```sql SHOW ALTER TABLE COLUMN\\G;\n``` ## 注意事项 - 一张表在同一时间只能有一个 Schema Change 作业在运行。 - Schema Change 操作不阻塞导入和查询操作。 - 分区列和分桶列不能修改。 - 如果 Schema 中有 REPLACE 方式聚合的 value 列，则不允许删除 Key 列。 如果删除 Key 列，Doris 无法决定 REPLACE 列的取值。 Unique 数据模型表的所有非 Key 列都是 REPLACE 聚合方式。\n- 在新增聚合类型为 SUM 或者 REPLACE 的 value 列时，该列的默认值对历史数据没有含义。 因为历史数据已经失去明细信息，所以默认值的取值并不能实际反映聚合后的取值。 - 当修改列类型时，除 Type 以外的字段都需要按原列上的信息补全。 如修改列 `k1 INT SUM NULL DEFAULT ""1""` 类型为 BIGINT，则需执行命令如下：\n`ALTER TABLE tbl1 MODIFY COLUMN `k1` BIGINT SUM NULL DEFAULT ""1"";` 注意，除新的列类型外，如聚合方式，Nullable 属性，以及默认值都要按照原信息补全。 - 不支持修改列名称、聚合类型、Nullable 属性、默认值以及列注释。 ## 常见问题 - Schema Change 的执行速度']","在 Doris 1.2.0 版本之后，通过开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。这意味着如果您的 Doris 版本是 1.2.0 或更新，并且您已经开启了上述选项，那么理论上是支持修改 Unique Key 模型表的列名称的。但是，根据提供的上下文信息，直接修改列名的操作并没有被明确列出为支持的操作之一。因此，虽然 Doris 在一定条件下支持列名的修改，但是没有具体说明是否适用于所有类型的模型，包括 Unique Key 模型。

建议在尝试之前，仔细阅读 Doris 的官方文档或者联系 Doris 社区获取更为确切的信息。",23.17,"['该机制是一个节点认证机制，以防止接收到集群外的节点发送来的错误的心跳信息。 如果需要恢复这个错误。首先要先确认所有节点是否都是正确的集群中的节点。之后，对于FE节点，可以尝试修改元数据目录下的 image/VERSION 文件中的 cluster_id 值后重启FE。对于BE节点，则可以删除所有数据目录下的 cluster_id 文件后重启 BE。 ### Q4. Unique Key 模型查询结果不一致\n某些情况下，当用户使用相同的 SQL 查询一个 Unique Key 模型的表时，可能会出现多次查询结果不一致的现象。并且查询结果总在 2-3 种之间变化。 这可能是因为，在同一批导入数据中，出现了 key 相同但 value 不同的数据，这会导致，不同副本间，因数据覆盖的先后顺序不确定而产生的结果不一致的问题。 比如表定义为 k1, v1。一批次导入数据如下： ```text 1, ""abc"" 1, ""def""\n``` 那么可能副本1 的结果是 `1, ""abc""`，而副本2 的结果是 `1, ""def""`。从而导致查询结果不一致。 为了确保不同副本之间的数据先后顺序唯一，可以参考 [Sequence Column](../data-operate/update-delete/sequence-column-manual.md) 功能。 ### Q5. 查询 bitmap/hll 类型的数据返回 NULL 的问题\n在 1.1.x 版本中，在开启向量化的情况下，执行查询数据表中 bitmap 类型字段返回结果为 NULL 的情况下， 1. 首先你要 `set return_object_data_as_binary=true;` 2. 关闭向量化 `set enable_vectorized_engine=false;` 3. 关闭 SQL 缓存 `set [global] enable_sql_cache = false;`\n这里是因为 bitmap / hll 类型在向量化执行引擎中：输入均为NULL，则输出的结果也是NULL而不是0 ### Q6. 访问对象存储时报错：curl 77: Problem with the SSL CA cert 如果 be.INFO 日志中出现 `curl 77: Problem with the SSL CA cert` 错误。可以尝试通过以下方式解决：\n1. 在 [https://curl.se/docs/caextract.html](https://curl.se/docs/caextract.html) 下载证书：cacert.pem 2. 拷贝证书到指定位置：`sudo cp /tmp/cacert.pem /etc/ssl/certs/ca-certificates.crt` 3. 重启 BE 节点。\n### Q7. 导入报错：""Message"": ""[INTERNAL_ERROR]single replica load is disabled on BE."" 1. be.conf中增加 enable_single_replica_load = true 2. 重启 BE 节点。--- { ""title"": ""数据操作问题"", ""language"": ""zh-CN"" } --- <!--split--> # 数据操作问题\n本文档主要用于记录 Doris 使用过程中的数据操作常见问题。会不定期更新。 ### Q1. 使用 Stream Load 访问 FE 的公网地址导入数据，被重定向到内网 IP？\n当 stream load 的连接目标为FE的http端口时，FE仅会随机选择一台BE节点做http 307 redirect 操作，因此用户的请求实际是发送给FE指派的某一个BE的。而redirect返回的是BE的ip，也即内网IP。所以如果你是通过FE的公网IP发送的请求，很有可能因为redirect到内网地址而无法连接。\n通常的做法，一种是确保自己能够访问内网IP地址，或者是给所有BE上层架设一个负载均衡，然后直接将 stream load 请求发送到负载均衡器上，由负载均衡将请求透传到BE节点。 ### Q2. Doris 是否支持修改列名？ 在 1.2.0 版本之后, 开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。 在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：\nDoris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。 因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。 我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。 ### Q3. Unique Key模型的表是否支持创建物化视图？ 不支持。', '``` ### Keywords ```text ALTER, TABLE, ROLLUP, ALTER TABLE\n``` ### Best Practice --- { ""title"": ""ALTER-CATALOG"", ""language"": ""zh-CN"" } --- <!--split--> ## ALTER-CATALOG ### Name <version since=""1.2""> ALTER CATALOG </version> ### Description 该语句用于设置指定数据目录的属性。（仅管理员使用） 1) 重命名数据目录\n```sql ALTER CATALOG catalog_name RENAME new_catalog_name;\n``` 注意： - `internal` 是内置数据目录，不允许重命名 - 对 `catalog_name` 拥有 Alter 权限才允许对其重命名 - 重命名数据目录后，如需要，请使用 REVOKE 和 GRANT 命令修改相应的用户权限。 2) 设置数据目录属性 ```sql ALTER CATALOG catalog_name SET PROPERTIES (\'key1\' = \'value1\' [, \'key\' = \'value2\']);\n``` 更新指定属性的值为指定的 value。如果 SET PROPERTIES 从句中的 key 在指定 catalog 属性中不存在，则新增此 key。 注意： - 不可更改数据目录类型，即 `type` 属性 - 不可更改内置数据目录 `internal` 的属性 3) 修改数据目录注释 ```sql ALTER CATALOG catalog_name MODIFY COMMENT ""new catalog comment"";\n``` 注意： - `internal` 是内置数据目录，不允许修改注释 ### Example 1. 将数据目录 ctlg_hive 重命名为 hive ```sql ALTER CATALOG ctlg_hive RENAME hive;\n``` 3. 更新名为 hive 数据目录的属性 `hive.metastore.uris` ```sql ALTER CATALOG hive SET PROPERTIES (\'hive.metastore.uris\'=\'thrift://172.21.0.1:9083\'); ``` 4. 更改名为 hive 数据目录的注释 ```sql ALTER CATALOG hive MODIFY COMMENT ""new catalog comment"";\n``` ### Keywords ALTER,CATALOG,RENAME,PROPERTY ### Best Practice --- { ""title"": ""ALTER-TABLE-RENAME"", ""language"": ""zh-CN"" } --- <!--split--> ## ALTER-TABLE-RENAME ### Name ALTER TABLE RENAME ### Description\n该语句用于对已有 table 属性的某些名称进行重命名操作。这个操作是同步的，命令返回表示执行完毕。 语法： ```sql ALTER TABLE [database.]table alter_clause;\n``` rename 的 alter_clause 支持对以下名称进行修改 1. 修改表名 语法： ```sql RENAME new_table_name; ``` 2. 修改 rollup index 名称 语法： ```sql RENAME ROLLUP old_rollup_name new_rollup_name;\n``` 3. 修改 partition 名称 语法： ```sql RENAME PARTITION old_partition_name new_partition_name; ``` 4. 修改 column 名称 <version since=""1.2""> 修改 column 名称 </version> 语法： ```sql RENAME COLUMN old_column_name new_column_name;', '| tbl1 | k1 | INT | No | true | N/A | | | | k2 | INT | No | true | N/A | | | | k3 | varchar(20) | No | true | N/A | |\n| | k4 | INT | No | false| N/A | | +-----------+-------+-------------+------+------+---------+-------+\n``` 修改语句如下，我们将 k3 列的长度改成 50 ```sql alter table example_tbl modify column k3 varchar(50) key null comment \'to 50\'\n```\n完成后，Schema 变为： ```text +-----------+-------+-------------+------+------+---------+-------+ | IndexName | Field | Type | Null | Key | Default | Extra | +-----------+-------+-------------+------+------+---------+-------+\n| tbl1 | k1 | INT | No | true | N/A | | | | k2 | INT | No | true | N/A | | | | k3 | varchar(50) | No | true | N/A | |\n| | k4 | INT | No | false| N/A | | +-----------+-------+-------------+------+------+---------+-------+\n``` 因为Schema Change 作业是异步操作，同一个表同时只能进行一个Schema change 作业，查看作业运行情况，可以通过下面这个命令 ```sql SHOW ALTER TABLE COLUMN\\G;\n``` ## 注意事项 - 一张表在同一时间只能有一个 Schema Change 作业在运行。 - Schema Change 操作不阻塞导入和查询操作。 - 分区列和分桶列不能修改。 - 如果 Schema 中有 REPLACE 方式聚合的 value 列，则不允许删除 Key 列。 如果删除 Key 列，Doris 无法决定 REPLACE 列的取值。 Unique 数据模型表的所有非 Key 列都是 REPLACE 聚合方式。\n- 在新增聚合类型为 SUM 或者 REPLACE 的 value 列时，该列的默认值对历史数据没有含义。 因为历史数据已经失去明细信息，所以默认值的取值并不能实际反映聚合后的取值。 - 当修改列类型时，除 Type 以外的字段都需要按原列上的信息补全。 如修改列 `k1 INT SUM NULL DEFAULT ""1""` 类型为 BIGINT，则需执行命令如下：\n`ALTER TABLE tbl1 MODIFY COLUMN `k1` BIGINT SUM NULL DEFAULT ""1"";` 注意，除新的列类型外，如聚合方式，Nullable 属性，以及默认值都要按照原信息补全。 - 不支持修改列名称、聚合类型、Nullable 属性、默认值以及列注释。 ## 常见问题 - Schema Change 的执行速度']","在 Doris 1.2.0 版本之后，通过开启 `""light_schema_change""=""true""` 选项，可以支持修改列名。这意味着如果您的 Doris 版本是 1.2.0 或更高，并且已经开启了这个选项，那么您可以修改 Unique Key 模型的表的列名称。请确保在尝试修改列名之前，此选项已被正确设置。","['如果 be.INFO 日志中出现 `curl 77: Problem with the SSL CA cert` 错误。可以尝试通过以下方式解决：\n\n1. 在 [https://curl.se/docs/caextract.html](https://curl.se/docs/caextract.html) 下载证书：cacert.pem\n2. 拷贝证书到指定位置：`sudo cp /tmp/cacert.pem /etc/ssl/certs/ca-certificates.crt`\n3. 重启 BE 节点。\n\n### Q7. 导入报错：""Message"": ""[INTERNAL_ERROR]single replica load is disabled on BE.""\n\n1. be.conf中增加 enable_single_replica_load = true\n2. 重启 BE 节点。---\n{\n    ""title"": ""数据操作问题"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 数据操作问题\n\n本文档主要用于记录 Doris 使用过程中的数据操作常见问题。会不定期更新。\n\n### Q1. 使用 Stream Load 访问 FE 的公网地址导入数据，被重定向到内网 IP？\n\n当 stream load 的连接目标为FE的http端口时，FE仅会随机选择一台BE节点做http 307 redirect 操作，因此用户的请求实际是发送给FE指派的某一个BE的。而redirect返回的是BE的ip，也即内网IP。所以如果你是通过FE的公网IP发送的请求，很有可能因为redirect到内网地址而无法连接。\n\n通常的做法，一种是确保自己能够访问内网IP地址，或者是给所有BE上层架设一个负载均衡，然后直接将 stream load 请求发送到负载均衡器上，由负载均衡将请求透传到BE节点。\n\n### Q2. Doris 是否支持修改列名？\n\n在 1.2.0 版本之后, 开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。\n\n在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：', '在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：\n\nDoris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。\n\n因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。\n\n我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。\n\n### Q3. Unique Key模型的表是否支持创建物化视图？\n\n不支持。\n\nUnique Key模型的表是一个对业务比较友好的表，因为其特有的按照主键去重的功能，能够很方便的同步数据频繁变更的业务数据库。因此，很多用户在将数据接入到Doris时，会首先考虑使用Unique Key模型。\n\n但遗憾的是，Unique Key模型的表是无法建立物化视图的。原因在于，物化视图的本质，是通过预计算来将数据“预先算好”，这样在查询时直接返回已经计算好的数据，来加速查询。在物化视图中，“预计算”的数据通常是一些聚合指标，比如求和、求count。这时，如果数据发生变更，如update或delete，因为预计算的数据已经丢失了明细信息，因此无法同步的进行更新。比如一个求和值5，可能是 1+4，也可能是2+3。因为明细信息的丢失，我们无法区分这个求和值是如何计算出来的，因此也就无法满足更新的需求。\n\n### Q4. tablet writer write failed, tablet_id=27306172, txn_id=28573520, err=-235 or -238\n\n这个错误通常发生在数据导入操作中。错误码为 -235。这个错误的含义是，对应tablet的数据版本超过了最大限制（默认500，由 BE 参数 `max_tablet_version_num` 控制），后续写入将被拒绝。比如问题中这个错误，即表示 27306172 这个tablet的数据版本超过了限制。']","在 1.2.0 版本之后, 开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。

在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：

Doris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。

因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。

我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。"
907462cb-c18a-11ee-acde-b026288862a5,Doris是否支持RAID？ ,Doris在数据压缩和均衡过程中对RAID是无感的，因此支持RAID。,"['| Hint log: | | Used: ORDERED |\n| UnUsed: | | SyntaxError: |\n+----------------------------------------------------------------------------------+ 16 rows in set (0.01 sec)\n``` - Explain shape plan里面会显示distribute算子相关的信息，其中DistributionSpecReplicated表示该算子将对应的数据变成所有be节点复制一份，DistributionSpecGather表示将数据gather到fe节点，DistributionSpecHash表示将数据按照特定的hashKey以及算法打散到不同的be节点。 # 待支持 - leadingHint待支持子查询解嵌套指定，当前和子查询提升以后不能混用，需要有hint来控制是否可以解嵌套\n- 需要新的distributeHint来更好且更全面地控制distribute算子 - 混合使用leadingHint与distributeHint来共同确定join的形状 --- { ""title"": ""标准部署"", ""language"": ""zh-CN"" } ---\n<!--split--> # 标准部署 该文档主要介绍了部署 Doris 所需软硬件环境、建议的部署方式、集群扩容缩容，以及集群搭建到运行过程中的常见问题。 在阅读本文档前，请先根据编译文档编译 Doris。 ## 软硬件需求 ### 概述 Doris 作为一款开源的 MPP 架构 OLAP 数据库，能够运行在绝大多数主流的商用服务器上。为了能够充分运用 MPP 架构的并发优势，以及 Doris 的高可用特性，我们建议 Doris 的部署遵循以下需求：\n#### Linux 操作系统版本需求 | Linux 系统 | 版本 | |---|---| | CentOS | 7.1 及以上 | | Ubuntu | 16.04 及以上 | #### 软件需求 | 软件 | 版本 | |---|---| | Java | 1.8 | | GCC | 4.8.2 及以上 | #### 操作系统安装要求 ##### 设置系统最大打开文件句柄数\n``` vi /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536\n``` ##### 时钟同步 Doris 的元数据要求时间精度要小于5000ms，所以所有集群所有机器要进行时钟同步，避免因为时钟问题引发的元数据不一致导致服务出现异常。 ##### 关闭交换分区（swap） Linux交换分区会给Doris带来很严重的性能问题，需要在安装之前禁用交换分区 ##### Linux文件系统 ext4和xfs文件系统均支持。 #### 开发测试环境\n| 模块 | CPU | 内存 | 磁盘 | 网络 | 实例数量 | |---|---|---|---|---|---| | Frontend | 8核+ | 8GB+ | SSD 或 SATA，10GB+ * | 千兆网卡 | 1 | | Backend | 8核+ | 16GB+ | SSD 或 SATA，50GB+ * | 千兆网卡 | 1-3 * | #### 生产环境\n| 模块 | CPU | 内存 | 磁盘 | 网络 | 实例数量（最低要求） | |---|---|---|---|---|------------| | Frontend | 16核+ | 64GB+ | SSD 或 RAID 卡，100GB+ * | 万兆网卡 | 1-3 * | | Backend | 16核+ | 64GB+ | SSD 或 SATA，100G+ * | 万兆网卡 | 3 * |', '2. 升级过程中可能的错误 - repeat 函数不可使用并报错：`vectorized repeat function cannot be executed`，可以在升级前先关闭向量化执行引擎。 - schema change 失败并报错：`desc_tbl is not set. Maybe the FE version is not equal to the BE`\n- 向量化 hash join 不可使用并报错。`vectorized hash join cannot be executed`。可以在升级前先关闭向量化执行引擎。\n以上错误在完全升级后会恢复正常。 ### 性能影响 - 默认使用 JeMalloc 作为新版本 BE 的内存分配器，替换 TcMalloc 。 JeMalloc 相比 TcMalloc 使用的内存更少、高并发场景性能更高，但在内存充足的性能测试时，TcMalloc 比 JeMalloc 性能高5%-10%，详细测试见: https://github.com/apache/doris/pull/12496\n- tablet sink 中的 batch size 修改为至少 8K。 - 默认关闭 Page Cache 和 减少 Chunk Allocator 预留内存大小 Page Cache 和 Chunk Allocator 分别缓存用户数据块和内存预分配，这两个功能会占用一定比例的内存并且不会释放。由于这部分内存占用无法灵活调配，导致在某些场景下可能因这部分内存占用而导致其他任务内存不足，影响系统稳定性和可用性，因此新版本中默认关闭了这两个功能。\n但在某些延迟敏感的报表场景下，关闭该功能可能会导致查询延迟增加。如用户担心升级后该功能对业务造成影响，可以通过在 be.conf 中增加以下参数以保持和之前版本行为一致。\n``` disable_storage_page_cache=false chunk_reserved_bytes_limit=10%\n``` ### API 变化 - BE 的 http api 错误返回信息，由 `{""status"": ""Fail"", ""msg"": ""xxx""}` 变更为更具体的 ``{""status"": ""Not found"", ""msg"": ""Tablet not found. tablet_id=1202""}`` - `SHOW CREATE TABLE` 中， comment的内容由双引号包裹变为单引号包裹 - 支持普通用户通过 http 命令获取 query profile。\n文档：[https://doris.apache.org/zh-CN/docs/dev/admin-manual/http-actions/fe/manager/query-profile-action](https://doris.apache.org/zh-CN/docs/dev/admin-manual/http-actions/fe/manager/query-profile-action) - 优化了 sequence 列的指定方式，可以直接指定列名。\n文档：[https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual](https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual)\n- `show backends` 和 `show tablets` 返回结果中，增加远端存储的空间使用情况 (#11450) - 移除了 Num-Based Compaction 相关代码(#13409) - 重构了BE的错误码机制，部分返回的错误信息会发生变化(#8855) # 其他\n- 支持Docker 官方镜像。 - 支持在 MacOS(x86/M1) 和 ubuntu-22.04 上编译 Doris - 支持进行image 文件的校验。', '| doris.request.query.timeout.s | 3600 | 查询doris的超时时间，默认值为1小时，-1表示无超时限制\n|\n| doris.request.tablet.size | Integer.MAX_VALUE | 一个RDD Partition对应的Doris Tablet个数。<br />此数值设置越小，则会生成越多的Partition。从而提升Spark侧的并行度，但同时会对Doris造成更大的压力。\n|\n| doris.read.field | -- | 读取Doris表的列名列表，多列之间使用逗号分隔\n|\n| doris.batch.size | 1024 | 一次从BE读取数据的最大行数。增大此数值可减少Spark与Doris之间建立连接的次数。<br />从而减轻网络延迟所带来的额外时间开销。\n|\n| doris.exec.mem.limit | 2147483648 | 单个查询的内存限制。默认为 2GB，单位为字节\n|\n| doris.deserialize.arrow.async | false | 是否支持异步转换Arrow格式到spark-doris-connector迭代所需的RowBatch']",是的，Doris 支持使用 RAID。在生产环境的硬件需求部分，推荐的磁盘配置包括 SSD 或 RAID 卡，100GB+。这表明 Doris 可以与 RAID 配置一起使用，以提高数据的可靠性和访问速度。,11.94,"['| Hint log: | | Used: ORDERED |\n| UnUsed: | | SyntaxError: |\n+----------------------------------------------------------------------------------+ 16 rows in set (0.01 sec)\n``` - Explain shape plan里面会显示distribute算子相关的信息，其中DistributionSpecReplicated表示该算子将对应的数据变成所有be节点复制一份，DistributionSpecGather表示将数据gather到fe节点，DistributionSpecHash表示将数据按照特定的hashKey以及算法打散到不同的be节点。 # 待支持 - leadingHint待支持子查询解嵌套指定，当前和子查询提升以后不能混用，需要有hint来控制是否可以解嵌套\n- 需要新的distributeHint来更好且更全面地控制distribute算子 - 混合使用leadingHint与distributeHint来共同确定join的形状 --- { ""title"": ""标准部署"", ""language"": ""zh-CN"" } ---\n<!--split--> # 标准部署 该文档主要介绍了部署 Doris 所需软硬件环境、建议的部署方式、集群扩容缩容，以及集群搭建到运行过程中的常见问题。 在阅读本文档前，请先根据编译文档编译 Doris。 ## 软硬件需求 ### 概述 Doris 作为一款开源的 MPP 架构 OLAP 数据库，能够运行在绝大多数主流的商用服务器上。为了能够充分运用 MPP 架构的并发优势，以及 Doris 的高可用特性，我们建议 Doris 的部署遵循以下需求：\n#### Linux 操作系统版本需求 | Linux 系统 | 版本 | |---|---| | CentOS | 7.1 及以上 | | Ubuntu | 16.04 及以上 | #### 软件需求 | 软件 | 版本 | |---|---| | Java | 1.8 | | GCC | 4.8.2 及以上 | #### 操作系统安装要求 ##### 设置系统最大打开文件句柄数\n``` vi /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536\n``` ##### 时钟同步 Doris 的元数据要求时间精度要小于5000ms，所以所有集群所有机器要进行时钟同步，避免因为时钟问题引发的元数据不一致导致服务出现异常。 ##### 关闭交换分区（swap） Linux交换分区会给Doris带来很严重的性能问题，需要在安装之前禁用交换分区 ##### Linux文件系统 ext4和xfs文件系统均支持。 #### 开发测试环境\n| 模块 | CPU | 内存 | 磁盘 | 网络 | 实例数量 | |---|---|---|---|---|---| | Frontend | 8核+ | 8GB+ | SSD 或 SATA，10GB+ * | 千兆网卡 | 1 | | Backend | 8核+ | 16GB+ | SSD 或 SATA，50GB+ * | 千兆网卡 | 1-3 * | #### 生产环境\n| 模块 | CPU | 内存 | 磁盘 | 网络 | 实例数量（最低要求） | |---|---|---|---|---|------------| | Frontend | 16核+ | 64GB+ | SSD 或 RAID 卡，100GB+ * | 万兆网卡 | 1-3 * | | Backend | 16核+ | 64GB+ | SSD 或 SATA，100G+ * | 万兆网卡 | 3 * |', '2. 升级过程中可能的错误 - repeat 函数不可使用并报错：`vectorized repeat function cannot be executed`，可以在升级前先关闭向量化执行引擎。 - schema change 失败并报错：`desc_tbl is not set. Maybe the FE version is not equal to the BE`\n- 向量化 hash join 不可使用并报错。`vectorized hash join cannot be executed`。可以在升级前先关闭向量化执行引擎。\n以上错误在完全升级后会恢复正常。 ### 性能影响 - 默认使用 JeMalloc 作为新版本 BE 的内存分配器，替换 TcMalloc 。 JeMalloc 相比 TcMalloc 使用的内存更少、高并发场景性能更高，但在内存充足的性能测试时，TcMalloc 比 JeMalloc 性能高5%-10%，详细测试见: https://github.com/apache/doris/pull/12496\n- tablet sink 中的 batch size 修改为至少 8K。 - 默认关闭 Page Cache 和 减少 Chunk Allocator 预留内存大小 Page Cache 和 Chunk Allocator 分别缓存用户数据块和内存预分配，这两个功能会占用一定比例的内存并且不会释放。由于这部分内存占用无法灵活调配，导致在某些场景下可能因这部分内存占用而导致其他任务内存不足，影响系统稳定性和可用性，因此新版本中默认关闭了这两个功能。\n但在某些延迟敏感的报表场景下，关闭该功能可能会导致查询延迟增加。如用户担心升级后该功能对业务造成影响，可以通过在 be.conf 中增加以下参数以保持和之前版本行为一致。\n``` disable_storage_page_cache=false chunk_reserved_bytes_limit=10%\n``` ### API 变化 - BE 的 http api 错误返回信息，由 `{""status"": ""Fail"", ""msg"": ""xxx""}` 变更为更具体的 ``{""status"": ""Not found"", ""msg"": ""Tablet not found. tablet_id=1202""}`` - `SHOW CREATE TABLE` 中， comment的内容由双引号包裹变为单引号包裹 - 支持普通用户通过 http 命令获取 query profile。\n文档：[https://doris.apache.org/zh-CN/docs/dev/admin-manual/http-actions/fe/manager/query-profile-action](https://doris.apache.org/zh-CN/docs/dev/admin-manual/http-actions/fe/manager/query-profile-action) - 优化了 sequence 列的指定方式，可以直接指定列名。\n文档：[https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual](https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual)\n- `show backends` 和 `show tablets` 返回结果中，增加远端存储的空间使用情况 (#11450) - 移除了 Num-Based Compaction 相关代码(#13409) - 重构了BE的错误码机制，部分返回的错误信息会发生变化(#8855) # 其他\n- 支持Docker 官方镜像。 - 支持在 MacOS(x86/M1) 和 ubuntu-22.04 上编译 Doris - 支持进行image 文件的校验。', '| doris.request.query.timeout.s | 3600 | 查询doris的超时时间，默认值为1小时，-1表示无超时限制\n|\n| doris.request.tablet.size | Integer.MAX_VALUE | 一个RDD Partition对应的Doris Tablet个数。<br />此数值设置越小，则会生成越多的Partition。从而提升Spark侧的并行度，但同时会对Doris造成更大的压力。\n|\n| doris.read.field | -- | 读取Doris表的列名列表，多列之间使用逗号分隔\n|\n| doris.batch.size | 1024 | 一次从BE读取数据的最大行数。增大此数值可减少Spark与Doris之间建立连接的次数。<br />从而减轻网络延迟所带来的额外时间开销。\n|\n| doris.exec.mem.limit | 2147483648 | 单个查询的内存限制。默认为 2GB，单位为字节\n|\n| doris.deserialize.arrow.async | false | 是否支持异步转换Arrow格式到spark-doris-connector迭代所需的RowBatch']",是的，Doris 支持 RAID 配置。在生产环境的硬件要求部分，建议的磁盘配置可以是 SSD 或 RAID 卡，容量100GB+。这表明 Doris 可以在使用 RAID 卡的存储系统上运行，以提高数据的可靠性和读写性能。 RAID 配置可以帮助提高数据的冗余和容错能力，对于确保数据库的高可用性和数据安全性至关重要。,"['- 删除数据目录：\n\n    `rm -rf data/0/12345/`\n\n  - 删除 Tablet 元数据（具体参考 [Tablet 元数据管理工具](./tablet-meta-tool.md)）\n\n    `./lib/meta_tool --operation=delete_header --root_path=/path/to/root_path --tablet_id=12345 --schema_hash= 352781111`\n---\n{\n    ""title"": ""数据副本管理"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 数据副本管理\n\n从 0.9.0 版本开始，Doris 引入了优化后的副本管理策略，同时支持了更为丰富的副本状态查看工具。本文档主要介绍 Doris 数据副本均衡、修复方面的调度策略，以及副本管理的运维方法。帮助用户更方便的掌握和管理集群中的副本状态。\n\n> Colocation 属性的表的副本修复和均衡可以参阅[这里](../../query-acceleration/join-optimization/colocation-join.md)\n\n## 名词解释\n\n1. Tablet：Doris 表的逻辑分片，一个表有多个分片。\n2. Replica：分片的副本，默认一个分片有3个副本。\n3. Healthy Replica：健康副本，副本所在 Backend 存活，且副本的版本完整。\n4. TabletChecker（TC）：是一个常驻的后台线程，用于定期扫描所有的 Tablet，检查这些 Tablet 的状态，并根据检查结果，决定是否将 tablet 发送给 TabletScheduler。\n5. TabletScheduler（TS）：是一个常驻的后台线程，用于处理由 TabletChecker 发来的需要修复的 Tablet。同时也会进行集群副本均衡的工作。\n6. TabletSchedCtx（TSC）：是一个 tablet 的封装。当 TC 选择一个 tablet 后，会将其封装为一个 TSC，发送给 TS。\n7. Storage Medium：存储介质。Doris 支持对分区粒度指定不同的存储介质，包括 SSD 和 HDD。副本调度策略也是针对不同的存储介质分别调度的。\n\n```', '所有 Follower 角色的 FE 节点会组成一个可选择组，类似 Paxos 一致性协议里的组概念。组内会选举出一个 Follower 作为 Master。当 Master 挂了，会自动选择新的 Follower 作为 Master。而 Observer 不会参与选举，因此 Observer 也不会成为 Master 。\n\n一条元数据日志需要在多数 Follower 节点写入成功，才算成功。比如3个 FE ，2个写入成功才可以。这也是为什么 Follower 角色的个数需要是奇数的原因。\n\nObserver 角色和这个单词的含义一样，仅仅作为观察者来同步已经成功写入的元数据日志，并且提供元数据读服务。他不会参与多数写的逻辑。\n\n通常情况下，可以部署 1 Follower + 2 Observer 或者 3 Follower + N Observer。前者运维简单，几乎不会出现 Follower 之间的一致性协议导致这种复杂错误情况（企业大多使用这种方式）。后者可以保证元数据写的高可用，如果是高并发查询场景，可以适当增加 Observer。\n\n### Q4. 节点新增加了新的磁盘，为什么数据没有均衡到新的磁盘上？\n\n当前Doris的均衡策略是以节点为单位的。也就是说，是按照节点整体的负载指标（分片数量和总磁盘利用率）来判断集群负载。并且将数据分片从高负载节点迁移到低负载节点。如果每个节点都增加了一块磁盘，则从节点整体角度看，负载并没有改变，所以无法触发均衡逻辑。\n\n此外，Doris目前并不支持单个节点内部，各个磁盘间的均衡操作。所以新增磁盘后，不会将数据均衡到新的磁盘。\n\n但是，数据在节点之间迁移时，Doris会考虑磁盘的因素。比如一个分片从A节点迁移到B节点，会优先选择B节点中，磁盘空间利用率较低的磁盘。\n\n这里我们提供3种方式解决这个问题：\n\n1. 重建新表\n\n   通过create table like 语句建立新表，然后使用 insert into select的方式将数据从老表同步到新表。因为创建新表时，新表的数据分片会分布在新的磁盘中，从而数据也会写入新的磁盘。这种方式适用于数据量较小的情况（几十GB以内）。\n\n2. 通过Decommission命令']",Apache Doris（以前称为Apache Incubator项目）不支持RAID配置。它主要用于优化数据副本管理和存储效率，但没有直接集成RAID功能。
8f801889-c195-11ee-bc72-b026288862a5,"Doris支持哪些云存储服务？
",Doris支持所有云商的对象存储，包括AWS的S3和阿里的OSS，它们可以使用相同的参数进行配置。,"['``` ### Example 1. 取消 example_db 下的 BACKUP 任务。 ```sql CANCEL BACKUP FROM example_db;\n``` ### Keywords CANCEL, BACKUP ### Best Practice --- { ""title"": ""CREATE-REPOSITORY"", ""language"": ""zh-CN"" } --- <!--split--> ## CREATE-REPOSITORY ### Name CREATE REPOSITORY ### Description\n该语句用于创建仓库。仓库用于属于备份或恢复。仅 root 或 superuser 用户可以创建仓库。 语法： ```sql CREATE [READ ONLY] REPOSITORY `repo_name` WITH [BROKER `broker_name`|S3|hdfs] ON LOCATION `repo_location` PROPERTIES (""key""=""value"", ...);\n``` 说明： - 仓库的创建，依赖于已存在的 broker 或者直接通过AWS s3 协议访问云存储，或者直接访问HDFS - 如果是只读仓库，则只能在仓库上进行恢复。如果不是，则可以进行备份和恢复操作。 - 根据 broker 或者S3、hdfs的不同类型，PROPERTIES 有所不同，具体见示例。 - ON LOCATION ,如果是 S3 , 这里后面跟的是 Bucket Name。 ### Example\n1. 创建名为 bos_repo 的仓库，依赖 BOS broker ""bos_broker""，数据根目录为：bos://palo_backup\n```sql CREATE REPOSITORY `bos_repo` WITH BROKER `bos_broker` ON LOCATION ""bos://palo_backup"" PROPERTIES ( ""bos_endpoint"" = ""http://gz.bcebos.com"", ""bos_accesskey"" = ""bos_accesskey"", ""bos_secret_accesskey""=""bos_secret_accesskey"" );\n``` 2. 创建和示例 1 相同的仓库，但属性为只读：\n```sql CREATE READ ONLY REPOSITORY `bos_repo` WITH BROKER `bos_broker` ON LOCATION ""bos://palo_backup"" PROPERTIES ( ""bos_endpoint"" = ""http://gz.bcebos.com"", ""bos_accesskey"" = ""bos_accesskey"", ""bos_secret_accesskey""=""bos_accesskey"" );\n``` 3. 创建名为 hdfs_repo 的仓库，依赖 Baidu hdfs broker ""hdfs_broker""，数据根目录为：hdfs://hadoop-name-node:54310/path/to/repo/\n```sql CREATE REPOSITORY `hdfs_repo` WITH BROKER `hdfs_broker` ON LOCATION ""hdfs://hadoop-name-node:54310/path/to/repo/"" PROPERTIES ( ""username"" = ""user"", ""password"" = ""password"" );\n``` 4. 创建名为 s3_repo 的仓库，直接链接云存储，而不通过broker.', '- **支持 Vetical Compaction**。在过去版本中，宽列场景 Compaction 往往会带来大量的内存开销。在 1.2.2 版本中，Vertical Compaction 采用了按列组的方式进行数据合并，单次合并只需要加载部分列的数据，能够极大减少合并过程中的内存占用。在实际测试中，Vertical compaction 使用内存仅为原有 compaction 算法的 1/10，同时 Compaction 速率提升15%。\n- 支持 **Segment Compaction**。在过去版本中，当用户大数据量高频导入时可能会遇到 -238 以及 -235 问题，Segment Compaction 允许在导入数据的同时进行数据的合并，以有效控制 Segment 文件的数量，提升高频导入的系统稳定性。\n参考文档：[https://doris.apache.org/docs/dev/advanced/best-practice/compaction](https://doris.apache.org/docs/dev/advanced/best-practice/compaction) ### 数据湖分析 - Hive Catalog 支持访问 Hive 1/2/3 版本。 - Hive Catalog 可以使用 Broker 访问数据存储在 JuiceFS 的 Hive。\n- Iceberg Catalog 支持 Hive Metastore 和 Rest 作为元数据服务。 - ES Catalog 支持 元数据字段 _id 列映射。 参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hive](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hive)\n- 优化 Iceberg V2 表有大量删除行诗时的读取性能。 - 支持读取 Schema Evolution 后 Iceberg 表。 - Parquet Reader 正确处理列名大小写。 ### 其他 - 支持访问 Hadoop KMS 加密的 HDFS 。 - 支持取消正在执行的导出任务。\n参考文档：[https://doris.apache.org/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/CANCEL-EXPORT](https://doris.apache.org/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/CANCEL-EXPORT)\n- 将`explode_split` 函数执行效率优化 1 倍。 - 将 nullable 列的读取性能优化 3 倍。 - 优化 Memtracker 的部分问题，提高内存管理精度，优化内存应用。 # BugFix - 修复了使用 Doris-Flink-Connector 导入数据时的内存泄漏问题；[#16430](https://github.com/apache/doris/pull/16430)\n- 修复了 BE 可能的线程调度问题，并减少了 BE 线程耗尽导致的 Fragment_sent_timeout。 - 修复了 datetimev2/decivalv3 的部分正确性和精度问题。 - 修复了 Light Schema Change 功能的各种已知问题。 - 修复了 bitmap 类型 Runtime Filter 的各种数据正确性问题。 - 修复了 1.2.1 版本中引入的 CSV 读取性能差的问题。 - 修复了 Spark Load 数据下载阶段导致的 BE OOM 问题。\n- 修复了从 1.1.x 版升级到 1.2.x 版时可能出现的元数据兼容性问题。 - 修复了创建 JDBC Catalog 时的元数据问题。 - 修复了由于导入操作导致的 CPU 使用率高的问题。 - 修复了大量失败 Broker Load 作业导致的 FE OOM 问题。 - 修复了加载浮点类型时精度丢失的问题。 - 修复了 Stream Load 使用两阶段提交时出现的内存泄漏问题。 # 其他\n添加指标以查看 BE 上的 Rowset 和 Segment 总数字 `doris_be_all_rowsets_num` 和 `doris_be_all_segments_num` # 致谢 有 53 位贡献者参与到 1.2.2 版本的开发与完善中，感谢他们的付出，他们分别是： @adonis0147 @AshinGau @BePPPower @BiteTheDDDDt @ByteYue @caiconghui @cambyzju @chenlinzhong\n@DarvenDuan @dataroaring @Doris-Extras @dutyu @englefly @freemandealer @Gabriel39 @HappenLee @Henry2SS @htyoung @isHuangXin @JackDrogon @jacktengg @Jibing-Li @kaka11chen @Kikyou1997 @Lchangliang @LemonLiTree @liaoxin01 @liqing-coder', '| OLAP_ERR_META_DELETE | -3006 | 删除元数据错误 | | OLAP_ERR_META_ALREADY_EXIST | -3007 | 元数据已经存在错误 |\n| Rowset错误代码 | | | | OLAP_ERR_ROWSET_WRITER_INIT | -3100 | Rowset写初始化错误 |\n| OLAP_ERR_ROWSET_SAVE_FAILED | -3101 | Rowset保存失败 | | OLAP_ERR_ROWSET_GENERATE_ID_FAILED | -3102 | Rowset生成ID失败 |\n| OLAP_ERR_ROWSET_DELETE_FILE_FAILED | -3103 | Rowset删除文件失败 | | OLAP_ERR_ROWSET_BUILDER_INIT | -3104 | Rowset初始化构建失败 |\n| OLAP_ERR_ROWSET_TYPE_NOT_FOUND | -3105 | Rowset类型没有发现 | | OLAP_ERR_ROWSET_ALREADY_EXIST | -3106 | Rowset已经存在 |\n| OLAP_ERR_ROWSET_CREATE_READER | -3107 | Rowset创建读对象失败 | | OLAP_ERR_ROWSET_INVALID | -3108 | Rowset无效 |\n| OLAP_ERR_ROWSET_READER_INIT | -3110 | Rowset读对象初始化失败 | | OLAP_ERR_ROWSET_INVALID_STATE_TRANSITION | -3112 | Rowset无效的事务状态 |\n| OLAP_ERR_ROWSET_RENAME_FILE_FAILED | -3116 | Rowset重命名文件失败 | | OLAP_ERR_SEGCOMPACTION_INIT_READER | -3117 | SegmentCompaction初始化Reader失败 |\n| OLAP_ERR_SEGCOMPACTION_INIT_WRITER | -3118 | SegmentCompaction初始化Writer失败 | | OLAP_ERR_SEGCOMPACTION_FAILED | -3119 | SegmentCompaction失败 | --- {\n""title"": ""服务自动拉起"", ""language"": ""zh-CN"" } ---\n<!--split--> # 服务自动拉起 本文档主要介绍如何配置Doris集群的自动拉起，保证生产环境中出现特殊情况导致服务宕机后未及时拉起服务从而影响到业务的正常运行。 Doris集群必须完全搭建完成后再配置FE和BE的自动拉起服务。 ## Systemd配置Doris服务 systemd具体使用以及参数解析可以参考[这里](https://systemd.io/) ### sudo 权限控制']","Doris支持多种云存储服务，包括但不限于AWS S3、Apache Hadoop HDFS以及通过Broker访问的其他云存储服务。通过创建仓库（REPOSITORY）的方式，Doris可以连接到这些云存储服务上。例如：

1. **AWS S3**：直接通过S3协议访问AWS上的S3服务。
2. **Hadoop HDFS**：直接访问HDFS存储。
3. **通过Broker访问的云存储**：Doris可以通过Broker机制连接到不同的云存储服务，例如Baidu Object Storage (BOS)、阿里云OSS等。Broker作为一个中间件，支持Doris访问多种类型的存储服务。

创建仓库时，需要指定云存储的相关配置，包括但不限于存储位置（LOCATION）、访问密钥和端点信息等。根据不同的云存储服务，这些配置项会有所不同。

综上，Doris通过灵活的配置，支持连接到多种云存储服务，为用户提供了广泛的数据存储和备份选项。",21.88,,ERROR,"['- 删除数据目录：\n\n    `rm -rf data/0/12345/`\n\n  - 删除 Tablet 元数据（具体参考 [Tablet 元数据管理工具](./tablet-meta-tool.md)）\n\n    `./lib/meta_tool --operation=delete_header --root_path=/path/to/root_path --tablet_id=12345 --schema_hash= 352781111`\n---\n{\n    ""title"": ""数据副本管理"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 数据副本管理\n\n从 0.9.0 版本开始，Doris 引入了优化后的副本管理策略，同时支持了更为丰富的副本状态查看工具。本文档主要介绍 Doris 数据副本均衡、修复方面的调度策略，以及副本管理的运维方法。帮助用户更方便的掌握和管理集群中的副本状态。\n\n> Colocation 属性的表的副本修复和均衡可以参阅[这里](../../query-acceleration/join-optimization/colocation-join.md)\n\n## 名词解释\n\n1. Tablet：Doris 表的逻辑分片，一个表有多个分片。\n2. Replica：分片的副本，默认一个分片有3个副本。\n3. Healthy Replica：健康副本，副本所在 Backend 存活，且副本的版本完整。\n4. TabletChecker（TC）：是一个常驻的后台线程，用于定期扫描所有的 Tablet，检查这些 Tablet 的状态，并根据检查结果，决定是否将 tablet 发送给 TabletScheduler。\n5. TabletScheduler（TS）：是一个常驻的后台线程，用于处理由 TabletChecker 发来的需要修复的 Tablet。同时也会进行集群副本均衡的工作。\n6. TabletSchedCtx（TSC）：是一个 tablet 的封装。当 TC 选择一个 tablet 后，会将其封装为一个 TSC，发送给 TS。\n7. Storage Medium：存储介质。Doris 支持对分区粒度指定不同的存储介质，包括 SSD 和 HDD。副本调度策略也是针对不同的存储介质分别调度的。\n\n```', '- 支持通过状态批量取消导入作业\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD)\n\n- Broker Load 新增支持阿里云 OSS，腾讯 CHDFS 和华为云 OBS。\n\n文档：[https://doris.apache.org/zh-CN/docs/dev/advanced/broker](https://doris.apache.org/zh-CN/docs/dev/advanced/broker)\n\n- 支持通过 hive-site.xml 文件配置访问 HDFS。\n\n文档：[https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir)\n\n### 5. 支持通过 `SHOW CATALOG RECYCLE BIN` 功能查看回收站中的内容。\n\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN)\n\n### 6. 支持 `SELECT * EXCEPT` 语法。', '### Stream load 支持带引号的 CSV 数据 \n\n通过导入任务参数 `trim_double_quotes` 来控制，默认值为 false，为 true 时表示裁剪掉 CSV 文件每个字段最外层的双引号。  [#15241](https://github.com/apache/doris/pull/15241)\n\n### Broker 支持腾讯云 CHDFS 和 百度云 BOS 、AFS \n\n可以通过 Broker 访问存储在腾讯云 CHDFS 和 百度智能云 BOS、AFS 上的数据。 [#15297](https://github.com/apache/doris/pull/15297) [#15448](https://github.com/apache/doris/pull/15448)\n\n### 新增函数\n\n新增函数 `substring_index`。 [#15373](https://github.com/apache/doris/pull/15373)\n\n\n\n# 问题修复\n\n- 修复部分情况下，从 1.1.x 版本升级到 1.2.0 版本后，用户权限信息丢失的问题。 [#15144](https://github.com/apache/doris/pull/15144)\n\n- 修复使用 date/datetimev2 类型进行分区时，分区值错误的问题。 [#15094](https://github.com/apache/doris/pull/15094)\n\n- 修复部分已发布功能的 Bug，具体列表可参阅：[PR List](https://github.com/apache/doris/pulls?q=is%3Apr+label%3Adev%2F1.2.1-merged+is%3Aclosed)\n\n\n# 升级注意事项\n\n### 已知问题\n\n- 请勿使用 JDK11 作为 BE 的运行时 JDK，会导致 BE Crash。\n\n- 该版本对csv格式的读取性能有下降，会影响csv格式的导入和读取效率，我们会在下一个三位版本尽快修复\n\n### 行为改变', '@qidaye\n\n@qzsee\n\n@SaintBacchus\n\n@SleepyBear96\n\n@smallhibiscus\n\n@spaces-X\n\n@stalary\n\n@starocean999\n\n@steadyBoy\n\n@SWJTU-ZhangLei\n\n@Tanya-W\n\n@tarepanda1024\n\n@tianhui5\n\n@Userwhite\n\n@wangbo\n\n@wangyf0555\n\n@weizuo93\n\n@whutpencil\n\n@wsjz\n\n@wunan1210\n\n@xiaokang\n\n@xinyiZzz\n\n@xlwh\n\n@xy720\n\n@yangzhg\n\n@Yankee24\n\n@yiguolei\n\n@yinzhijian\n\n@yixiutt\n\n@zbtzbtzbt\n\n@zenoyang\n\n@zhangstar333\n\n@zhangyifan27\n\n@zhannngchen\n\n@zhengshengjun\n\n@zhengshiJ\n\n@zingdle\n\n@zuochunwei\n\n@zy-kkk\n```---\n{\n    ""title"": ""Release 1.1.4"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n\n\n作为 1.1 LTS（Long-term Support，长周期支持）版本基础之上的 Bugfix 版本，在 Apache Doris 1.1.4 版本中，Doris 团队修复了自 1.1.3 版本以来的约 60 个 Issue 或性能优化项。改进了 Spark Load 的使用体验，优化了诸多内存以及 BE 异常宕机的问题，系统稳定性和性能得以进一步加强，推荐所有用户下载和使用。\n\n# 新增功能\n\n- Broker Load 支持 华为云 OBS 对象存储。[#13523](https://github.com/apache/doris/pull/13523)\n\n- Spark Load 支持 Parquet 和 Orc 文件。[#13438](https://github.com/apache/doris/pull/13438)\n\n\n# 优化改进\n\n- 禁用 Metric Hook 中的互斥量，其将影响数据导入过程中的查询性能。 [#10941](https://github.com/apache/doris/pull/10941)', '-   数据湖联邦查询：通过外表的方式联邦分析位于 Hive、Iceberg、Hudi 中的数据，在避免数据拷贝的前提下，查询性能大幅提升。\n\n# 技术概述\n\nDoris**整体架构**如下图所示，Doris 架构非常简单，只有两类进程\n\n-   **Frontend（FE）**，主要负责用户请求的接入、查询解析规划、元数据的管理、节点管理相关工作。\n\n-   **Backend（BE）**，主要负责数据存储、查询计划的执行。\n\n这两类进程都是可以横向扩展的，单集群可以支持到数百台机器，数十 PB 的存储容量。并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了一款分布式系统的运维成本。\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mnz20ae3s23vv3e9ltmi.png)\n\n在**使用接口**方面，Doris 采用 MySQL 协议，高度兼容 MySQL 语法，支持标准 SQL，用户可以通过各类客户端工具来访问 Doris，并支持与 BI 工具的无缝对接。Doris 当前支持多种主流的 BI 产品，包括不限于 SmartBI、DataEase、FineBI、Tableau、Power BI、SuperSet 等，只要支持 MySQL 协议的 BI 工具，Doris 就可以作为数据源提供查询支持。\n\n在**存储引擎**方面，Doris 采用列式存储，按列进行数据的编码压缩和读取，能够实现极高的压缩比，同时减少大量非相关数据的扫描，从而更加有效利用 IO 和 CPU 资源。\n\nDoris 也支持比较丰富的索引结构，来减少数据的扫描：\n\n-   Sorted Compound Key Index，可以最多指定三个列组成复合排序键，通过该索引，能够有效进行数据裁剪，从而能够更好支持高并发的报表场景\n\n-   Min/Max ：有效过滤数值类型的等值和范围查询\n\n-   Bloom Filter ：对高基数列的等值过滤裁剪非常有效\n\n-   Invert Index ：能够对任意字段实现快速检索\n\n在存储模型方面，Doris 支持多种存储模型，针对不同的场景做了针对性的优化：']",Doris支持多种云存储服务，包括阿里云OSS，腾讯云CHDFS，华为云OBS，以及百度云BOS和AFS。
22a30406-c173-11ee-80cd-b026288862a5,"如何在Apache Doris中创建外部表访问SqlServer？有参考样例可以看吗？
","可以使用以下参数创建外部表访问SqlServer：
   jdbc catalog mssql
   例如：
   CREATE EXTERNAL TABLE IF NOT EXISTS db_name.table_name
   (column_list)
   COMMENT 'comment'
   LOCATION 'jdbc:sqlserver://host:port;databaseName=dbname;user=username;password=password;encrypt=true;trustServerCertificate=false;loginTimeout=30;'
   PROPERTIES('jdbc.driverClassName' = 'com.microsoft.sqlserver.jdbc.SQLServerDriver', 'jdbc.catalog' = 'mssql');
   注意：需要将上述参数中的host、port、dbname、username和password替换为实际的SqlServer连接信息。","['""AWS_ACCESS_KEY"" = ""AWS_ACCESS_KEY"", ""AWS_SECRET_KEY""=""AWS_SECRET_KEY"", ""AWS_REGION"" = ""AWS_REGION"" ) PROPERTIES ( ""timeout"" = ""3600"" );\n``` ### 常见问题 1. S3 SDK 默认使用 `virtual-hosted style` 方式。但某些对象存储系统可能没开启或没支持 `virtual-hosted style` 方式的访问，此时我们可以添加 `use_path_style` 参数来强制使用 `path style` 方式：\n``` WITH S3 ( ""AWS_ENDPOINT"" = ""AWS_ENDPOINT"", ""AWS_ACCESS_KEY"" = ""AWS_ACCESS_KEY"", ""AWS_SECRET_KEY""=""AWS_SECRET_KEY"", ""AWS_REGION"" = ""AWS_REGION"", ""use_path_style"" = ""true"" )\n``` <version since=""1.2""> 2. 支持使用临时秘钥（TOKEN) 访问所有支持 S3 协议的对象存储，用法如下：\n``` WITH S3 ( ""AWS_ENDPOINT"" = ""AWS_ENDPOINT"", ""AWS_ACCESS_KEY"" = ""AWS_TEMP_ACCESS_KEY"", ""AWS_SECRET_KEY"" = ""AWS_TEMP_SECRET_KEY"", ""AWS_TOKEN"" = ""AWS_TEMP_TOKEN"", ""AWS_REGION"" = ""AWS_REGION"" )\n``` </version> --- { ""title"": ""数据导入事务及原子性"", ""language"": ""zh-CN"" } --- <!--split--> # 数据导入事务及原子性 Doris 中的所有导入操作都有原子性保证，即一个导入作业中的数据要么全部成功，要么全部失败。不会出现仅部分数据导入成功的情况。\n在 [BROKER LOAD](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/BROKER-LOAD.md) 中我们也可以实现多表的原子性导入。 对于表所附属的 [物化视图](../../../query-acceleration/materialized-view.md)，也同时保证和基表的原子性和一致性。 ## Label 机制\nDoris 的导入作业都可以设置一个 Label。这个 Label 通常是用户自定义的、具有一定业务逻辑属性的字符串。 Label 的主要作用是唯一标识一个导入任务，并且能够保证相同的 Label 仅会被成功导入一次。 Label 机制可以保证导入数据的不丢不重。如果上游数据源能够保证 At-Least-Once 语义，则配合 Doris 的 Label 机制，能够保证 Exactly-Once 语义。\nLabel 在一个数据库下具有唯一性。Label 的保留期限默认是 3 天。即 3 天后，已完成的 Label 会被自动清理，之后 Label 可以被重复使用。 ## 最佳实践 Label 通常被设置为 `业务逻辑+时间` 的格式。如 `my_business1_20220330_125000`。\n这个 Label 通常用于表示：业务 `my_business1` 这个业务在 `2022-03-30 12:50:00` 产生的一批数据。通过这种 Label 设定，业务上可以通过 Label 查询导入任务状态，来明确的获知该时间点批次的数据是否已经导入成功。如果没有成功，则可以使用这个 Label 继续重试导入。 --- { ""title"": ""通过外部表同步数据"", ""language"": ""zh-CN"" } --- <!--split--> # 通过外部表同步数据\nDoris 可以创建外部表。创建完成后，可以通过 SELECT 语句直接查询外部表的数据，也可以通过 `INSERT INTO SELECT` 的方式导入外部表的数据。 Doris 外部表目前支持的数据源包括： - MySQL - Oracle - PostgreSQL - SQLServer - Hive - Iceberg - ElasticSearch 本文档主要介绍如何创建通过 ODBC 协议访问的外部表，以及如何导入这些外部表的数据。 ## 创建外部表', '```sql mysql> show last insert\\G *************************** 1. row *************************** TransactionId: 64067 Label: insert_ba8f33aea9544866-8ed77e2844d0cc9b Database: default_cluster:db1 Table: t1\nTransactionStatus: VISIBLE LoadedRows: 2 FilteredRows: 0\n``` 该命令会返回 insert 以及对应事务的详细信息。因此，用户可以在每次执行完 insert 操作后，继续执行 `show last insert` 命令来获取 insert 的结果。 > 注意：该命令只会返回在同一 session 连接中，最近一次 insert 操作的结果。如果连接断开或更换了新的连接，则将返回空集。 ## 相关系统配置 ### FE 配置 + timeout\n导入任务的超时时间(以秒为单位)，导入任务在设定的 timeout 时间内未完成则会被系统取消，变成 CANCELLED。\n目前 Insert Into 并不支持自定义导入的 timeout 时间，所有 Insert Into 导入的超时时间是统一的，默认的 timeout 时间为4小时。如果导入的源文件无法在规定时间内完成导入，则需要调整 FE 的参数```insert_load_default_timeout_second```。 <version since=""dev""></version>\n同时 Insert Into 语句受到 Session 变量 `insert_timeout`的限制。可以通过 `SET insert_timeout = xxx;` 来增加超时时间，单位是秒。\n### Session 变量 + enable\\_insert\\_strict Insert Into 导入本身不能控制导入可容忍的错误率。用户只能通过 `enable_insert_strict` 这个 Session 参数用来控制。 当该参数设置为 false 时，表示至少有一条数据被正确导入，则返回成功。如果有失败数据，则还会返回一个 Label。 当该参数设置为 true 时，表示如果有一条数据错误，则导入失败。\n默认为 false。可通过 `SET enable_insert_strict = true;` 来设置。 + insert\\_timeout Insert Into 本身也是一个 SQL 命令，Insert Into 语句受到 Session 变量 <version since=""dev"" type=""inline"">`insert_timeout`</version> 的限制。可以通过 `SET insert_timeout = xxx;` 来增加超时时间，单位是秒。 ## 最佳实践\n### 应用场景\n1. 用户希望仅导入几条假数据，验证一下 Doris 系统的功能。此时适合使用 [INSERT INTO VALUES](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/INSERT.md) 的语法，这里语法和MySql语法一样。 2. 用户希望将已经在 Doris 表中的数据进行 ETL 转换并导入到一个新的 Doris 表中，此时适合使用 INSERT INTO SELECT 语法。\n3. 用户可以创建一种外部表，如 MySQL 外部表映射一张 MySQL 系统中的表。或者创建 [Broker](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/BROKER-LOAD.md) 外部表来映射 HDFS 上的数据文件。然后通过 INSERT INTO SELECT 语法将外部表中的数据导入到 Doris 表中存储。', 'Multi-Catalog 多源数据目录功能的目标在于能够帮助用户更方便对接外部数据目录，以增强 Apache Doris 的数据湖分析和联邦数据查询能力。\n在过去版本中，当我们需要对接外部数据源时，只能在 Database 或 Table 层级对接。当外部数据目录 Schema 发生变化、或者外部数据目录的 Database 或 Table 非常多时，需要用户手工进行一一映射，维护量非常大。1.2.0 版本新增的多源数据目录功能为 Apache Doris 提供了快速接入外部数据源进行访问的能力，用户可以通过 `CREATE CATALOG` 命令连接到外部数据源，Doris\n会自动映射外部数据源的库、表信息。之后，用户就可以像访问普通表一样，对这些外部数据源中的数据进行访问，避免了之前用户需要对每张表手动建立外表映射的复杂操作。\n目前能支持以下数据源： 1. Hive Metastore：可以访问包括 Hive、Iceberg、Hudi 在内的数据表，也可对接兼容 Hive Metastore 的数据源，如阿里云的 DataLake Formation，同时支持 HDFS 和对象存储上的数据访问。 2. Elasticsearch：访问 ES 数据源。 3. JDBC：支持通过 JDBC 访问 MySQL 数据源。 注：相应的权限层级也会自动变更，详见“升级注意事项”部分\n文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog) ### 4. 轻量表结构变更 Light Schema Change\n在过去版本中，Schema Change 是一项相对消耗比较大的工作，需要对数据文件进行修改，在集群规模和表数据量较大时执行效率会明显降低。同时由于是异步作业，当上游 Schema 发生变更时，需要停止数据同步任务并手动执行 Schema Change，增加开发和运维成本的同时还可能造成消费数据的积压。\n在 1.2.0 新版本中，对数据表的加减列操作，不再需要同步更改数据文件，仅需在 FE 中更新元数据即可，从而实现毫秒级的 Schema Change 操作，且存在导入任务时效率的提升更为显著。与此同时，使得 Apache Doris 在面对上游数据表维度变化时，可以更加快速稳定实现表结构同步，保证系统的高效且平稳运转。如用户可以通过 Flink CDC，可实现上游数据库到 Doris 的 DML 和 DDL 同步，进一步提升了实时数仓数据处理和分析链路的时效性与便捷性。\n![lightschemachange_compare.png](/images/lightschemachange_compare.png) 使用说明：作为新的 Feature 默认关闭，用户可以通过在建表时添加下面的 Property 来开启：\n``` ""light_schema_change"" = ""true""\n``` 文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE) ### 5. JDBC 外部表\n在过去版本中，Apache Doris 提供了 ODBC 外部表的方式来访问 MySQL、Oracle、SQL Server、PostgreSQL 等数据源，但由于 ODBC 驱动版本问题可能造成系统的不稳定。相对于 ODBC，JDBC 接口更为统一且支持数据库众多，因此在 1.2.0 版本中我们实现了 JDBC 外部表以替换原有的 ODBC 外部表。在新版本中，用户可以通过 JDBC 连接支持 JDBC 协议的外部数据源， 当前已适配的数据源包括：']","在Apache Doris中创建外部表以访问SQL Server的功能是通过JDBC外部表实现的。这一功能从1.2.0版本开始提供，允许用户通过JDBC连接支持JDBC协议的外部数据源，其中就包括SQL Server。

创建JDBC外部表的基本语法如下：

```sql
CREATE EXTERNAL TABLE `table_name` (
    `column_name` data_type [COMMENT 'column_comment'] [, ...]
)
ENGINE=JDBC
PROPERTIES (
    ""host"" = ""your_sql_server_host"",
    ""port"" = ""your_sql_server_port"",
    ""username"" = ""your_username"",
    ""password"" = ""your_password"",
    ""database"" = ""your_database_name"",
    ""table"" = ""your_table_name"",
    ""driver"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver"",
    ""instance"" = ""[your_sql_server_instance]"",
    ""type"" = ""mssql""
);
```

请注意，你需要将`your_sql_server_host`、`your_sql_server_port`、`your_username`、`your_password`、`your_database_name`和`your_table_name`替换为实际的SQL Server连接信息。如果SQL Server实例名（instance）不是默认的，还需要提供`your_sql_server_instance`。

此外，确保Doris集群能够访问SQL Server所在的网络，并且已经配置了相应的JDBC驱动。在创建表之前，需要将SQL Server的JDBC驱动（例如：`sqljdbc4.jar`或对应版本的驱动文件）放置到Doris集群的指定目录，并确保FE（Frontend）和BE（Backend）能够加载这个驱动。

请注意，这只是一个基本示例，实际的创建语句可能需要根据你的具体情况进行调整，例如数据类型映射、端口号、是否使用SSL等。

以上就是在Apache Doris中创建外部表以访问SQL Server的参考样例。如有更具体的需求或遇到问题，建议参照Apache Doris的官方文档或社区支持获取帮助。",34.72,,ERROR,"['Label 在一个数据库下具有唯一性。Label 的保留期限默认是 3 天。即 3 天后，已完成的 Label 会被自动清理，之后 Label 可以被重复使用。\n\n## 最佳实践\n\nLabel 通常被设置为 `业务逻辑+时间` 的格式。如 `my_business1_20220330_125000`。\n\n这个 Label 通常用于表示：业务 `my_business1` 这个业务在 `2022-03-30 12:50:00` 产生的一批数据。通过这种 Label 设定，业务上可以通过 Label 查询导入任务状态，来明确的获知该时间点批次的数据是否已经导入成功。如果没有成功，则可以使用这个 Label 继续重试导入。\n---\n{\n    ""title"": ""通过外部表同步数据"",\n    ""language"": ""zh-CN""\n}\n\n---\n\n<!--split-->\n\n# 通过外部表同步数据\n\nDoris 可以创建外部表。创建完成后，可以通过 SELECT 语句直接查询外部表的数据，也可以通过 `INSERT INTO SELECT` 的方式导入外部表的数据。\n\nDoris 外部表目前支持的数据源包括：\n\n- MySQL\n- Oracle\n- PostgreSQL\n- SQLServer\n- Hive \n- Iceberg\n- ElasticSearch\n\n本文档主要介绍如何创建通过 ODBC 协议访问的外部表，以及如何导入这些外部表的数据。\n\n## 创建外部表\n\n创建 ODBC 外部表的详细介绍请参阅 [CREATE EXTERNAL TABLE](../../../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-EXTERNAL-TABLE.md) 语法帮助手册。\n\n这里仅通过示例说明使用方式。\n\n1. 创建 ODBC Resource\n\n   ODBC Resource 的目的是用于统一管理外部表的连接信息。', '这里我们创建一个 `ext_oracle_demo` 外部表，并引用了之前创建的 `oracle_test_odbc` Resource\n\n## 导入数据\n\n1. 创建 Doris 表\n\n   这里我们创建一张 Doris 的表，列信息和上一步创建的外部表 `ext_oracle_demo` 一样：\n\n   ```sql\n   CREATE TABLE `doris_oralce_tbl` (\n     `k1` decimal(9, 3) NOT NULL COMMENT """",\n     `k2` char(10) NOT NULL COMMENT """",\n     `k3` datetime NOT NULL COMMENT """",\n     `k5` varchar(20) NOT NULL COMMENT """",\n     `k6` double NOT NULL COMMENT """"\n   )\n   COMMENT ""Doris Table""\n   DISTRIBUTED BY HASH(k1) BUCKETS 2\n   PROPERTIES (\n       ""replication_num"" = ""1""\n   );\n   ```\n\n   关于创建 Doris 表的详细说明，请参阅 [CREATE-TABLE](../../../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE.md) 语法帮助。\n\n2. 导入数据 (从 `ext_oracle_demo`表 导入到 `doris_oralce_tbl` 表)\n\n   \n\n   ```sql\n   INSERT INTO doris_oralce_tbl SELECT k1,k2,k3 FROM ext_oracle_demo limit 100;\n   ```\n\n   INSERT 命令是同步命令，返回成功，即表示导入成功。\n\n## 注意事项\n\n- 必须保证外部数据源与 Doris 集群是可以互通，包括BE节点和外部数据源的网络是互通的。\n- ODBC 外部表本质上是通过单一 ODBC 客户端访问数据源，因此并不合适一次性导入大量的数据，建议分批多次导入。\n\n## 更多帮助', '## 注意事项\n\n- 必须保证外部数据源与 Doris 集群是可以互通，包括BE节点和外部数据源的网络是互通的。\n- ODBC 外部表本质上是通过单一 ODBC 客户端访问数据源，因此并不合适一次性导入大量的数据，建议分批多次导入。\n\n## 更多帮助\n\n关于 CREATE EXTERNAL TABLE 的更多详细语法和最佳实践，请参阅 [CREATE EXTERNAL TABLE](../../../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-EXTERNAL-TABLE.md) 命令手册。\n---\n{\n    ""title"": ""导入严格模式"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n\n# 导入严格模式\n\n严格模式（strict_mode）为导入操作中的一个参数配置。该参数会影响某些数值的导入行为和最终导入的数据。\n\n本文档主要说明如何设置严格模式，以及严格模式产生的影响。\n\n## 如何设置\n\n严格模式默认情况下都为 False，即关闭状态。\n\n不同的导入方式设置严格模式的方式不尽相同。\n\n1. [BROKER LOAD](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/BROKER-LOAD)', '```\nDoris-Operator 部署的 Service 分为两类，后缀`-internal`为集群内部组件通信使用的 Service，后缀`-service`为用户可使用的 Service。 \n  \n**集群内部访问**  \n  \n在 K8S 内部可通过 Service 的`CLUSTER-IP`访问对应的组件。如上图可使用访问 FE 的 Service`doriscluster-sample-fe-service`对应的 CLUSTER-IP 为`10.152.183.37`，使用如下命令连接 FE 服务。\n```shell\nmysql -h 10.152.183.37 -uroot -P9030\n```\n  \n**集群外部访问**  \n  \nDoris 集群部署默认不提供 K8S 外部访问，如果集群需要被集群外部访问，需要集群能够申请 lb 资源。具备前提后，参考 [api.md](https://github.com/selectdb/doris-operator/blob/master/doc/api.md) 文档配置相关组件`service`字段，部署后通过对应 Service 的`EXTERNAL-IP`进行访问。以上图中 FE 为例，使用如下命令连接：\n```shell\nmysql -h a7509284bf3784983a596c6eec7fc212-618xxxxxx.com -uroot -P9030\n```\n### 后记\n本文简述 Doris 在 Kubernetes 的部署使用，Doris-Operator 提供的其他能力请参看[主要能力介绍](https://github.com/selectdb/doris-operator/tree/master/doc/how_to_use_cn.md)，DorisCluster 资源的 [api](https://github.com/selectdb/doris-operator/blob/master/doc/api.md) 可读性文档定制化部署 Doris 集群。', '在比较有代表性的 SSB-Flat 数据集上，通过模拟多个持续导入场景，**新版本的大部分查询取得了 3-6 倍的性能提升**。\n\n![mergeonwrite_ssb](/images/mergeonwrite_ssb.png)\n\n使用场景：所有对主键唯一性有需求，需要频繁进行实时 Upsert 更新的用户建议打开。\n\n使用说明：作为新的 Feature 默认关闭，用户可以通过在建表时添加下面的 Property 来开启：\n\n```\n“enable_unique_key_merge_on_write” = “true”\n```\n\n另外新版本 Merge-on-Write 数据更新模式与旧版本 Merge-on-Read 实现方式存在差异，因此已经创建的 Unique Key 表无法直接通过 Alter Table 添加 Property 来支持，只能在新建表的时候指定。如果用户需要将旧表转换到新表，可以使用 `insert into new_table select * from old_table` 的方式来实现。\n\n### 3. Multi Catalog 多源数据目录\n\nMulti-Catalog 多源数据目录功能的目标在于能够帮助用户更方便对接外部数据目录，以增强 Apache Doris 的数据湖分析和联邦数据查询能力。\n\n在过去版本中，当我们需要对接外部数据源时，只能在 Database 或 Table 层级对接。当外部数据目录 Schema 发生变化、或者外部数据目录的 Database 或 Table 非常多时，需要用户手工进行一一映射，维护量非常大。1.2.0 版本新增的多源数据目录功能为 Apache Doris 提供了快速接入外部数据源进行访问的能力，用户可以通过 `CREATE CATALOG` 命令连接到外部数据源，Doris 会自动映射外部数据源的库、表信息。之后，用户就可以像访问普通表一样，对这些外部数据源中的数据进行访问，避免了之前用户需要对每张表手动建立外表映射的复杂操作。\n\n目前能支持以下数据源：\n\n1. Hive Metastore：可以访问包括 Hive、Iceberg、Hudi 在内的数据表，也可对接兼容 Hive Metastore 的数据源，如阿里云的 DataLake Formation，同时支持 HDFS 和对象存储上的数据访问。', '使用说明：作为新的 Feature 默认关闭，用户可以通过在建表时添加下面的 Property 来开启：\n\n```\n""light_schema_change"" = ""true""\n```\n\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE)\n\n### 5. JDBC 外部表\n\n在过去版本中，Apache Doris 提供了 ODBC 外部表的方式来访问 MySQL、Oracle、SQL Server、PostgreSQL 等数据源，但由于 ODBC 驱动版本问题可能造成系统的不稳定。相对于 ODBC，JDBC 接口更为统一且支持数据库众多，因此在 1.2.0 版本中我们实现了 JDBC 外部表以替换原有的 ODBC 外部表。在新版本中，用户可以通过 JDBC 连接支持 JDBC 协议的外部数据源，\n\n当前已适配的数据源包括：\n\n- MySQL\n- PostgreSQL\n- Oracle\n- SQLServer\n- ClickHouse\n\n更多数据源的适配已经在规划之中，原则上任何支持 JDBC 协议访问的数据库均能通过 JDBC 外部表的方式来访问。而之前的 ODBC 外部表功能将会在后续的某个版本中移除，还请尽量切换到 JDBC 外表功能。\n\n文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/jdbc](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/jdbc)\n\n### 6. JAVA UDF']","要创建Apache Doris中的外部表以访问SQL Server，您需要遵循以下步骤：

1. 创建ODBC资源：ODBC资源用于统一管理外部表的连接信息。以下是创建ODBC资源的示例：

```sql
CREATE ODBC RESOURCE my_sqlserver_resource
WITH (
    driver=""FreeTDS"",
    server=""your_server_ip"",
    database=""your_database_name"",
    uid=""your_username"",
    pwd=""your_password""
);
```

1. 创建外部表：接下来，您可以使用CREATE EXTERNAL TABLE语句创建外部表，引用先前创建的ODBC资源。以下是创建SQL Server外部表的示例：

```sql
CREATE EXTERNAL TABLE ext_sqlserver_table (
    column1 data_type,
    column2 data_type,
    ...
)
USING ODBC
WITH (
    resource=""my_sqlserver_resource"",
    table_name=""your_external_table_name"",
    fetch_size=10000
);
```

请注意用您的实际数据类型、服务器IP、数据库名称、用户名、密码、表名和列信息替换上述示例中的占位符。

1. 查询或导入数据：一旦创建了外部表，您就可以使用SELECT语句直接查询外部表的数据，或者使用INSERT INTO SELECT语句将数据导入到Doris表中。

以下是一些示例查询和插入操作：

查询数据：
```sql
SELECT * FROM ext_sqlserver_table LIMIT 100;
```

将数据导入到Doris表中：
```sql
INSERT INTO your_doris_table SELECT * FROM ext_sqlserver_table WHERE some_condition LIMIT 1000;
```

请注意用您的实际Doris表名和条件替换上述示例中的占位符。在插入大量数据时，建议分批多次导入以提高效率。"
8f7cc4f7-c195-11ee-a6fc-b026288862a5,"Apache Doris的OUTFILE支持导出阿里的OSS吗？
","是的，OUTFILE支持导出到阿里的OSS。 
","['### 3. 更丰富权限管理 - 支持行级权限 可以通过 `CREATE ROW POLICY` 命令创建行级权限。\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-POLICY](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-POLICY) - 支持指定密码强度、过期时间等。\n- 支持在多次失败登录后锁定账户。 文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Account-Management-Statements/ALTER-USER](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Account-Management-Statements/ALTER-USER) ### 4. 导入相关\n- CSV 导入支持带 header 的 CSV 文件。\n在文档中搜索 `csv_with_names`：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD/](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD/)\n- Stream Load 新增 `hidden_columns`，可以显式指定 delete flag 列和 sequence 列。\n在文档中搜索 `hidden_columns`：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD)\n- Spark Load 支持 Parquet 和 ORC 文件导入。 - 支持清理已完成的导入的 Label\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CLEAN-LABEL](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CLEAN-LABEL)\n- 支持通过状态批量取消导入作业 文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD)\n- Broker Load 新增支持阿里云 OSS，腾讯 CHDFS 和华为云 OBS。 文档：[https://doris.apache.org/zh-CN/docs/dev/advanced/broker](https://doris.apache.org/zh-CN/docs/dev/advanced/broker) - 支持通过 hive-site.xml 文件配置访问 HDFS。', '[CloudCanal 最新版下载地址](https://www.clougence.com/) ## 功能说明\n- 推荐使用2.2.5.0及以上的CloudCanal版本写入Doris - 建议您在使用 CloudCanal 将 **增量数据** 导入至 Doris 时，控制导入的频率，CloudCanal写入Doris的默认导入频率可以通过参数`realFlushPauseSec`调整，默认为10秒。\n- 当前社区版本最大的内存配置为2g，如果同步任务运行产生OOM异常或者GC停顿严重可以调小以下参数减少批次大小从而减少内存占用。全量参数为`fullBatchSize`和`fullRingBufferSize`，增量参数为`increBatchSize`和`increRingBufferSize` - 支持的源端以及功能项：\n| 数据源 \\ 功能项 | 结构迁移 | 全量数据迁移 | 增量实时同步 | 数据校验 | | --- | --- | --- | --- | --- | | Oracle 源端 | 支持 | 支持 | 支持 | 支持 | | PostgreSQL 源端 | 支持 | 支持 | 支持 | 支持 | | Greenplum 源端 | 支持 | 支持 | 不支持 | 支持 | | MySQL 源端 | 支持 | 支持 | 支持 | 支持 | ## 使用方法\nCloudCanal 提供了完整的产品化能力，用户在可视化界面完成数据源添加和任务创建即可自动完成结构迁移、全量迁移、增量实时同步。下文演示如何将 MySQL 数据库中的数据迁移同步到对端 Doris 中。其他源端同步到 Doris 也可以按照类似的方式进行。 ### 前置条件 首先参考 [CloudCanal 快速开始](https://www.clougence.com/cc-doc/quick/quick_start) 完成 CloudCanal 社区版的安装和部署。 ### 添加数据源\n- 登录 CloudCanal 平台 - 数据源管理-> 新增数据源 - 选择自建数据库中 Doris ![image.png](/images/cloudcanal/cloudcanal-1.png) > Tips: > > - Client 地址： 为 Doris 提供给 MySQL Client 的服务端口，CloudCanal 主要用其查询库表的元数据信息 > > - Http 地址： Http 地址主要用于接收 CloudCanal 数据导入的请求 ### 任务创建\n添加好数据源之后可以按照如下步骤进行数据迁移、同步任务的创建。 - **任务管理**-> **任务创建** - 选择 **源** 和 **目标** 数据库 - 点击 下一步 ![image.png](/images/cloudcanal/cloudcanal-2.png) - 选择 **增量同步**，并且启用 **全量数据初始化** - 勾选 DDL 同步 - 点击下一步 ![image.png](/images/cloudcanal/cloudcanal-3.png)\n- 选择订阅的表，**结构迁移自动创建的表为主键模型的表，因此暂不支持无主键表** - 点击下一步 ![image.png](/images/cloudcanal/cloudcanal-5.png) - 配置列映射 - 点击下一步 ![image.png](/images/cloudcanal/cloudcanal-6.png) - 创建任务 ![image.png](/images/cloudcanal/cloudcanal-7.png)\n- 查看任务状态。任务创建后，会自动完成结构迁移、全量、增量阶段。 ![image.png](/images/cloudcanal/cloudcanal-8.jpg) ## 参考资料 更多关于 CloudCanal 同步 Doris 的资料，可以查看 - [5 分钟搞定 PostgreSQL 到 Doris 数据迁移同步-CloudCanal 实战](https://www.clougence.com/cc-doc/blog/postgresql_doris_sync/)\n- [CloudCanal 官网](https://www.clougence.com/) --- { ""title"": ""OUTFILE"", ""language"": ""zh-CN"" } --- <!--split--> ## OUTFILE ### Name OURFILE ### description\n`SELECT INTO OUTFILE` 命令用于将查询结果导出为文件。目前支持通过 Broker 进程, S3 协议或HDFS 协议，导出到远端存储，如 HDFS，S3，BOS，COS（腾讯云）上。 #### 语法：', 'Broker 相关属性需加前缀 `broker.`： broker.name: broker名称 broker.hadoop.security.authentication: 指定认证方式为 kerberos broker.kerberos_principal: 指定 kerberos 的 principal\nbroker.kerberos_keytab: 指定 kerberos 的 keytab 文件路径。该文件必须为 Broker 进程所在服务器上的文件的绝对路径。并且可以被 Broker 进程访问 HDFS 相关属性: fs.defaultFS: namenode 地址和端口 hadoop.username: hdfs 用户名\ndfs.nameservices: name service名称，与hdfs-site.xml保持一致 dfs.ha.namenodes.[nameservice ID]: namenode的id列表,与hdfs-site.xml保持一致 dfs.namenode.rpc-address.[nameservice ID].[name node ID]: Name node的rpc地址，数量与namenode数量相同，与hdfs-site.xml保持一致\ndfs.client.failover.proxy.provider.[nameservice ID]: HDFS客户端连接活跃namenode的java类，通常是""org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider""\n对于开启kerberos认证的Hadoop 集群，还需要额外设置如下 PROPERTIES 属性: dfs.namenode.kerberos.principal: HDFS namenode 服务的 principal 名称 hadoop.security.authentication: 认证方式设置为 kerberos\nhadoop.kerberos.principal: 设置 Doris 连接 HDFS 时使用的 Kerberos 主体 hadoop.kerberos.keytab: 设置 keytab 本地文件路径\nS3 协议则直接执行 S3 协议配置即可: s3.endpoint s3.access_key s3.secret_key s3.region\nuse_path_stype: (选填) 默认为false 。S3 SDK 默认使用 virtual-hosted style 方式。但某些对象存储系统可能没开启或不支持virtual-hosted style 方式的访问，此时可以添加 use_path_style 参数来强制使用 path style 访问方式。\n``` > 注意：若要使用delete_existing_files参数，还需要在fe.conf中添加配置`enable_delete_existing_files = true`并重启fe，此时delete_existing_files才会生效。delete_existing_files = true 是一个危险的操作，建议只在测试环境中使用。 4. 导出的数据类型\n所有文件类型都支持到处基本数据类型，而对于复杂数据类型（ARRAY/MAP/STRUCT），当前只有csv/orc/csv_with_names/csv_with_names_and_types支持导出复杂类型,且不支持嵌套复杂类型。 5. 并发导出\n设置session变量```set enable_parallel_outfile = true;```可开启outfile并发导出，详细使用方法见[导出查询结果集](../../../data-operate/export/outfile.md) 6. 导出到本地 导出到本地文件时需要先在fe.conf中配置`enable_outfile_to_local=true`']","Apache Doris 的 `SELECT INTO OUTFILE` 命令支持通过 Broker 进程导出数据到远端存储，如 HDFS、S3、BOS、COS（腾讯云）等。关于阿里云 OSS 的直接支持，上述提供的文档中没有明确说明 `SELECT INTO OUTFILE` 命令支持导出数据到阿里云 OSS。然而，由于 OSS 与 S3 兼容，理论上可以通过配置 S3 协议的方式来实现对 OSS 的支持。这需要配置相关的 S3 协议参数，如 `s3.endpoint`、`s3.access_key`、`s3.secret_key` 和 `s3.region`，来适配 OSS 的访问。

综上所述，虽然文档中没有直接提到对阿里云 OSS 的支持，但通过 S3 协议的兼容性，可以间接实现数据的导出到 OSS。具体操作时，用户需要根据 OSS 的 S3 兼容接口来配置相关参数。",21.87,,ERROR,"['- 支持通过状态批量取消导入作业\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD)\n\n- Broker Load 新增支持阿里云 OSS，腾讯 CHDFS 和华为云 OBS。\n\n文档：[https://doris.apache.org/zh-CN/docs/dev/advanced/broker](https://doris.apache.org/zh-CN/docs/dev/advanced/broker)\n\n- 支持通过 hive-site.xml 文件配置访问 HDFS。\n\n文档：[https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir)\n\n### 5. 支持通过 `SHOW CATALOG RECYCLE BIN` 功能查看回收站中的内容。\n\n文档：[https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN)\n\n### 6. 支持 `SELECT * EXCEPT` 语法。', '</version>\n\n### Keywords\n\n    UPDATE\n---\n{\n    ""title"": ""EXPORT"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## EXPORT\n\n### Name\n\nEXPORT\n\n### Description\n\n `EXPORT` 命令用于将指定表的数据导出为文件到指定位置。目前支持通过 Broker 进程, S3 协议或HDFS 协议，导出到远端存储，如 HDFS，S3，BOS，COS（腾讯云）上。\n\n`EXPORT`是一个异步操作，该命令会提交一个`EXPORT JOB`到Doris，任务提交成功立即返回。执行后可使用 [SHOW EXPORT](../../Show-Statements/SHOW-EXPORT.md) 命令查看进度。\n\n语法：\n\n  ```sql\n  EXPORT TABLE table_name\n  [PARTITION (p1[,p2])]\n  [WHERE]\n  TO export_path\n  [opt_properties]\n  WITH BROKER/S3/HDFS\n  [broker_properties];\n  ```\n\n\n\n**说明**：\n\n- `table_name`\n\n  当前要导出的表的表名。支持 Doris 本地表、视图View、Catalog外表数据的导出。\n\n- `partition`\n\n  可以只导出指定表的某些指定分区，只对Doris本地表有效。\n\n- `export_path`\n\n  导出的文件路径。可以是目录，也可以是文件目录加文件前缀，如`hdfs://path/to/my_file_`\n\n- `opt_properties`\n\n  用于指定一些导出参数。\n\n  ```sql\n  [PROPERTIES (""key""=""value"", ...)]\n  ```\n\n  可以指定如下参数：\n\n  - `label`: 可选参数，指定此次Export任务的label，当不指定时系统会随机生成一个label。\n\n  - `column_separator`：指定导出的列分隔符，默认为\\t，支持多字节。该参数只用于csv文件格式。\n\n  - `line_delimiter`：指定导出的行分隔符，默认为\\n，支持多字节。该参数只用于csv文件格式。']","是的，Apache Doris 的 `EXPORT` 命令支持通过 Broker 进程将数据导出到阿里云 OSS。您只需要在 `EXPORT` 命令中指定 `TO export_path` 为 OSS 的路径，并确保您的 Doris 集群已经配置了正确的 OSS 访问参数即可。

例如，如果您想将名为 `my_table` 的表导出到阿里云 OSS 的 `my_bucket` 桶中，您可以使用以下 SQL 命令：

```sql
EXPORT TABLE my_table
TO 'oss://my_bucket/my_export_path/'
WITH BROKER
```

请确保您的 OSS 桶已经存在，并且您的 Doris 集群有足够的权限访问 OSS 服务。此外，您可能还需要根据您的具体需求，设置一些导出参数，比如列分隔符、行分隔符等。这些参数可以在 `opt_properties` 中指定。"
90607b0e-c18a-11ee-83af-b026288862a5,在执行BACKUP SNAPSHOT后，通过show backup查看备份状态时，报错ERROR 2020 (HY000): Got packet bigger than 'max_allowed_packet' bytes，有遇到过这个问题的吗？,这个问题可能是由于备份数据量过大，导致返回的数据包大小超过了max_allowed_packet参数设置的大小。可以尝试增大max_allowed_packet参数的数值，或者通过分批备份的方式来解决这个问题。,"['``` ### Keywords SHOW, PARTITION, ID ### Best Practice --- { ""title"": ""SHOW ALTER TABLE MATERIALIZED VIEW"", ""language"": ""zh-CN"" } --- <!--split--> ## SHOW ALTER TABLE MATERIALIZED VIEW ### Name SHOW ALTER TABLE MATERIALIZED VIEW\n### Description 该命令用于查看通过 [CREATE-MATERIALIZED-VIEW](../../sql-reference/Data-Definition-Statements/Create/CREATE-MATERIALIZED-VIEW.md) 语句提交的创建物化视图作业的执行情况。 > 该语句等同于 `SHOW ALTER TABLE ROLLUP`;\n```sql SHOW ALTER TABLE MATERIALIZED VIEW [FROM database] [WHERE] [ORDER BY] [LIMIT OFFSET]\n``` - database：查看指定数据库下的作业。如不指定，使用当前数据库。 - WHERE：可以对结果列进行筛选，目前仅支持对以下列进行筛选： - TableName：仅支持等值筛选。 - State：仅支持等值筛选。 - Createtime/FinishTime：支持 =，>=，<=，>，<，!= - ORDER BY：可以对结果集按任意列进行排序。 - LIMIT：配合 ORDER BY 进行翻页查询。 返回结果说明：\n```sql mysql> show alter table materialized view\\G *************************** 1. row *************************** JobId: 11001 TableName: tbl1 CreateTime: 2020-12-23 10:41:00 FinishTime: NULL BaseIndexName: tbl1\nRollupIndexName: r1 RollupId: 11002 TransactionId: 5070 State: WAITING_TXN Msg: Progress: NULL Timeout: 86400 1 row in set (0.00 sec)\n``` - `JobId`：作业唯一ID。 - `TableName`：基表名称 - `CreateTime/FinishTime`：作业创建时间和结束时间。 - `BaseIndexName/RollupIndexName`：基表名称和物化视图名称。 - `RollupId`：物化视图的唯一 ID。 - `TransactionId`：见 State 字段说明。 - `State`：作业状态。 - PENDING：作业准备中。 - WAITING_TXN：\n在正式开始产生物化视图数据前，会等待当前这个表上的正在运行的导入事务完成。而 `TransactionId` 字段就是当前正在等待的事务ID。当这个ID之前的导入都完成后，就会实际开始作业。 - RUNNING：作业运行中。 - FINISHED：作业运行成功。 - CANCELLED：作业运行失败。 - `Msg`：错误信息 - `Progress`：作业进度。这里的进度表示 `已完成的tablet数量/总tablet数量`。创建物化视图是按 tablet 粒度进行的。\n- `Timeout`：作业超时时间，单位秒。 ### Example 1. 查看数据库 example_db 下的物化视图作业 ```sql SHOW ALTER TABLE MATERIALIZED VIEW FROM example_db;\n``` ### Keywords SHOW, ALTER, TABLE, MATERIALIZED, VIEW ### Best Practice --- { ""title"": ""SHOW-SNAPSHOT"", ""language"": ""zh-CN"" } --- <!--split--> ## SHOW-SNAPSHOT ### Name SHOW SNAPSHOT ### Description 该语句用于查看仓库中已存在的备份。 语法：\n```sql SHOW SNAPSHOT ON `repo_name` [WHERE SNAPSHOT = ""snapshot"" [AND TIMESTAMP = ""backup_timestamp""]];', 'RollupIndexName: r1 RollupId: 11002 TransactionId: 5070 State: WAITING_TXN Msg: Progress: NULL Timeout: 86400 1 row in set (0.00 sec)\n``` - `JobId`：作业唯一ID。 - `TableName`：基表名称 - `CreateTime/FinishTime`：作业创建时间和结束时间。 - `BaseIndexName/RollupIndexName`：基表名称和物化视图名称。 - `RollupId`：物化视图的唯一 ID。 - `TransactionId`：见 State 字段说明。 - `State`：作业状态。 - PENDING：作业准备中。 - WAITING_TXN：\n在正式开始产生物化视图数据前，会等待当前这个表上的正在运行的导入事务完成。而 `TransactionId` 字段就是当前正在等待的事务ID。当这个ID之前的导入都完成后，就会实际开始作业。 - RUNNING：作业运行中。 - FINISHED：作业运行成功。 - CANCELLED：作业运行失败。 - `Msg`：错误信息 - `Progress`：作业进度。这里的进度表示 `已完成的tablet数量/总tablet数量`。创建物化视图是按 tablet 粒度进行的。\n- `Timeout`：作业超时时间，单位秒。 ### Example 1. 查看数据库 example_db 下的物化视图作业 ```sql SHOW ALTER TABLE MATERIALIZED VIEW FROM example_db;\n``` ### Keywords SHOW, ALTER, TABLE, MATERIALIZED, VIEW ### Best Practice --- { ""title"": ""SHOW-SNAPSHOT"", ""language"": ""zh-CN"" } --- <!--split--> ## SHOW-SNAPSHOT ### Name SHOW SNAPSHOT ### Description 该语句用于查看仓库中已存在的备份。 语法：\n```sql SHOW SNAPSHOT ON `repo_name` [WHERE SNAPSHOT = ""snapshot"" [AND TIMESTAMP = ""backup_timestamp""]];\n```\n说明： 1. 各列含义如下： Snapshot： 备份的名称 Timestamp： 对应备份的时间版本 Status： 如果备份正常，则显示 OK，否则显示错误信息 2. 如果指定了 TIMESTAMP，则会额外显示如下信息： Database： 备份数据原属的数据库名称\nDetails： 以 Json 的形式，展示整个备份的数据目录及文件结构\n### Example 1. 查看仓库 example_repo 中已有的备份 ```sql SHOW SNAPSHOT ON example_repo;\n``` 2. 仅查看仓库 example_repo 中名称为 backup1 的备份： ```sql SHOW SNAPSHOT ON example_repo WHERE SNAPSHOT = ""backup1"";', '``` 3. 从 example_repo 中恢复备份 snapshot_3 中除了表 backup_tbl 的其他所有表到数据库 example_db1，时间版本为 ""2018-05-04-18-12-18""。\n```sql RESTORE SNAPSHOT example_db1.`snapshot_3` FROM `example_repo` EXCLUDE ( `backup_tbl` ) PROPERTIES ( ""backup_timestamp""=""2018-05-04-18-12-18"" );\n``` ### Keywords RESTORE ### Best Practice 1. 同一数据库下只能有一个正在执行的恢复操作。 2. 可以将仓库中备份的表恢复替换数据库中已有的同名表，但须保证两张表的表结构完全一致。表结构包括：表名、列、分区、物化视图等等。 3. 当指定恢复表的部分分区时，系统会检查分区范围是否能够匹配。 4. 恢复操作的效率：\n在集群规模相同的情况下，恢复操作的耗时基本等同于备份操作的耗时。如果想加速恢复操作，可以先通过设置 `replication_num` 参数，仅恢复一个副本，之后在通过调整副本数 [ALTER TABLE PROPERTY](../../Data-Definition-Statements/Alter/ALTER-TABLE-PROPERTY.md)，将副本补齐。 --- { ""title"": ""DROP-REPOSITORY"", ""language"": ""zh-CN"" } ---\n<!--split--> ## DROP-REPOSITORY ### Name DROP REPOSITORY ### Description 该语句用于删除一个已创建的仓库。仅 root 或 superuser 用户可以删除仓库。 语法： ```sql DROP REPOSITORY `repo_name`;\n``` 说明： - 删除仓库，仅仅是删除该仓库在 Palo 中的映射，不会删除实际的仓库数据。删除后，可以再次通过指定相同的 broker 和 LOCATION 映射到该仓库。 ### Example 1. 删除名为 bos_repo 的仓库： ```sql DROP REPOSITORY `bos_repo`;\n``` ### Keywords DROP, REPOSITORY ### Best Practice --- { ""title"": ""CANCEL-RESTORE"", ""language"": ""zh-CN"" } --- <!--split--> ## CANCEL-RESTORE ### Name CANCEL RESTORE ### Description 该语句用于取消一个正在进行的 RESTORE 任务。 语法：\n```sql CANCEL RESTORE FROM db_name;\n``` 注意： - 当取消处于 COMMIT 或之后阶段的恢复左右时，可能导致被恢复的表无法访问。此时只能通过再次执行恢复作业进行数据恢复。 ### Example 1. 取消 example_db 下的 RESTORE 任务。 ```sql CANCEL RESTORE FROM example_db;\n``` ### Keywords CANCEL, RESTORE ### Best Practice --- { ""title"": ""BACKUP"", ""language"": ""zh-CN"" } --- <!--split--> ## BACKUP ### Name BACKUP ### Description 该语句用于备份指定数据库下的数据。该命令为异步操作。 仅 root 或 superuser 用户可以创建仓库。\n提交成功后，需通过 SHOW BACKUP 命令查看进度。仅支持备份 OLAP 类型的表。 语法： ```sql BACKUP SNAPSHOT [db_name].{snapshot_name} TO `repository_name` [ON|EXCLUDE] ( `table_name` [PARTITION (`p1`, ...)], ... ) PROPERTIES (""key""=""value"", ...);']","在执行`BACKUP SNAPSHOT`后，使用`SHOW BACKUP`命令查看备份状态时遇到""ERROR 2020 (HY000): Got packet bigger than 'max_allowed_packet' bytes""的问题，通常是因为MySQL或兼容的数据库服务器配置中的`max_allowed_packet`参数设置过小导致的。该参数限制了MySQL客户端和服务器之间传输的最大数据包大小。

解决该问题的方法是增加`max_allowed_packet`的值。你可以通过以下两种方式之一来调整该参数：

1. **临时调整**（对当前会话有效，重启后失效）：
   执行SQL命令来临时增加`max_allowed_packet`的值：
   ```sql
   SET GLOBAL max_allowed_packet=1073741824; -- 设置为1GB，或根据需要调整
   ```
   注意，该方法仅对当前数据库会话有效，数据库重启后将恢复默认值。

2. **永久调整**（修改配置文件）：
   在MySQL的配置文件（通常是`my.cnf`或`my.ini`，位置根据操作系统和MySQL版本而异）中，增加或修改`max_allowed_packet`参数：
   ```
   [mysqld]
   max_allowed_packet=1073741824
   ```
   修改配置文件后，需要重启MySQL服务以使更改生效。

建议首先尝试临时调整方法，以验证是否解决了问题。如果问题解决，再进行永久调整以避免将来再次遇到此问题。

注意：在调整`max_allowed_packet`值时，应确保设置的值不会超过服务器可接受的最大值，并考虑到增大数据包大小可能对服务器性能产生的影响。",38.95,,ERROR,"['- `Timeout`：作业超时时间，单位秒。\n\n### Example\n\n1. 查看数据库 example_db 下的物化视图作业\n\n   ```sql\n   SHOW ALTER TABLE MATERIALIZED VIEW FROM example_db;\n   ```\n\n### Keywords\n\n    SHOW, ALTER, TABLE, MATERIALIZED, VIEW\n\n### Best Practice\n\n---\n{\n    ""title"": ""SHOW-SNAPSHOT"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## SHOW-SNAPSHOT\n\n### Name\n\nSHOW SNAPSHOT\n\n### Description\n\n该语句用于查看仓库中已存在的备份。\n\n语法：\n\n```sql\nSHOW SNAPSHOT ON `repo_name`\n[WHERE SNAPSHOT = ""snapshot"" [AND TIMESTAMP = ""backup_timestamp""]];\n```\n\n说明：\n        1. 各列含义如下：\n            Snapshot：   备份的名称\n            Timestamp：  对应备份的时间版本\n            Status：     如果备份正常，则显示 OK，否则显示错误信息\n                2. 如果指定了 TIMESTAMP，则会额外显示如下信息：\n            Database：   备份数据原属的数据库名称\n            Details：    以 Json 的形式，展示整个备份的数据目录及文件结构\n\n### Example\n\n1. 查看仓库 example_repo 中已有的备份\n    \n    ```sql\n    SHOW SNAPSHOT ON example_repo;\n    ```\n\n2. 仅查看仓库 example_repo 中名称为 backup1 的备份：\n    \n    ```sql\n    SHOW SNAPSHOT ON example_repo WHERE SNAPSHOT = ""backup1"";\n    ```', '## 相关命令\n\n和备份恢复功能相关的命令如下。以下命令，都可以通过 mysql-client 连接 Doris 后，使用 `help cmd;` 的方式查看详细帮助。\n\n1. CREATE REPOSITORY\n\n   创建一个远端仓库路径，用于备份或恢复。该命令需要借助 Broker 进程访问远端存储，不同的 Broker 需要提供不同的参数，具体请参阅 [Broker文档](../../advanced/broker.md)，也可以直接通过S3 协议备份到支持AWS S3协议的远程存储上去，也可以直接备份到HDFS，具体参考 [创建远程仓库文档](../../sql-manual/sql-reference/Data-Definition-Statements/Backup-and-Restore/CREATE-REPOSITORY.md)\n\n2. BACKUP\n\n   执行一次备份操作。\n\n3. SHOW BACKUP\n\n   查看最近一次 backup 作业的执行情况，包括：', '2. BACKUP\n\n   执行一次备份操作。\n\n3. SHOW BACKUP\n\n   查看最近一次 backup 作业的执行情况，包括：\n\n   - JobId：本次备份作业的 id。\n   - SnapshotName：用户指定的本次备份作业的名称（Label）。\n   - DbName：备份作业对应的 Database。\n   - State：备份作业当前所在阶段：\n     - PENDING：作业初始状态。\n     - SNAPSHOTING：正在进行快照操作。\n     - UPLOAD_SNAPSHOT：快照结束，准备上传。\n     - UPLOADING：正在上传快照。\n     - SAVE_META：正在本地生成元数据文件。\n     - UPLOAD_INFO：上传元数据文件和本次备份作业的信息。\n     - FINISHED：备份完成。\n     - CANCELLED：备份失败或被取消。\n   - BackupObjs：本次备份涉及的表和分区的清单。\n   - CreateTime：作业创建时间。\n   - SnapshotFinishedTime：快照完成时间。\n   - UploadFinishedTime：快照上传完成时间。\n   - FinishedTime：本次作业完成时间。\n   - UnfinishedTasks：在 `SNAPSHOTTING`，`UPLOADING` 等阶段，会有多个子任务在同时进行，这里展示的当前阶段，未完成的子任务的 task id。\n   - TaskErrMsg：如果有子任务执行出错，这里会显示对应子任务的错误信息。\n   - Status：用于记录在整个作业过程中，可能出现的一些状态信息。\n   - Timeout：作业的超时时间，单位是秒。\n\n4. SHOW SNAPSHOT\n\n   查看远端仓库中已存在的备份。\n\n   - Snapshot：备份时指定的该备份的名称（Label）。\n   - Timestamp：备份的时间戳。\n   - Status：该备份是否正常。\n\n   如果在 `SHOW SNAPSHOT` 后指定了 where 子句，则可以显示更详细的备份信息。\n\n   - Database：备份时对应的 Database。\n   - Details：展示了该备份完整的数据目录结构。\n\n5. CANCEL BACKUP']","在执行 `BACKUP SNAPSHOT` 命令后，如果在通过 `SHOW BACKUP` 查看备份状态时遇到错误 `ERROR 2020 (HY000): Got packet bigger than 'max_allowed_packet' bytes`，这通常意味着备份过程中产生的数据包大小超过了系统配置的 `max_allowed_packet` 参数的限制。

`max_allowed_packet` 是 MySQL 客户端配置的一个参数，用来限制客户端能够发送和接收的最大数据包大小。如果在执行备份操作时，生成的数据包大小超过了这个值，就会导致报错，并且无法正确显示备份状态。

解决这个问题，通常需要按照以下步骤进行：

1. 检查 `max_allowed_packet` 参数的设置值，确保它足够大，以容纳备份操作中产生的数据包。
2. 如果需要，可以在 MySQL 客户端配置文件中（如 `my.cnf` 或 `my.ini`）调整 `max_allowed_packet` 的值，例如将其设置为 `1024M` 或更大的值。
3. 确保配置文件已经被正确加载，可以通过在 MySQL 客户端中执行 `SHOW VARIABLES LIKE 'max_allowed_packet';` 命令来验证 `max_allowed_packet` 参数的实际值。
4. 调整参数后，需要重启 MySQL 服务使更改生效。

如果调整 `max_allowed_packet` 参数后问题仍然存在，可能还需要检查其他相关配置，或联系 Doris 社区寻求更专业的帮助。

在处理这类问题时，请确保遵循系统管理的最佳实践，并在进行任何配置更改前做好备份，以防不测。"
f87e6679-c17f-11ee-9297-b026288862a5,"Doris中对分区数有没有限制？一张表最大能添加多少个分区？
","Doris中默认分区数的限制是500个，但可以通过修改配置来改变这个限制。可以通过 `FROM(...) TO (...) INTERVAL ...` 来批量创建分区，如果需要创建很多历史分区的话。

这样的QA对可以帮助其他Doris用户在遇到类似问题时快速找到答案。","['### load_cluster --- { ""title"": ""UNION"", ""language"": ""zh-CN"" } --- <!--split--> ## UNION <version since=""2.0.0""> </version> ### description #### Syntax `AGGREGATE_FUNCTION_UNION(agg_state)` 将多个聚合中间结果聚合为一个。 结果的类型为agg_state，函数签名与入参一致。\n### example\n``` mysql [test]>select avg_merge(t) from (select avg_union(avg_state(1)) as t from d_table group by k1)p; +----------------+ | avg_merge(`t`) | +----------------+ | 1 | +----------------+\n``` ### keywords AGG_STATE, UNION --- { ""title"": ""STATE"", ""language"": ""zh-CN"" } --- <!--split--> ## STATE <version since=""2.0.0""> </version> ### description #### Syntax\n`AGGREGATE_FUNCTION_STATE(arg...)` 返回聚合函数的中间结果，可以用于后续的聚合或者通过merge组合器获得实际计算结果，也可以直接写入agg_state类型的表保存下来。 结果的类型为agg_state，agg_state中的函数签名为`AGGREGATE_FUNCTION(arg...)`。 ### example\n``` mysql [test]>select avg_merge(t) from (select avg_union(avg_state(1)) as t from d_table group by k1)p; +----------------+ | avg_merge(`t`) | +----------------+ | 1 | +----------------+\n``` ### keywords AGG_STATE,STATE --- { ""title"": ""MERGE"", ""language"": ""zh-CN"" } --- <!--split--> ## MERGE <version since=""2.0.0""> </version> ### description #### Syntax\n`AGGREGATE_FUNCTION_MERGE(agg_state)` 将聚合中间结果进行聚合并计算获得实际结果。 结果的类型与`AGGREGATE_FUNCTION`一致。 ### example\n``` mysql [test]>select avg_merge(avg_state(1)) from d_table; +-------------------------+ | avg_merge(avg_state(1)) | +-------------------------+ | 1 | +-------------------------+\n``` ### keywords AGG_STATE, MERGE --- { ""title"": ""临时分区"", ""language"": ""zh-CN"" } --- <!--split--> # 临时分区 在 0.12 版本中，Doris 支持了临时分区功能。 临时分区是归属于某一分区表的。只有分区表可以创建临时分区。 ## 规则\n- 临时分区的分区列和正式分区相同，且不可修改。 - 一张表所有临时分区之间的分区范围不可重叠，但临时分区的范围和正式分区范围可以重叠。 - 临时分区的分区名称不能和正式分区以及其他临时分区重复。 ## 支持的操作 临时分区支持添加、删除、替换操作。 ### 添加临时分区 可以通过 `ALTER TABLE ADD TEMPORARY PARTITION` 语句对一个表添加临时分区：', '第二种是 rpc 的包大小超过 max_body_size。如果查询中带有超大 String 类型，或者 bitmap 类型时，可能出现这个问题。可以通过修改以下 BE 参数规避：\n``` brpc_max_body_size：默认 3GB.\n``` ### Q10. [ Broker load ] org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe 出现这个问题的原因可能是到从外部存储（例如HDFS）导入数据的时候，因为目录下文件太多，列出文件目录的时间太长，这里Broker RPC Timeout 默认是10秒，这里需要适当调整超时时间。 修改 `fe.conf` 配置文件，添加下面的参数：\n``` broker_timeout_ms = 10000 ##这里默认是10秒，需要适当加大这个参数\n``` 这里添加参数，需要重启 FE 服务。 ### Q11.[ Routine load ] ReasonOfStateChanged: ErrorReason{code=errCode = 104, msg=\'be 10004 abort task with reason: fetch failed due to requested offset not available on the broker: Broker: Offset out of range\'}\n出现这个问题的原因是因为kafka的清理策略默认为7天，当某个routine load任务因为某种原因导致任务暂停，长时间没有恢复，当重新恢复任务的时候routine load记录了消费的offset,而kafka的清理策略已经清理了对应的offset,就会出现这个问题 所以这个问题可以用alter routine load解决方式： 查看kafka最小的offset ,使用ALTER ROUTINE LOAD命令修改offset,重新恢复任务即可\n```sql ALTER ROUTINE LOAD FOR db.tb FROM kafka ( ""kafka_partitions"" = ""0"", ""kafka_offsets"" = ""xxx"", ""property.group.id"" = ""xxx"" );\n``` ### Q12. ERROR 1105 (HY000): errCode = 2, detailMessage = (192.168.90.91)[CANCELLED][INTERNAL_ERROR]error setting certificate verify locations: CAfile: /etc/ssl/certs/ca-certificates.crt CApath: none\n``` yum install -y ca-certificates ln -s /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt /etc/ssl/certs/ca-certificates.crt\n``` ### Q13. create partition failed. partition numbers will exceed limit variable max_auto_partition_num\n对自动分区表导入数据时，为防止意外创建过多分区，我们使用了FE配置项`max_auto_partition_num`管控此类表自动创建时的最大分区数。如果确需创建更多分区，请修改FE master节点的该配置项。 --- { ""title"": ""运维常见问题"", ""language"": ""zh-CN"" } --- <!--split--> # 运维常见问题 本文档主要用于记录 Doris 使用过程中的运维常见问题。会不定期更新。', '23:59:59 | | NULL | 0.000 | false | tag.location.default: 1 | true |\n| 180039 | p20120101000000 | 2 | 2023-09-18 21:49:29 | NORMAL | TRADE_DATE | [types: [DATEV2]; keys: [2012-01-01]; ..types: [DATEV2]; keys: [2013-01-01]; ) | TRADE_DATE | 10 | 1 | HDD | 9999-12-31\n23:59:59 | | NULL | 0.000 | false | tag.location.default: 1 | true |\n| 180018 | p20140101000000 | 2 | 2023-09-18 21:49:29 | NORMAL | TRADE_DATE | [types: [DATEV2]; keys: [2014-01-01]; ..types: [DATEV2]; keys: [2015-01-01]; ) | TRADE_DATE | 10 | 1 | HDD | 9999-12-31\n23:59:59 | | NULL | 0.000 | false | tag.location.default: 1 | true |\n+-------------+-----------------+----------------+---------------------+--------+--------------+--------------------------------------------------------------------------------+-----------------+---------+----------------+---------------+-----------------\n----+---------------------+--------------------------+----------+------------+-------------------------+-----------+\n3 rows in set (0.12 sec)\n``` 经过自动分区功能所创建的PARTITION，与手动创建的PARTITION具有完全一致的功能性质。 ## 注意事项\n- 在数据的插入或导入过程中如果创建了分区，而整个导入过程没有完成（失败或被取消），被创建的分区不会被自动删除。 - 使用AUTO PARTITION的表，只是分区创建方式上由手动转为了自动。表及其所创建分区的原本使用方法都与非AUTO PARTITION的表或分区相同。\n- 为防止意外创建过多分区，我们通过[FE配置项](../../admin-manual/config/fe-config)中的`max_auto_partition_num`控制了一个AUTO PARTITION表最大容纳分区数。如有需要可以调整该值']",Doris中对分区数确实有限制。为了防止意外创建过多分区，Doris通过FE（Frontend）配置项`max_auto_partition_num`控制了一个表能够自动创建的最大分区数。如果需要创建更多分区，需要修改FE master节点的`max_auto_partition_num`配置项。因此，一张表最大能添加的分区数取决于`max_auto_partition_num`的配置值。默认值或具体限制数值在给定的上下文中没有提及，建议查阅Doris的官方文档或联系Doris的支持团队以获取最准确的信息。,20.06,,ERROR,"['默认值：1048576  （1M）\n\nhttp header size 配置参数\n\n#### `http_sql_submitter_max_worker_threads`\n\n默认值：2\n\nhttp请求处理/api/query中sql任务的最大线程池\n\n#### `http_load_submitter_max_worker_threads`\n\n默认值：2\n\nhttp请求处理/api/upload任务的最大线程池\n\n### 查询引擎\n\n#### `default_max_query_instances`\n\n默认值：-1\n\n用户属性max_query_instances小于等于0时，使用该配置，用来限制单个用户同一时刻可使用的查询instance个数。该参数小于等于0表示无限制。\n\n#### `max_query_retry_time`\n\n默认值：1\n\n是否可以动态配置：true\n\n查询重试次数。 如果我们遇到 RPC 异常并且没有将结果发送给用户，则可能会重试查询。 您可以减少此数字以避免雪崩灾难。\n\n#### `max_dynamic_partition_num`\n\n默认值：500\n\n是否可以动态配置：true\n\n是否为 Master FE 节点独有的配置项：true\n\n用于限制创建动态分区表时可以创建的最大分区数，避免一次创建过多分区。 数量由动态分区参数中的“开始”和“结束”决定。\n\n#### `dynamic_partition_enable`\n\n默认值：true\n\n是否可以动态配置：true\n\n是否为 Master FE 节点独有的配置项：true\n\n是否启用动态分区调度，默认启用\n\n#### `dynamic_partition_check_interval_seconds`\n\n默认值：600秒，10分钟\n\n是否可以动态配置：true\n\n是否为 Master FE 节点独有的配置项：true\n\n检查动态分区的频率\n\n<version since=""1.2.0"">\n\n#### `max_multi_partition_num`\n\n默认值：4096\n\n是否可以动态配置：true\n\n是否为 Master FE 节点独有的配置项：true\n\n用于限制批量创建分区表时可以创建的最大分区数，避免一次创建过多分区。\n\n</version>\n\n#### `multi_partition_name_prefix`', '经过自动分区功能所创建的PARTITION，与手动创建的PARTITION具有完全一致的功能性质。\n\n## 注意事项\n\n- 在数据的插入或导入过程中如果创建了分区，而整个导入过程没有完成（失败或被取消），被创建的分区不会被自动删除。\n- 使用AUTO PARTITION的表，只是分区创建方式上由手动转为了自动。表及其所创建分区的原本使用方法都与非AUTO PARTITION的表或分区相同。\n- 为防止意外创建过多分区，我们通过[FE配置项](../../admin-manual/config/fe-config)中的`max_auto_partition_num`控制了一个AUTO PARTITION表最大容纳分区数。如有需要可以调整该值\n- 向开启了AUTO PARTITION的表导入数据时，Coordinator发送数据的轮询间隔与普通表有所不同。具体请见[BE配置项](../../admin-manual/config/be-config)中的`olap_table_sink_send_interval_auto_partition_factor`。\n- 在使用[insert-overwrite](../../sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/INSERT-OVERWRITE)插入数据时，如果指定了覆写的partition，则AUTO PARTITION表在此过程中表现得如同普通表，不创建新的分区。\n---\n{\n    ""title"": ""动态分区"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 动态分区\n\n动态分区是在 Doris 0.12 版本中引入的新功能。旨在对表级别的分区实现生命周期管理(TTL)，减少用户的使用负担。\n\n目前实现了动态添加分区及动态删除分区的功能。\n\n动态分区只支持 Range 分区。', '注：批量创建分区可以和常规手动创建分区混用，使用时需要限制分区列只能有一个，批量创建分区实际创建默认最大数量为4096，这个参数可以在fe配置项 `max_multi_partition_num` 调整\n\n</version>\n\n<version since=""2.0"">\n\n14. 批量无排序列Duplicate表\n\n    ```sql\n    CREATE TABLE example_db.table_hash\n    (\n        k1 DATE,\n        k2 DECIMAL(10, 2) DEFAULT ""10.5"",\n        k3 CHAR(10) COMMENT ""string column"",\n        k4 INT NOT NULL DEFAULT ""1"" COMMENT ""int column""\n    )\n    COMMENT ""duplicate without keys""\n    PARTITION BY RANGE(k1)\n    (\n        PARTITION p1 VALUES LESS THAN (""2020-02-01""),\n        PARTITION p2 VALUES LESS THAN (""2020-03-01""),\n        PARTITION p3 VALUES LESS THAN (""2020-04-01"")\n    )\n    DISTRIBUTED BY HASH(k1) BUCKETS 32\n    PROPERTIES (\n        ""replication_num"" = ""1"",\n        ""enable_duplicate_without_keys_by_default"" = ""true""\n    );\n    ```\n\n</version>\n\n### Keywords\n\n    CREATE, TABLE\n\n### Best Practice\n\n#### 分区和分桶\n\n一个表必须指定分桶列，但可以不指定分区。关于分区和分桶的具体介绍，可参阅 [数据划分](../../../../data-table/data-partition.md) 文档。', '### Best Practice\n\n#### 分区和分桶\n\n一个表必须指定分桶列，但可以不指定分区。关于分区和分桶的具体介绍，可参阅 [数据划分](../../../../data-table/data-partition.md) 文档。\n\nDoris 中的表可以分为分区表和无分区的表。这个属性在建表时确定，之后不可更改。即对于分区表，可以在之后的使用过程中对分区进行增删操作，而对于无分区的表，之后不能再进行增加分区等操作。\n\n同时，分区列和分桶列在表创建之后不可更改，既不能更改分区和分桶列的类型，也不能对这些列进行任何增删操作。\n\n所以建议在建表前，先确认使用方式来进行合理的建表。\n\n#### 动态分区\n\n动态分区功能主要用于帮助用户自动的管理分区。通过设定一定的规则，Doris 系统定期增加新的分区或删除历史分区。可参阅 [动态分区](../../../../advanced/partition/dynamic-partition.md) 文档查看更多帮助。\n\n#### 物化视图\n\n用户可以在建表的同时创建多个物化视图（ROLLUP）。物化视图也可以在建表之后添加。写在建表语句中可以方便用户一次性创建所有物化视图。\n\n如果在建表时创建好物化视图，则后续的所有数据导入操作都会同步生成物化视图的数据。物化视图的数量可能会影响数据导入的效率。\n\n如果在之后的使用过程中添加物化视图，如果表中已有数据，则物化视图的创建时间取决于当前数据量大小。\n\n关于物化视图的介绍，请参阅文档 [物化视图](../../../../query-acceleration/materialized-view.md)。\n\n#### 索引\n\n用户可以在建表的同时创建多个列的索引。索引也可以在建表之后再添加。\n\n如果在之后的使用过程中添加索引，如果表中已有数据，则需要重写所有数据，因此索引的创建时间取决于当前数据量。\n\n---\n{\n    ""title"": ""CREATE-CATALOG"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## CREATE-CATALOG\n\n### Name\n\nCREATE CATALOG\n\n### Description\n\n该语句用于创建外部数据目录（catalog）\n\n语法：', '- Export 作业会扫描数据，占用 IO 资源，可能会影响系统的查询延迟。\n\n- 目前在export时只是简单检查tablets版本是否一致，建议在执行export过程中不要对该表进行导入数据操作。\n\n- 一个Export Job允许导出的分区数量最大为2000，可以在fe.conf中添加参数`maximum_number_of_export_partitions`并重启FE来修改该设置。\n---\n{\n    ""title"": ""权限管理"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 权限管理\n\nDoris 新的权限管理系统参照了 Mysql 的权限管理机制，做到了行级别细粒度的权限控制，基于角色的权限访问控制，并且支持白名单机制。\n\n## 名词解释\n\n1. 用户标识 user_identity\n\n   在权限系统中，一个用户被识别为一个 User Identity（用户标识）。用户标识由两部分组成：username 和 userhost。其中 username 为用户名，由英文大小写组成。userhost 表示该用户链接来自的 IP。user_identity 以 username@\'userhost\' 的方式呈现，表示来自 userhost 的 username。\n\n   user_identity 的另一种表现方式为 username@[\'domain\']，其中 domain 为域名，可以通过 DNS 或 BNS（百度名字服务）解析为一组 ip。最终表现为一组 username@\'userhost\'，所以后面我们统一使用 username@\'userhost\' 来表示。\n\n2. 权限 Privilege\n\n   权限作用的对象是节点、数据目录、数据库或表。不同的权限代表不同的操作许可。\n\n3. 角色 Role\n\n   Doris可以创建自定义命名的角色。角色可以被看做是一组权限的集合。新创建的用户可以被赋予某一角色，则自动被赋予该角色所拥有的权限。后续对角色的权限变更，也会体现在所有属于该角色的用户权限上。\n\n4. 用户属性 user_property', '4. query_cache_max_size_mb 和 query_cache_elasticity_size\n\nquery_cache_max_size_mb 缓存的内存上限，query_cache_elasticity_size 缓存可拉伸的内存大小，BE上的缓存总大小超过 query_cache_max_size + cache_elasticity_size 后会开始清理，并把内存控制到 query_cache_max_size 以下。\n\n可以根据BE节点数量，节点内存大小，和缓存命中率来设置这两个参数。计算方法：假如缓存10000个Query，每个Query缓存1000行，每行是128个字节，分布在10台BE上，则每个BE需要约128M内存（10000 * 1000 * 128/10）。\n\n```text\nvim be/conf/be.conf\nquery_cache_max_size_mb=256\nquery_cache_elasticity_size_mb=128\n```\n\n5. cache_max_partition_count\n\nPartition Cache 独有的参数。BE最大分区数量，指每个SQL对应的最大分区数，如果是按日期分区，能缓存2年多的数据，假如想保留更长时间的缓存，请把这个参数设置得更大，同时修改参数 cache_result_max_row_count 和 cache_result_max_data_size。\n\n```text\nvim be/conf/be.conf\ncache_max_partition_count=1024\n```\n---\n{\n    ""title"": ""Partition Cache"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# Partition Cache\n\n多个 SQL 使用相同的表分区时可命中缓存。\n\n```\n**Partition Cache是个试验性功能，没有得到很好的维护，谨慎使用**\n```\n\n## 需求场景 & 解决方案\n\n见 query-cache.md。\n\n## 设计原理\n\n1. SQL可以并行拆分，Q = Q1 ∪ Q2 ... ∪ Qn，R= R1 ∪ R2 ... ∪ Rn，Q为查询语句，R为结果集', '```\n\n### 列定义\n\n这里我们只以 AGGREGATE KEY 数据模型为例进行说明。更多数据模型参阅 [Doris 数据模型](./data-model.md)。\n\n列的基本类型，可以通过在 mysql-client 中执行 `HELP CREATE TABLE;` 查看。\n\nAGGREGATE KEY 数据模型中，所有没有指定聚合方式（SUM、REPLACE、MAX、MIN）的列视为 Key 列。而其余则为 Value 列。\n\n定义列时，可参照如下建议：\n\n1. Key 列必须在所有 Value 列之前。\n2. 尽量选择整型类型。因为整型类型的计算和查找效率远高于字符串。\n3. 对于不同长度的整型类型的选择原则，遵循 **够用即可**。\n4. 对于 VARCHAR 和 STRING 类型的长度，遵循 **够用即可**。\n\n### 分区和分桶\n\nDoris 支持两层的数据划分。第一层是 Partition，支持 Range 和 List 的划分方式。第二层是 Bucket（Tablet），支持 Hash 和 Random 的划分方式。\n\n也可以仅使用一层分区，建表时如果不写分区的语句即可，此时Doris会生成一个默认的分区，对用户是透明的。使用一层分区时，只支持 Bucket 划分。下面我们来分别介绍下分区以及分桶：\n\n1. **Partition**\n\n   - Partition 列可以指定一列或多列，分区列必须为 KEY 列。多列分区的使用方式在后面 **多列分区** 小结介绍。\n   - 不论分区列是什么类型，在写分区值时，都需要加双引号。\n   - 分区数量理论上没有上限。\n   - 当不使用 Partition 建表时，系统会自动生成一个和表名同名的，全值范围的 Partition。该 Partition 对用户不可见，并且不可删改。\n   - 创建分区时**不可添加范围重叠**的分区。\n\n   **Range 分区**\n\n   - 分区列通常为时间列，以方便的管理新旧数据。\n\n   - Range 分区支持的列类型：[DATE,DATETIME,TINYINT,SMALLINT,INT,BIGINT,LARGEINT]', '2. **Bucket**\n\n   - 如果使用了 Partition，则 `DISTRIBUTED ...` 语句描述的是数据在**各个分区内**的划分规则。如果不使用 Partition，则描述的是对整个表的数据的划分规则。\n   - 分桶列可以是多列，Aggregate 和 Unique 模型必须为 Key 列，Duplicate 模型可以是 key 列和 value 列。分桶列可以和 Partition 列相同或不同。\n   - 分桶列的选择，是在 **查询吞吐** 和 **查询并发** 之间的一种权衡：\n     1. 如果选择多个分桶列，则数据分布更均匀。如果一个查询条件不包含所有分桶列的等值条件，那么该查询会触发所有分桶同时扫描，这样查询的吞吐会增加，单个查询的延迟随之降低。这个方式适合大吞吐低并发的查询场景。\n     2. 如果仅选择一个或少数分桶列，则对应的点查询可以仅触发一个分桶扫描。此时，当多个点查询并发时，这些查询有较大的概率分别触发不同的分桶扫描，各个查询之间的IO影响较小（尤其当不同桶分布在不同磁盘上时），所以这种方式适合高并发的点查询场景。\n   - AutoBucket: 根据数据量，计算分桶数。 对于分区表，可以根据历史分区的数据量、机器数、盘数，确定一个分桶。\n   - 分桶的数量理论上没有上限。\n\n3. **关于 Partition 和 Bucket 的数量和数据量的建议。**', '```sql\nmysql> USE example_db;\nDatabase changed\n```\n\nDoris支持[复合分区和单分区](./data-partition.md)两种建表方式。下面以聚合模型为例，分别演示如何创建两种分区的数据表。\n\n#### 单分区\n\n建立一个名字为 table1 的逻辑表。分桶列为 siteid，桶数为 10。\n\n这个表的 schema 如下：\n\n- siteid：类型是INT（4字节）, 默认值为10\n- citycode：类型是SMALLINT（2字节）\n- username：类型是VARCHAR, 最大长度为32, 默认值为空字符串\n- pv：类型是BIGINT（8字节）, 默认值是0; 这是一个指标列, Doris内部会对指标列做聚合操作, 这个列的聚合方法是求和（SUM）\n\n建表语句如下：\n\n```sql\nCREATE TABLE table1\n(\n    siteid INT DEFAULT \'10\',\n    citycode SMALLINT,\n    username VARCHAR(32) DEFAULT \'\',\n    pv BIGINT SUM DEFAULT \'0\'\n)\nAGGREGATE KEY(siteid, citycode, username)\nDISTRIBUTED BY HASH(siteid) BUCKETS 10\nPROPERTIES(""replication_num"" = ""1"");\n```\n\n#### 多分区\n\n建立一个名字为 table2 的逻辑表。\n\n这个表的 schema 如下：\n\n- event_day：类型是DATE，无默认值\n- siteid：类型是INT（4字节）, 默认值为10\n- citycode：类型是SMALLINT（2字节）\n- username：类型是VARCHAR, 最大长度为32, 默认值为空字符串\n- pv：类型是BIGINT（8字节）, 默认值是0; 这是一个指标列, Doris 内部会对指标列做聚合操作, 这个列的聚合方法是求和（SUM）\n\n我们使用 event_day 列作为分区列，建立3个分区: p201706, p201707, p201708', '#### DUPLICATE KEY\n\n只指定排序列，相同的行不会合并。适用于数据无需提前聚合的分析业务。\n\n```sql\nCREATE TABLE session_data\n(\n    visitorid   SMALLINT,\n    sessionid   BIGINT,\n    visittime   DATETIME,\n    city        CHAR(20),\n    province    CHAR(20),\n    ip          varchar(32),\n    brower      CHAR(20),\n    url         VARCHAR(1024)\n)\nDUPLICATE KEY(visitorid, sessionid)\nDISTRIBUTED BY HASH(sessionid, visitorid) BUCKETS 10;\n```\n\n### 大宽表与 Star Schema\n\n业务方建表时, 为了和前端业务适配, 往往不对维度信息和指标信息加以区分, 而将 Schema 定义成大宽表，这种操作对于数据库其实不是那么友好，我们更建议用户采用星型模型。\n\n- Schema 中字段数比较多, 聚合模型中可能 key 列比较多, 导入过程中需要排序的列会增加。\n- 维度信息更新会反应到整张表中，而更新的频率直接影响查询的效率。\n\n使用过程中，建议用户尽量使用 Star Schema 区分维度表和指标表。频繁更新的维度表也可以放在 MySQL 外部表中。而如果只有少量更新, 可以直接放在 Doris 中。在 Doris 中存储维度表时，可对维度表设置更多的副本，提升 Join 的性能。\n\n### 分区和分桶\n\nDoris 支持两级分区存储, 第一层为分区(partition)，目前支持 RANGE 分区和 LIST 分区两种类型, 第二层为 HASH 分桶(bucket)。\n\n#### 分区(partition)\n\n分区用于将数据划分成不同区间, 逻辑上可以理解为将原始表划分成了多个子表。可以方便的按分区对数据进行管理，例如，删除数据时，更加迅速。\n\n##### RANGE分区\n\n业务上，多数用户会选择采用按时间进行partition, 让时间进行partition有以下好处：\n\n* 可区分冷热数据\n* 可用上Doris分级存储(SSD + SATA)的功能']",在 Apache Doris 中，确实存在对分区数的限制。默认情况下，单个表最多可以添加 500 个动态分区（由 `max_dynamic_partition_num` 配置参数控制）。此外，对于批量创建分区的情况，最多可以创建 4096 个分区（由 `max_multi_partition_num` 配置参数控制）。这些限制主要是为了防止一次性创建过多的分区，从而影响系统的性能。
f87cfc6b-c17f-11ee-81a8-b026288862a5,"对历史数据如果要搞分区有什么好的解决方案吗？
","可以通过添加分区来解决，使用ALTER TABLE语句添加分区，例如：ALTER TABLE example_db.my_table，ADD PARTITION p1 VALUES LESS THAN (""2015-01-01"")。","['> 注意： > > 1. 2.0.0版本仅在Unique Key的Merge-on-Write实现中支持了部分列更新能力 > 3. 2.0.2版本支持使用`INSERT INTO`进行部分列更新 > 3. 2.1.0版本将支持更为灵活的列更新，见下文“使用限制”部分的说明 ### Aggregate Key模型 Aggregate Key表主要在预聚合场景使用而非数据更新的场景使用，但也可以通过将聚合函数设置为`REPLACE_IF_NOT_NULL`来实现列更新效果 ## 适用场景\n- 实时的动态列更新，需要在表中实时的高频更新某些字段值。例如T+1生成的用户标签表中有一些关于用户最新行为信息的字段需要实时的更新，以实现广告/推荐等系统能够据其进行实时的分析和决策 - 将多张源表拼接成一张大宽表 - 数据修正 ## 基本原理 关于Unique Key模型和Aggregate Key模型的原理，可以主要参考[数据模型](../../data-table/data-model.md)的介绍 ### Unique Key 模型\n**Unique Key模型目前仅支持在Merge-on-Write实现上进行列更新** 用户通过正常的导入方式将一部分列的数据写入Doris的Memtable，此时Memtable中并没有整行数据，在Memtable下刷的时候，会查找历史数据，用历史数据补齐一整行，并写入数据文件中，同时将历史数据文件中相同key的数据行标记删除\n当出现并发导入时，Doris会利用MVCC机制来保证数据的正确性。如果两批数据导入都更新了一个相同key的不同列，则其中系统版本较高的导入任务会在版本较低的导入任务成功后，使用版本较低的导入任务写入的相同key的数据行重新进行补齐 ### Aggregate Key模型 将聚合函数设置为`REPLACE_IF_NOT_NULL`即可实现部分列更新的支持，详细用法参考下文示例 ## 并发写入和数据可见性\n部分列更新支持高频的并发写入，写入成功后数据即可见，系统自动通过MVCC机制来保证并发写入的数据正确性 ## 性能 使用建议： 1. 对写入性能要求较高，查询性能要求较低的用户，建议使用Aggregate Key模型 2. 对查询性能要求较高，对写入性能要求不高（例如数据的写入和更新基本都在凌晨低峰期完成），或者写入频率不高的用户，建议使用Unique Key模型merge-on-write实现 ### Unique Key模型Merge-on-Write实现\n由于Merge-on-Write实现需要在数据写入的时候，进行整行数据的补齐，以保证最优的查询性能，因此使用Merge-on-Write实现进行部分列更新会有较为明显的导入性能下降。 写入性能优化建议： 1. 使用配备了NVMe的SSD，或者极速SSD云盘。因为补齐数据时会大量的读取历史数据，产生较高的读IOPS，以及读吞吐 2. 开启行存将能够大大减少补齐数据时产生的IOPS，导入性能提升明显，用户可以在建表时通过如下property来开启行存：\n``` ""store_row_column"" = ""true""\n``` ### Aggregate Key模型 Aggregate Key模型在写入过程中不做任何额外处理，所以写入性能不受影响，与普通的数据导入相同。但是在查询时进行聚合的代价较大，典型的聚合查询性能相比Unique Key模型的Merge-on-Write实现会有5-10倍的下降。 ## 使用方式及示例 ### Unique Key模型 #### 建表 建表时需要指定如下property，以开启Merge-on-Write实现\n``` enable_unique_key_merge_on_write = true ``` #### StreamLoad/BrokerLoad/RoutineLoad 如果使用的是StreamLoad/BrokerLoad/RoutineLoad，在导入时添加如下header ``` partial_columns:true\n``` 同时在`columns`中指定要导入的列（必须包含所有key列，不然无法更新） #### Flink Connector 如果使用Flink Connector, 需要添加如下配置： ``` \'sink.properties.partial_columns\' = \'true\',\n``` 同时在`sink.properties.column`中指定要导入的列（必须包含所有key列，不然无法更新） #### INSERT INTO 在所有的数据模型中，`INSERT INTO` 给定一部分列时默认行为都是整行写入，为了防止误用，在Merge-on-Write实现中，`INSERT INTO`默认仍然保持整行UPSERT的语意，如果需要开启部分列更新的语意，需要设置如下 session variable', '3. **关于 Partition 和 Bucket 的数量和数据量的建议。**\n- 一个表的 Tablet 总数量等于 (Partition num * Bucket num)。 - 一个表的 Tablet 数量，在不考虑扩容的情况下，推荐略多于整个集群的磁盘数量。\n- 单个 Tablet 的数据量理论上没有上下界，但建议在 1G - 10G 的范围内。如果单个 Tablet 数据量过小，则数据的聚合效果不佳，且元数据管理压力大。如果数据量过大，则不利于副本的迁移、补齐，且会增加 Schema Change 或者 Rollup 操作失败重试的代价（这些操作失败重试的粒度是 Tablet）。 - 当 Tablet 的数据量原则和数量原则冲突时，建议优先考虑数据量原则。\n- 在建表时，每个分区的 Bucket 数量统一指定。但是在动态增加分区时（`ADD PARTITION`），可以单独指定新分区的 Bucket 数量。可以利用这个功能方便的应对数据缩小或膨胀。 - 一个 Partition 的 Bucket 数量一旦指定，不可更改。所以在确定 Bucket 数量时，需要预先考虑集群扩容的情况。比如当前只有 3 台 host，每台 host 有 1 块盘。如果 Bucket 的数量只设置为 3 或更小，那么后期即使再增加机器，也不能提高并发度。\n- 举一些例子：假设在有10台BE，每台BE一块磁盘的情况下。如果一个表总大小为 500MB，则可以考虑4-8个分片。5GB：8-16个分片。50GB：32个分片。500GB：建议分区，每个分区大小在 50GB 左右，每个分区16-32个分片。5TB：建议分区，每个分区大小在 50GB 左右，每个分区16-32个分片。\n> 注：表的数据量可以通过 [`SHOW DATA`](../sql-manual/sql-reference/Show-Statements/SHOW-DATA.md) 命令查看，结果除以副本数，即表的数据量。\n4. **关于 Random Distribution 的设置以及使用场景。** - 如果 OLAP 表没有更新类型的字段，将表的数据分桶模式设置为 RANDOM，则可以避免严重的数据倾斜(数据在导入表对应的分区的时候，单次导入作业每个 batch 的数据将随机选择一个tablet进行写入)。\n- 当表的分桶模式被设置为RANDOM 时，因为没有分桶列，无法根据分桶列的值仅对几个分桶查询，对表进行查询的时候将对命中分区的全部分桶同时扫描，该设置适合对表数据整体的聚合查询分析而不适合高并发的点查询。\n- 如果 OLAP 表的是 Random Distribution 的数据分布，那么在数据导入的时候可以设置单分片导入模式（将 `load_to_single_tablet` 设置为 true），那么在大数据量的导入的时候，一个任务在将数据写入对应的分区时将只写入一个分片，这样将能提高数据导入的并发度和吞吐量，减少数据导入和 Compaction 导致的写放大问题，保障集群的稳定性。\n#### 复合分区与单分区 复合分区 - 第一级称为 Partition，即分区。用户可以指定某一维度列作为分区列（当前只支持整型和时间类型的列），并指定每个分区的取值范围。 - 第二级称为 Distribution，即分桶。用户可以指定一个或多个维度列以及桶数对数据进行 HASH 分布 或者不指定分桶列设置成 Random Distribution 对数据进行随机分布。 以下场景推荐使用复合分区\n- 有时间维度或类似带有有序值的维度，可以以这类维度列作为分区列。分区粒度可以根据导入频次、分区数据量等进行评估。 - 历史数据删除需求：如有删除历史数据的需求（比如仅保留最近N 天的数据）。使用复合分区，可以通过删除历史分区来达到目的。也可以通过在指定分区内发送 DELETE 语句进行数据删除。 - 解决数据倾斜问题：每个分区可以单独指定分桶数量。如按天分区，当每天的数据量差异很大时，可以通过指定分区的分桶数，合理划分不同分区的数据,分桶列建议选择区分度大的列。', '### Load Type `/mem_tracker?type=load` ![image](https://user-images.githubusercontent.com/13197424/202925855-936889e3-c910-4ca5-bc12-1b9849a09c33.png)\n1. Limit: 导入分为 Fragment Scan 和 Load Channel 写 Segment 到磁盘两个阶段。Scan 阶段的内存上限通过`show session variables`查看和修改`load_mem_limit`；Segment 写磁盘阶段每个导入没有单独的内存上限，而是所有导入的总上限，对应 be.conf 中的 `load_process_max_memory_limit_percent`。\n2. Label: 单个导入 Scan 阶段 Tracker 的 Label 命名规则为`Load#Id=xxx`；单个导入 Segment 写磁盘阶段 Tracker 的 Label 命名规则为`LoadChannel#senderIp=xxx#loadID=xxx`。\n3. Parent Label: Parent是 `Load#Id=xxx` 的 Tracker 记录导入 Scan 阶段不同算子执行过程使用的内存；Parent是 `LoadChannelMgrTrackerSet` 的 Tracker 记录 Segment 写磁盘阶段每个中间数据结构 MemTable 的 Insert 和 Flush 磁盘过程使用的内存，用 Label 最后的 `loadID` 关联 Segment 写磁盘阶段 Tracker。\n</version> --- { ""title"": ""Schema 变更"", ""language"": ""zh-CN"" } --- <!--split--> # Schema Change 用户可以通过 Schema Change 操作来修改已存在表的 Schema。目前 Doris 支持以下几种修改: - 增加、删除列 - 修改列类型 - 调整列顺序 - 增加、修改 Bloom Filter - 增加、删除 bitmap index\n本文档主要介绍如何创建 Schema Change 作业，以及进行 Schema Change 的一些注意事项和常见问题。 ## 名词解释 - Base Table：基表。每一个表被创建时，都对应一个基表。 - Rollup：基于基表或者其他 Rollup 创建出来的上卷表。 - Index：物化索引。Rollup 或 Base Table 都被称为物化索引。 - Transaction：事务。每一个导入任务都是一个事务，每个事务有一个唯一递增的 Transaction ID。 ## 原理介绍\n执行 Schema Change 的基本过程，是通过原 Index 的数据，生成一份新 Schema 的 Index 的数据。其中主要需要进行两部分数据转换，一是已存在的历史数据的转换，二是在 Schema Change 执行过程中，新到达的导入数据的转换。\n```text +----------+ | Load Job | +----+-----+ | | Load job generates both origin and new index data | | +------------------+ +---------------+ | | Origin Index | | Origin Index |\n+------> New Incoming Data| | History Data | | +------------------+ +------+--------+ | | | | Convert history data | |\n| +------------------+ +------v--------+ | | New Index | | New Index | +------> New Incoming Data| | History Data | +------------------+ +---------------+\n``` 在开始转换历史数据之前，Doris 会获取一个最新的 Transaction ID。并等待这个 Transaction ID 之前的所有导入事务完成。这个 Transaction ID 成为分水岭。意思是，Doris 保证在分水岭之后的所有导入任务，都会同时为原 Index 和新 Index 生成数据。这样当历史数据转换完成后，可以保证新的 Index 中的数据是完整的。 ## 创建作业']","对于已有的历史数据进行分区，通常有以下几个解决方案：

1. **重新创建表并指定分区**：这是最直接的方法，即创建一个新表，定义好分区策略，然后将旧表的数据导入到新表中。这种方法的优点是可以根据需要自由地设计分区策略，但缺点是数据迁移可能会消耗较多时间和资源。

2. **使用Doris的Schema Change功能**：Doris支持Schema Change，允许用户在不停机的情况下修改表的Schema，包括增加、删除列，修改列类型，调整列顺序等。虽然直接通过Schema Change对已有数据进行分区的支持可能有限，但可以通过Schema Change实现一些列的调整，为后续的表重建或数据迁移做准备。

3. **动态增加分区**：对于Doris等数据库系统，如果表是以分区的形式创建的，可以动态地向表中添加新的分区。对于历史数据，可以通过`ADD PARTITION`语句增加新分区，并将历史数据导入到对应的分区中。这种方法适用于已经按某种逻辑（如时间）分区，但需要细化分区策略的场景。

4. **使用ETL工具或脚本**：可以使用ETL工具（如Apache NiFi, Apache Airflow等）或编写自定义脚本来处理历史数据。这些工具或脚本可以帮助你按照新的分区策略重新组织数据，并将其导入到新的或现有的分区表中。这种方法的灵活性很高，但需要一定的开发工作。

在选择解决方案时，需要考虑数据量大小、系统性能、维护难度以及业务需求等因素。对于大规模数据的迁移或重组，建议在低峰时间进行，并提前进行充分的测试以确保数据的完整性和准确性。

注意：在文档上下文中，直接针对已有数据进行分区的具体操作方法没有提及，因此建议结合实际使用的数据库系统的官方文档和支持情况，选择最适合您需求的方法。",44.0,,ERROR,"['注意：\n\n- partition_desc 支持以下两种写法\n  - VALUES LESS THAN [MAXVALUE|(""value1"", ...)]\n  - VALUES [(""value1"", ...), (""value1"", ...))\n- 分区为左闭右开区间，如果用户仅指定右边界，系统会自动确定左边界\n- 如果没有指定分桶方式，则自动使用建表使用的分桶方式和分桶数。\n- 如指定分桶方式，只能修改分桶数，不可修改分桶方式或分桶列。如果指定了分桶方式，但是没有指定分桶数，则分桶数会使用默认值10，不会使用建表时指定的分桶数。如果要指定分桶数，则必须指定分桶方式。\n- [""key""=""value""] 部分可以设置分区的一些属性，具体说明见 [CREATE TABLE](../Create/CREATE-TABLE.md)\n- 如果建表时用户未显式创建Partition,则不支持通过ALTER的方式增加分区\n- 如果用户使用的是List Partition则可以增加default partition，default partition将会存储所有不满足其他分区键要求的数据。\n  -  ALTER TABLE table_name ADD PARTITION partition_name\n\n2. 删除分区\n\n语法：\n\n```sql\nDROP PARTITION [IF EXISTS] partition_name [FORCE]\n```\n\n 注意：\n\n- 使用分区方式的表至少要保留一个分区。\n- 执行 DROP PARTITION 一段时间内，可以通过 RECOVER 语句恢复被删除的分区。详见 SQL手册-数据库管理-RECOVER 语句\n- 如果执行 DROP PARTITION FORCE，则系统不会检查该分区是否存在未完成的事务，分区将直接被删除并且不能被恢复，一般不建议执行此操作\n\n3. 修改分区属性\n\n 语法：\n\n```sql\nMODIFY PARTITION p1|(p1[, p2, ...]) SET (""key"" = ""value"", ...)\n```\n\n说明：', '### 4. 冷热数据分离\n\n- 用户可以通过 SQL 设置冷热数据策略，从而将历史数据转移到对象存储等廉价存储中，以降低存储成本。\n- 冷数据仍然可以被直接访问，Doris 提供了本地缓存以提高冷数据的访问效率。\n\n\t参考文档：[https://doris.apache.org/zh-CN/docs/dev/advanced/cold_hot_separation](https://doris.apache.org/zh-CN/docs/dev/advanced/cold_hot_separation)\n\n\n### 5. Pipeline 执行引擎（默认未开启）\n\n- 阻塞算子异步化：各个查询执行过程之中的阻塞算子被拆分成不同 Pipeline，各个 Pipeline 能否获取执行线程调度执行取决于前置数据是否就绪。阻塞算子将不再占用线程资源，不再产生线程切换的开销。\n- 自适应负载：采用多级反馈队列来调度查询优先级。在混合负载场景中，每个查询都可以公平地分配到一个固定的线程调度时间片，从而确保 Doris 可以在不同的负载下具有更稳定的性能表现。\n- 可控的线程数目：Pipeline 执行引擎默认的执行线程数目为 CPU 和核数，Doris 启动了对应的执行线程池进行执行线程的管理。用户的 SQL 执行和线程进行了解绑，对于线程的资源使用更加可控。\n\n\t参考文档：[https://doris.apache.org/zh-CN/docs/dev/query-acceleration/pipeline-execution-engine](https://doris.apache.org/zh-CN/docs/dev/query-acceleration/pipeline-execution-engine)\n\n### 6. 基于代价模型的全新查询优化器 Nereids （默认未开启）', ""该表内存储了大量业务历史数据，依据交易发生的日期进行分区。可以看到在建表时，我们需要预先手动创建分区。如果分区列的数据范围发生变化，例如上表中增加了2022年的数据，则我们需要通过[ALTER-TABLE-PARTITION](../../sql-manual/sql-reference/Data-Definition-Statements/Alter/ALTER-TABLE-PARTITION)对表的分区进行更改。如果这种分区需要变更，或者进行更细粒度的细分，修改起来非常繁琐。此时我们就可以使用AUTO PARTITION改写该表DDL。\n\n## 语法\n\n建表时，使用以下语法填充[CREATE-TABLE](../../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE)时的`partition_info`部分：\n\n1. AUTO RANGE PARTITION:\n\n  ```sql\n  AUTO PARTITION BY RANGE FUNC_CALL_EXPR\n  (\n  )\n  ```\n  其中\n  ```sql\n  FUNC_CALL_EXPR ::= date_trunc ( <partition_column>, '<interval>' )\n  ```\n\n2. AUTO LIST PARTITION:\n\n  ```sql\n  AUTO PARTITION BY LIST(`partition_col`)\n  (\n  )\n  ```\n\n### 用法示例\n\n1. AUTO RANGE PARTITION"", '- SELECT后被选择的列，可以在ORDER IN和GROUP BY中，通过列名、列别名或者代表列位置的整数（从1开始）来引用\n\n     ```sql\n     SELECT college, region, seed FROM tournament\n       ORDER BY region, seed;\n     \n     SELECT college, region AS r, seed AS s FROM tournament\n       ORDER BY r, s;\n     \n     SELECT college, region, seed FROM tournament\n       ORDER BY 2, 3;\n     ```\n\n   - 如果ORDER BY出现在子查询中，并且也应用于外部查询，则最外层的ORDER BY优先。\n\n   - 如果使用了GROUP BY，被分组的列会自动按升序排列（就好像有一个ORDER BY语句后面跟了同样的列）。如果要避免GROUP BY因为自动排序生成的开销，添加ORDER BY NULL可以解决：\n\n     ```sql\n     SELECT a, COUNT(b) FROM test_table GROUP BY a ORDER BY NULL;\n     ```\n\n     \n\n   - 当使用ORDER BY或GROUP BY对SELECT中的列进行排序时，服务器仅使用max_sort_length系统变量指示的初始字节数对值进行排序。\n\n   - Having子句一般应用在最后，恰好在结果集被返回给MySQL客户端前，且没有进行优化。（而LIMIT应用在HAVING后）\n\n     SQL标准要求：HAVING必须引用在GROUP BY列表中或者聚合函数使用的列。然而，MySQL对此进行了扩展，它允许HAVING引用Select子句列表中的列，还有外部子查询的列。\n\n     如果HAVING引用的列具有歧义，会有警告产生。下面的语句中，col2具有歧义：\n\n     ```sql\n     SELECT COUNT(col1) AS col2 FROM t GROUP BY col2 HAVING col2 = 2;\n     ```', '> 注：表的数据量可以通过 [`SHOW DATA`](../sql-manual/sql-reference/Show-Statements/SHOW-DATA.md) 命令查看，结果除以副本数，即表的数据量。\n\n4. **关于 Random Distribution 的设置以及使用场景。**   \n    - 如果 OLAP 表没有更新类型的字段，将表的数据分桶模式设置为 RANDOM，则可以避免严重的数据倾斜(数据在导入表对应的分区的时候，单次导入作业每个 batch 的数据将随机选择一个tablet进行写入)。\n    - 当表的分桶模式被设置为RANDOM 时，因为没有分桶列，无法根据分桶列的值仅对几个分桶查询，对表进行查询的时候将对命中分区的全部分桶同时扫描，该设置适合对表数据整体的聚合查询分析而不适合高并发的点查询。\n    - 如果 OLAP 表的是 Random Distribution 的数据分布，那么在数据导入的时候可以设置单分片导入模式（将 `load_to_single_tablet` 设置为 true），那么在大数据量的导入的时候，一个任务在将数据写入对应的分区时将只写入一个分片，这样将能提高数据导入的并发度和吞吐量，减少数据导入和 Compaction\n    导致的写放大问题，保障集群的稳定性。 \n\n#### 复合分区与单分区\n\n复合分区\n\n- 第一级称为 Partition，即分区。用户可以指定某一维度列作为分区列（当前只支持整型和时间类型的列），并指定每个分区的取值范围。\n- 第二级称为 Distribution，即分桶。用户可以指定一个或多个维度列以及桶数对数据进行 HASH 分布 或者不指定分桶列设置成 Random Distribution 对数据进行随机分布。\n\n以下场景推荐使用复合分区\n\n- 有时间维度或类似带有有序值的维度，可以以这类维度列作为分区列。分区粒度可以根据导入频次、分区数据量等进行评估。\n- 历史数据删除需求：如有删除历史数据的需求（比如仅保留最近N 天的数据）。使用复合分区，可以通过删除历史分区来达到目的。也可以通过在指定分区内发送 DELETE 语句进行数据删除。\n- 解决数据倾斜问题：每个分区可以单独指定分桶数量。如按天分区，当每天的数据量差异很大时，可以通过指定分区的分桶数，合理划分不同分区的数据,分桶列建议选择区分度大的列。']","对历史数据如果要搞分区，可以使用 Apache Doris 的 AUTO PARTITION 功能。AUTO PARTITION 可以帮助你自动创建和管理分区，以便于对历史数据进行有效管理。

在使用 AUTO PARTITION 时，你可以选择 RANGE PARTITION 或 LIST PARTITION 两种方式。RANGE PARTITION 是根据某个范围的值进行分区，而 LIST PARTITION 则是根据具体的值列表进行分区。

例如，如果你的数据是根据日期进行分区的，你可以使用 RANGE PARTITION 并根据日期范围自动创建分区。如果你知道具体的日期值列表，那么 LIST PARTITION 可能是更好的选择。

使用 AUTO PARTITION 可以让你更方便地管理历史数据，特别是在数据范围发生变化或需要进行更细粒度分区的情况下。"
8f7610e3-c195-11ee-a34a-b026288862a5,Doris 2.x版本的FE支持切换catalog吗？,切换catalog switch就可以了,"['下载 [orai18n.jar](https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html) 并放到 Doris FE 的 lib 目录以及 BE 的 lib/java_extensions 目录 (Doris 2.0 之前的版本需放到 BE 的 lib 目录下) 下即可。\n从 2.0.2 版本起，可以将这个文件放置在 FE 和 BE 的 `custom_lib/` 目录下（如不存在，手动创建即可），以防止升级集群时因为 lib 目录被替换而导致文件丢失。\n10. 通过jdbc catalog 读取Clickhouse数据出现`NoClassDefFoundError: net/jpountz/lz4/LZ4Factory` 错误信息\n可以先下载[lz4-1.3.0.jar](https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar)包，然后放到DorisFE lib 目录以及BE 的 `lib/lib/java_extensions`目录中（Doris 2.0 之前的版本需放到 BE 的 lib 目录下）。\n从 2.0.2 版本起，可以将这个文件放置在 FE 和 BE 的 `custom_lib/` 目录下（如不存在，手动创建即可），以防止升级集群时因为 lib 目录被替换而导致文件丢失。 --- { ""title"": ""概述"", ""language"": ""zh-CN"" } --- <!--split--> # 概述 多源数据目录（Multi-Catalog）功能，旨在能够更方便对接外部数据目录，以增强Doris的数据湖分析和联邦数据查询能力。\n在之前的 Doris 版本中，用户数据只有两个层级：Database 和 Table。当我们需要连接一个外部数据目录时，我们只能在Database 或 Table 层级进行对接。比如通过 `create external table` 的方式创建一个外部数据目录中的表的映射，或通过 `create external database` 的方式映射一个外部数据目录中的 Database。 如果外部数据目录中的 Database 或 Table 非常多，则需要用户手动进行一一映射，使用体验不佳。\n而新的 Multi-Catalog 功能在原有的元数据层级上，新增一层Catalog，构成 Catalog -> Database -> Table 的三层元数据层级。其中，Catalog 可以直接对应到外部数据目录。目前支持的外部数据目录包括：\n1. Apache Hive 2. Apache Iceberg 3. Apache Hudi 4. Elasticsearch 5. JDBC: 对接数据库访问的标准接口(JDBC)来访问各式数据库的数据。 6. Apache Paimon(Incubating) 该功能将作为之前外表连接方式（External Table）的补充和增强，帮助用户进行快速的多数据目录联邦查询。 ## 基础概念 1. Internal Catalog\nDoris 原有的 Database 和 Table 都将归属于 Internal Catalog。Internal Catalog 是内置的默认 Catalog，用户不可修改或删除。 2. External Catalog\n可以通过 [CREATE CATALOG](../../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-CATALOG.md) 命令创建一个 External Catalog。创建后，可以通过 [SHOW CATALOGS](../../sql-manual/sql-reference/Show-Statements/SHOW-CATALOGS.md) 命令查看已创建的 Catalog。\n3. 切换 Catalog 用户登录 Doris 后，默认进入 Internal Catalog，因此默认的使用和之前版本并无差别，可以直接使用 `SHOW DATABASES`，`USE DB` 等命令查看和切换数据库。 用户可以通过 [SWITCH](../../sql-manual/sql-reference/Utility-Statements/SWITCH.md) 命令切换 Catalog。如：', '10. 通过jdbc catalog 读取Clickhouse数据出现`NoClassDefFoundError: net/jpountz/lz4/LZ4Factory` 错误信息\n可以先下载[lz4-1.3.0.jar](https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar)包，然后放到DorisFE lib 目录以及BE 的 `lib/lib/java_extensions`目录中（Doris 2.0 之前的版本需放到 BE 的 lib 目录下）。\n从 2.0.2 版本起，可以将这个文件放置在 FE 和 BE 的 `custom_lib/` 目录下（如不存在，手动创建即可），以防止升级集群时因为 lib 目录被替换而导致文件丢失。 --- { ""title"": ""概述"", ""language"": ""zh-CN"" } --- <!--split--> # 概述 多源数据目录（Multi-Catalog）功能，旨在能够更方便对接外部数据目录，以增强Doris的数据湖分析和联邦数据查询能力。\n在之前的 Doris 版本中，用户数据只有两个层级：Database 和 Table。当我们需要连接一个外部数据目录时，我们只能在Database 或 Table 层级进行对接。比如通过 `create external table` 的方式创建一个外部数据目录中的表的映射，或通过 `create external database` 的方式映射一个外部数据目录中的 Database。 如果外部数据目录中的 Database 或 Table 非常多，则需要用户手动进行一一映射，使用体验不佳。\n而新的 Multi-Catalog 功能在原有的元数据层级上，新增一层Catalog，构成 Catalog -> Database -> Table 的三层元数据层级。其中，Catalog 可以直接对应到外部数据目录。目前支持的外部数据目录包括：\n1. Apache Hive 2. Apache Iceberg 3. Apache Hudi 4. Elasticsearch 5. JDBC: 对接数据库访问的标准接口(JDBC)来访问各式数据库的数据。 6. Apache Paimon(Incubating) 该功能将作为之前外表连接方式（External Table）的补充和增强，帮助用户进行快速的多数据目录联邦查询。 ## 基础概念 1. Internal Catalog\nDoris 原有的 Database 和 Table 都将归属于 Internal Catalog。Internal Catalog 是内置的默认 Catalog，用户不可修改或删除。 2. External Catalog\n可以通过 [CREATE CATALOG](../../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-CATALOG.md) 命令创建一个 External Catalog。创建后，可以通过 [SHOW CATALOGS](../../sql-manual/sql-reference/Show-Statements/SHOW-CATALOGS.md) 命令查看已创建的 Catalog。\n3. 切换 Catalog 用户登录 Doris 后，默认进入 Internal Catalog，因此默认的使用和之前版本并无差别，可以直接使用 `SHOW DATABASES`，`USE DB` 等命令查看和切换数据库。 用户可以通过 [SWITCH](../../sql-manual/sql-reference/Utility-Statements/SWITCH.md) 命令切换 Catalog。如：\n``` SWITCH internal; SWITCH hive_catalog;\n``` 切换后，可以直接通过 `SHOW DATABASES`，`USE DB` 等命令查看和切换对应 Catalog 中的 Database。Doris 会自动通过 Catalog 中的 Database 和 Table。用户可以像使用 Internal Catalog 一样，对 External Catalog 中的数据进行查看和访问。 当前，Doris 只支持对 External Catalog 中的数据进行只读访问。 4. 删除 Catalog', '作为 1.1 LTS（Long-term Support，长周期支持）版本基础之上的 Bugfix 版本，在 Apache Doris 1.1.4 版本中，Doris 团队修复了自 1.1.3 版本以来的约 60 个 Issue 或性能优化项。改进了 Spark Load 的使用体验，优化了诸多内存以及 BE 异常宕机的问题，系统稳定性和性能得以进一步加强，推荐所有用户下载和使用。 # 新增功能\n- Broker Load 支持 华为云 OBS 对象存储。[#13523](https://github.com/apache/doris/pull/13523) - Spark Load 支持 Parquet 和 Orc 文件。[#13438](https://github.com/apache/doris/pull/13438) # 优化改进\n- 禁用 Metric Hook 中的互斥量，其将影响数据导入过程中的查询性能。 [#10941](https://github.com/apache/doris/pull/10941) # Bug 修复 - 修复了当 Spark Load 加载文件时 Where 条件不生效的问题。 [#13804](https://github.com/apache/doris/pull/13804)\n- 修复了 If 函数存在 Nullable 列时开启向量化返回错误结果的问题。 [#13779](https://github.com/apache/doris/pull/13779) - 修复了在使用 Anti Join 和其他 Join 谓词时产生错误结果的问题。 [#13743](https://github.com/apache/doris/pull/13743)\n- 修复了当调用函数 concat(ifnull)时 BE 宕机的问题。 [#13693](https://github.com/apache/doris/pull/13693) - 修复了 group by 语句中存在函数时 planner 错误的问题。 [#13613](https://github.com/apache/doris/pull/13613)\n- 修复了 lateral view 语句不能正确识别表名和列名的问题。 [#13600](https://github.com/apache/doris/pull/13600) - 修复了使用物化视图和表别名时出现未知列的问题。 [#13605](https://github.com/apache/doris/pull/13605)\n- 修复了 JSONReader 无法释放值和解析 allocator 内存的问题。 [#13513](https://github.com/apache/doris/pull/13513) - 修复了当 enable_vectorized_alter_table 为 true 时允许使用 to_bitmap() 对负值列创建物化视图的问题。 [#13448](https://github.com/apache/doris/pull/13448)\n- 修复了函数 from_date_format_str 中微秒数丢失的问题。 [#13446](https://github.com/apache/doris/pull/13446) - 修复了排序 exprs 的 nullability 属性在使用子 smap 信息进行替换后可能不正确的问题。 [#13328](https://github.com/apache/doris/pull/13328)\n- 修复了 case when 有 1000 个条件时出现 Core 的问题。 [#13315](https://github.com/apache/doris/pull/13315) - 修复了 Stream Load 导入数据时最后一行数据丢失的问题。 [#13066](https://github.com/apache/doris/pull/13066)\n- 恢复表或分区的副本数与备份前相同。 [#11942](https://github.com/apache/doris/pull/11942) --- { ""title"": ""Release 1.2.3"", ""language"": ""zh-CN"" } --- <!--split-->\n在 1.2.3 版本中，Doris 团队已经修复了自 1.2.2 版本发布以来超过 200 个问题或性能改进项。同时，1.2.3 版本也作为 1.2.2 的迭代版本，具备更高的稳定性，建议所有用户升级到这个版本。 # Improvement ### JDBC Catalog - 支持通过 JDBC Catalog 连接到另一个 Doris 数据库。']",是的，Doris 2.x版本的FE支持切换catalog。用户登录Doris后，默认进入Internal Catalog，但可以通过`SWITCH`命令切换Catalog。例如，使用`SWITCH hive_catalog;`命令可以切换到hive_catalog。这样，用户可以直接通过`SHOW DATABASES`，`USE DB`等命令查看和切换对应Catalog中的Database，进而对External Catalog中的数据进行查看和访问。这是Doris 2.x版本中引入的多源数据目录（Multi-Catalog）功能的一部分，旨在提供更方便的外部数据目录对接，增强数据湖分析和联邦数据查询能力。,17.72,,ERROR,"['`ADMIN REPAIR TABLE tbl [PARTITION (p1, p2, ...)];`\n\n这个命令，告诉 TC，在扫描 Tablet 时，对需要优先修复的表或分区中的有问题的 Tablet，给予 VERY\\_HIGH 的优先级。\n    \n> 注：这个命令只是一个 hint，并不能保证一定能修复成功，并且优先级也会随 TS 的调度而发生变化。并且当 Master FE 切换或重启后，这些信息都会丢失。\n\n可以通过以下命令取消优先级：\n\n`ADMIN CANCEL REPAIR TABLE tbl [PARTITION (p1, p2, ...)];`\n\n### 优先级调度\n\n优先级保证了损坏严重的分片能够优先被修复，提高系统可用性。但是如果高优先级的修复任务一直失败，则会导致低优先级的任务一直得不到调度。因此，我们会根据任务的运行状态，动态的调整任务的优先级，保证所有任务都有机会被调度到。\n\n* 连续5次调度失败（如无法获取资源，无法找到合适的源端或目的端等），则优先级会被下调。\n* 持续 30 分钟未被调度，则上调优先级。 \n* 同一 tablet 任务的优先级至少间隔 5 分钟才会被调整一次。\n\n同时为了保证初始优先级的权重，我们规定，初始优先级为 VERY\\_HIGH 的，最低被下调到 NORMAL。而初始优先级为 LOW 的，最多被上调为 HIGH。这里的优先级调整，也会调整用户手动设置的优先级。\n\n## 副本均衡\n\nDoris 会自动进行集群内的副本均衡。目前支持两种均衡策略，负载/分区。负载均衡适合需要兼顾节点磁盘使用率和节点副本数量的场景；而分区均衡会使每个分区的副本都均匀分布在各个节点，避免热点，适合对分区读写要求比较高的场景。但是，分区均衡不考虑磁盘使用率，使用分区均衡时需要注意磁盘的使用情况。 策略只能在fe启动前配置[tablet_rebalancer_type](../config/fe-config.md)  ，不支持运行时切换。\n\n### 负载均衡', '### 行数限制\n\n如果在查询中带有 limit 关键字，Doris 会将其转译成适合不同数据源的语义。\n\n## 数据写入\n\n在 Doris 中建立 JDBC Catalog 后，可以通过 insert into 语句直接写入数据，也可以将 Doris 执行完查询之后的结果写入 JDBC Catalog，或者是从一个 JDBC Catalog 将数据导入另一个 JDBC Catalog。\n\n### 示例\n\n```sql\ninsert into mysql_catalog.mysql_database.mysql_table values(1, ""doris"");\ninsert into mysql_catalog.mysql_database.mysql_table select * from table;\n```\n\n### 事务\n\nDoris 的数据是由一组 batch 的方式写入 JDBC Catalog 的，如果中途导入中断，之前写入数据可能需要回滚。所以 JDBC Catalog 支持数据写入时的事务，事务的支持需要通过设置 session variable: `enable_odbc_transcation `。\n\n```sql\nset enable_odbc_transcation = true; \n```\n\n事务保证了JDBC外表数据写入的原子性，但是一定程度上会降低数据写入的性能，可以考虑酌情开启该功能。\n\n## 使用指南\n\n### 查看 JDBC Catalog\n\n可以通过 SHOW CATALOGS 查询当前所在 Doris 集群里所有 Catalog：\n\n```sql\nSHOW CATALOGS;\n```\n\n通过 SHOW CREATE CATALOG 查询某个 Catalog 的创建语句：\n\n```sql\nSHOW CREATE CATALOG <catalog_name>;\n```\n\n### 删除 JDBC Catalog\n\n可以通过 DROP CATALOG 删除某个 Catalog：\n\n```sql\nDROP CATALOG <catalog_name>;\n```\n\n### 查询 JDBC Catalog\n\n1. 通过 SWITCH 切换当前会话生效的 Catalog：\n\n    ```sql\n    SWITCH <catalog_name>;\n    ```', 'Doris 原有的 Database 和 Table 都将归属于 Internal Catalog。Internal Catalog 是内置的默认 Catalog，用户不可修改或删除。\n\n2. External Catalog\n\n    可以通过 [CREATE CATALOG](../../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-CATALOG.md) 命令创建一个 External Catalog。创建后，可以通过 [SHOW CATALOGS](../../sql-manual/sql-reference/Show-Statements/SHOW-CATALOGS.md) 命令查看已创建的 Catalog。\n\n3. 切换 Catalog\n\n    用户登录 Doris 后，默认进入 Internal Catalog，因此默认的使用和之前版本并无差别，可以直接使用 `SHOW DATABASES`，`USE DB` 等命令查看和切换数据库。\n    \n    用户可以通过 [SWITCH](../../sql-manual/sql-reference/Utility-Statements/SWITCH.md) 命令切换 Catalog。如：\n    \n    ```\n    SWITCH internal;\n    SWITCH hive_catalog;\n    ```\n    \n    切换后，可以直接通过 `SHOW DATABASES`，`USE DB` 等命令查看和切换对应 Catalog 中的 Database。Doris 会自动通过 Catalog 中的 Database 和 Table。用户可以像使用 Internal Catalog 一样，对 External Catalog 中的数据进行查看和访问。\n    \n    当前，Doris 只支持对 External Catalog 中的数据进行只读访问。\n    \n4. 删除 Catalog', '```\n\t\n\t> [SHOW CATALOGS 语法帮助](../../sql-manual/sql-reference/Show-Statements/SHOW-CATALOGS.md)\n\t\n\t> 可以通过 [SHOW CREATE CATALOG](../../sql-manual/sql-reference/Show-Statements/SHOW-CREATE-CATALOG.md) 查看创建 Catalog 的语句。\n\t\n\t> 可以通过 [ALTER CATALOG](../../sql-manual/sql-reference/Data-Definition-Statements/Alter/ALTER-CATALOG.md) 修改 Catalog 的属性。\n\t\n3. 切换 Catalog']","在 Doris 2.x 版本中，Front-end (FE) 支持切换 Catalog。用户可以通过 `SWITCH CATALOG` 命令在不同的 Catalog 之间进行切换。一旦切换到 External Catalog，用户就可以查看和访问该 Catalog 中的 Database 和 Table，并对其进行只读访问。

```sql
SWITCH CATALOG <catalog_name>;
```

请注意，Doris 2.x 版本对 External Catalog 的支持主要集中在只读访问上。如果您需要进行数据写入或其他更高级的操作，请确保您是在 Internal Catalog 中进行。"
1548043d-c17a-11ee-a3b7-b026288862a5,我使用insert into values语句插入了8819条数据，但在Apache Doris库里只查询到了7400多条数据，日志只有内存发生gc的告警。有遇到过这种情况吗？,你可以尝试打开严格模式，查看是否设置为true,"['``` --- { ""title"": ""INSERT"", ""language"": ""zh-CN"" } --- <!--split--> ## INSERT ### Name INSERT ### Description 该语句是完成数据插入操作。\n```sql INSERT INTO table_name [ PARTITION (p1, ...) ] [ WITH LABEL label] [ (column [, ...]) ] [ [ hint [, ...] ] ] { VALUES ( { expression | DEFAULT } [, ...] ) [, ...] | query }\n``` Parameters\n> tablet_name: 导入数据的目的表。可以是 `db_name.table_name` 形式 > > partitions: 指定待导入的分区，必须是 `table_name` 中存在的分区，多个分区名称用逗号分隔 > > label: 为 Insert 任务指定一个 label > > column_name: 指定的目的列，必须是 `table_name` 中存在的列 > > expression: 需要赋值给某个列的对应表达式 > > DEFAULT: 让对应列使用默认值 >\n> query: 一个普通查询，查询的结果会写入到目标中 > > hint: 用于指示 `INSERT` 执行行为的一些指示符。目前 hint 有三个可选值`/*+ STREAMING */`、`/*+ SHUFFLE */`或`/*+ NOSHUFFLE */` > 1. STREAMING：目前无实际作用，只是为了兼容之前的版本，因此保留。（之前的版本加上这个 hint 会返回 label，现在默认都会返回 label）\n> 2. SHUFFLE：当目标表是分区表，开启这个 hint 会进行 repartiiton。 > 3. NOSHUFFLE：即使目标表是分区表，也不会进行 repartiiton，但会做一些其他操作以保证数据正确落到各个分区中。\n对于开启了merge-on-write的Unique表，还可以使用insert语句进行部分列更新的操作。要使用insert语句进行部分列更新，需要将会话变量enable_unique_key_partial_update的值设置为true(该变量默认值为false，即默认无法通过insert语句进行部分列更新)。进行部分列更新时，插入的列必须至少包含所有的Key列，同时指定需要更新的列。如果插入行Key列的值在原表中存在，则将更新具有相同key列值那一行的数据。如果插入行Key列的值在原表中不存在，则将向\n表中插入一条新的数据，此时insert语句中没有指定的列必须有默认值或可以为null，这些缺失列会首先尝试用默认值填充，如果该列没有默认值，则尝试使用null值填充，如果该列不能为null，则本次插入失败。\n需要注意的是，控制insert语句是否开启严格模式的会话变量`enable_insert_strict`的默认值为true，即insert语句默认开启严格模式，而在严格模式下进行部分列更新不允许更新不存在的key。所以，在使用insert语句进行部分列更新的时候如果希望能插入不存在的key，需要在`enable_unique_key_partial_update`设置为true的基础上同时将`enable_insert_strict`设置为false。 注意：\n当前执行 `INSERT` 语句时，对于有不符合目标表格式的数据，默认的行为是过滤，比如字符串超长等。但是对于有要求数据不能够被过滤的业务场景，可以通过设置会话变量 `enable_insert_strict` 为 `true` 来确保当有数据被过滤掉的时候，`INSERT` 不会被执行成功。 ### Example `test` 表包含两个列`c1`, `c2`。 1. 向`test`表中导入一行数据\n```sql INSERT INTO test VALUES (1, 2); INSERT INTO test (c1, c2) VALUES (1, 2); INSERT INTO test (c1, c2) VALUES (1, DEFAULT); INSERT INTO test (c1) VALUES (1);', '``` Parameters\n> tablet_name: 导入数据的目的表。可以是 `db_name.table_name` 形式 > > partitions: 指定待导入的分区，必须是 `table_name` 中存在的分区，多个分区名称用逗号分隔 > > label: 为 Insert 任务指定一个 label > > column_name: 指定的目的列，必须是 `table_name` 中存在的列 > > expression: 需要赋值给某个列的对应表达式 > > DEFAULT: 让对应列使用默认值 >\n> query: 一个普通查询，查询的结果会写入到目标中 > > hint: 用于指示 `INSERT` 执行行为的一些指示符。目前 hint 有三个可选值`/*+ STREAMING */`、`/*+ SHUFFLE */`或`/*+ NOSHUFFLE */` > 1. STREAMING：目前无实际作用，只是为了兼容之前的版本，因此保留。（之前的版本加上这个 hint 会返回 label，现在默认都会返回 label）\n> 2. SHUFFLE：当目标表是分区表，开启这个 hint 会进行 repartiiton。 > 3. NOSHUFFLE：即使目标表是分区表，也不会进行 repartiiton，但会做一些其他操作以保证数据正确落到各个分区中。\n对于开启了merge-on-write的Unique表，还可以使用insert语句进行部分列更新的操作。要使用insert语句进行部分列更新，需要将会话变量enable_unique_key_partial_update的值设置为true(该变量默认值为false，即默认无法通过insert语句进行部分列更新)。进行部分列更新时，插入的列必须至少包含所有的Key列，同时指定需要更新的列。如果插入行Key列的值在原表中存在，则将更新具有相同key列值那一行的数据。如果插入行Key列的值在原表中不存在，则将向\n表中插入一条新的数据，此时insert语句中没有指定的列必须有默认值或可以为null，这些缺失列会首先尝试用默认值填充，如果该列没有默认值，则尝试使用null值填充，如果该列不能为null，则本次插入失败。\n需要注意的是，控制insert语句是否开启严格模式的会话变量`enable_insert_strict`的默认值为true，即insert语句默认开启严格模式，而在严格模式下进行部分列更新不允许更新不存在的key。所以，在使用insert语句进行部分列更新的时候如果希望能插入不存在的key，需要在`enable_unique_key_partial_update`设置为true的基础上同时将`enable_insert_strict`设置为false。 注意：\n当前执行 `INSERT` 语句时，对于有不符合目标表格式的数据，默认的行为是过滤，比如字符串超长等。但是对于有要求数据不能够被过滤的业务场景，可以通过设置会话变量 `enable_insert_strict` 为 `true` 来确保当有数据被过滤掉的时候，`INSERT` 不会被执行成功。 ### Example `test` 表包含两个列`c1`, `c2`。 1. 向`test`表中导入一行数据\n```sql INSERT INTO test VALUES (1, 2); INSERT INTO test (c1, c2) VALUES (1, 2); INSERT INTO test (c1, c2) VALUES (1, DEFAULT); INSERT INTO test (c1) VALUES (1);\n``` 其中第一条、第二条语句是一样的效果。在不指定目标列时，使用表中的列顺序来作为默认的目标列。 第三条、第四条语句表达的意思是一样的，使用`c2`列的默认值，来完成数据导入。 2. 向`test`表中一次性导入多行数据\n```sql INSERT INTO test VALUES (1, 2), (3, 2 + 2); INSERT INTO test (c1, c2) VALUES (1, 2), (3, 2 * 2); INSERT INTO test (c1) VALUES (1), (3); INSERT INTO test (c1, c2) VALUES (1, DEFAULT), (3, DEFAULT);', 'stmt.setString(2, ""name"" + i); stmt.setInt(3, i + 10); int result = stmt.executeUpdate(); System.out.println(""rows: "" + result); } } } catch (Exception e) {\ne.printStackTrace(); } }\nprivate static void groupCommitInsertBatch() throws Exception { Class.forName(JDBC_DRIVER); // add rewriteBatchedStatements=true and cachePrepStmts=true in JDBC url\n// set session variables by sessionVariables=group_commit=async_mode in JDBC url try (Connection conn = DriverManager.getConnection(\nString.format(URL_PATTERN + ""&rewriteBatchedStatements=true&cachePrepStmts=true&sessionVariables=group_commit=async_mode"", HOST, PORT, DB), USER, PASSWD)) {\nString query = ""insert into "" + TBL + "" values(?, ?, ?)""; try (PreparedStatement stmt = conn.prepareStatement(query)) { for (int j = 0; j < 5; j++) { // 10 rows per insert\nfor (int i = 0; i < INSERT_BATCH_SIZE; i++) { stmt.setInt(1, i); stmt.setString(2, ""name"" + i); stmt.setInt(3, i + 10); stmt.addBatch(); }\nint[] result = stmt.executeBatch(); } } } catch (Exception e) { e.printStackTrace(); } }\n``` 关于**JDBC**的更多用法，参考[使用Insert方式同步数据](../import-scenes/jdbc-load.md)。 ## 修改group commit默认提交间隔 group commit 的默认提交间隔为 10 秒，用户可以通过修改表的配置，调整 group commit 的提交间隔： ```sql # 修改提交间隔为 2 秒 ALTER TABLE dt SET (""group_commit_interval_ms""=""2000"");\n``` ## 使用限制 * 当开启了 group commit 模式，系统会判断用户发起的`INSERT INTO VALUES`语句是否符合 group commit 的条件，如果符合，该语句的执行会进入到 group commit 写入中。符合以下条件会自动退化为非 group commit 方式： + 事务写入，即`Begin`; `INSERT INTO VALUES`; `COMMIT`方式\n+ 指定 label，即`INSERT INTO dt WITH LABEL {label} VALUES` + VALUES 中包含表达式，即`INSERT INTO dt VALUES (1 + 100)` + 列更新写入 + 表不支持 light schema change']","在使用Apache Doris进行数据插入时，如果发现实际插入的数据量少于预期，可能的原因包括但不限于以下几点：

1. **数据过滤**：Apache Doris在执行`INSERT INTO VALUES`语句时，对于不符合目标表格式的数据，默认行为是过滤这些数据。例如，如果某个字符串值超过了目标列定义的长度，这条数据可能会被自动过滤。如果业务场景中有要求数据不能被过滤的话，可以通过设置会话变量`enable_insert_strict`为`true`来确保当有数据被过滤掉时，`INSERT`操作不会被成功执行。然而，根据您提供的信息，日志中没有明确的错误信息，只有内存GC的告警，这可能意味着数据过滤并非主要原因。

2. **事务和批处理**：Apache Doris支持事务，如果在批量插入过程中某些批次因为各种原因（比如内存问题、网络问题等）未能成功提交，那么这部分数据将不会被持久化。此外，如果您使用了JDBC批处理插入，确保每个批次执行了`executeBatch()`方法，并检查了返回的结果数组以确认每个批次是否成功。

3. **内存限制和GC**：您提到的日志中有内存GC的告警，这暗示在数据插入过程中可能出现了内存压力。Apache Doris在处理大量插入时，如果内存不足，可能会导致部分数据插入失败。建议优化内存配置，增加可用内存，或者调整批量插入的批次大小，尝试减少每批次的数据量。

4. **配置和版本问题**：检查Apache Doris的版本和相关配置，确保没有已知的bug或者配置限制影响数据插入。尤其是检查`group commit`相关配置，如`group_commit_interval_ms`等，这些配置可能影响数据插入行为。

解决方案建议：

- **检查日志和监控**：仔细检查Apache Doris的日志文件，包括fe.log和be.log，寻找任何可能指示数据插入失败的错误信息。
- **优化内存配置**：根据GC告警，优化JVM和Doris的内存配置，确保有足够的内存用于处理大量插入操作。
- **分批插入数据**：如果可能，尝试减小每次插入的数据量，分多次插入，以减轻服务器的压力。
- **确认数据格式和类型**：确保插入的数据完全符合目标表的列定义，没有超出长度或类型不匹配的情况。

如果以上建议都不能解决问题，建议在Apache Doris的社区或者GitHub issue中寻求帮助，提供更详细的日志和配置信息以便社区成员或开发者提供更具体的指导。",39.87,,ERROR,"['---\n\n<!--split-->\n# 使用 Insert 方式同步数据\n\n用户可以通过 MySQL 协议，使用 INSERT 语句进行数据导入。\n\nINSERT 语句的使用方式和 MySQL 等数据库中 INSERT 语句的使用方式类似。 INSERT 语句支持以下两种语法：\n\n```sql\n* INSERT INTO table SELECT ...\n* INSERT INTO table VALUES(...)\n```\n\n这里我们仅介绍第二种方式。关于 INSERT 命令的详细说明，请参阅 [INSERT](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/INSERT.md) 命令文档。\n\n## 单次写入\n\n单次写入是指用户直接执行一个 INSERT 命令。示例如下：\n\n```sql\nINSERT INTO example_tbl (col1, col2, col3) VALUES (1000, ""test"", 3.25);\n```\n\n对于 Doris 来说，一个 INSERT 命令就是一个完整的导入事务。\n\n因此不论是导入一条数据，还是多条数据，我们都不建议在生产环境使用这种方式进行数据导入。高频次的 INSERT 操作会导致在存储层产生大量的小文件，会严重影响系统性能。\n\n该方式仅用于线下简单测试或低频少量的操作。\n\n或者可以使用以下方式进行批量的插入操作：\n\n```sql\nINSERT INTO example_tbl VALUES\n(1000, ""baidu1"", 3.25)\n(2000, ""baidu2"", 4.25)\n(3000, ""baidu3"", 5.25);\n```\n\n我们建议一批次插入条数在尽量大，比如几千甚至一万条一次。或者可以通过下面的程序的方式，使用 PreparedStatement 来进行批量插入。\n\n## JDBC 示例\n\n这里我们给出一个简单的 JDBC 批量 INSERT 代码示例：\n\n```java\npackage demo.doris;', '1. bdb 目录\n\n    我们将 [bdbje](https://www.oracle.com/technetwork/database/berkeleydb/overview/index-093405.html) 作为一个分布式的 kv 系统，存放元数据的 journal。这个 bdb 目录相当于 bdbje 的 “数据目录”。\n    \n    其中 `.jdb` 后缀的是 bdbje 的数据文件。这些数据文件会随着元数据 journal 的不断增多而越来越多。当 Doris 定期做完 image 后，旧的日志就会被删除。所以正常情况下，这些数据文件的总大小从几 MB 到几 GB 不等（取决于使用 Doris 的方式，如导入频率等）。当数据文件的总大小大于 10GB，则可能需要怀疑是否是因为 image 没有成功，或者分发 image 失败导致的历史 journal 一直无法删除。\n    \n    `je.info.0` 是 bdbje 的运行日志。这个日志中的时间是 UTC+0 时区的。我们可能在后面的某个版本中修复这个问题。通过这个日志，也可以查看一些 bdbje 的运行情况。\n\n2. image 目录', '自 1.2.0 版本起，推荐使用以下功能验证 `image_mem` 文件：\n\n    ```\n    sh start_fe.sh --image path_to_image_mem\n    ```\n\n    > 注意：`path_to_image_mem` 是 image_mem 文件的路径。\n    >\n    > 如果文件有效会输出 `Load image success. Image file /absolute/path/to/image.xxxxxx is valid`。\n    >\n    > 如果文件无效会输出 `Load image failed. Image file /absolute/path/to/image.xxxxxx is invalid`。\n\n4. 依次用 image_mem 文件替换掉 FOLLOWER FE 节点上`meta_dir/image`目录下的 image 文件，重启 FOLLOWER FE 节点，\n确认元数据和查询服务都正常\n\n5. 用 image_mem 文件替换掉 Master FE 节点上`meta_dir/image`目录下的 image 文件，重启 Master FE 节点，\n确认 FE Master 切换正常， Master FE 节点可以通过 checkpoint 正常生成新的 image 文件\n6. 集群恢复所有 Load,Create,Alter 操作\n\n**注意：如果 Image 文件很大，整个操作过程耗时可能会很长，所以在此期间，要确保 Master FE 不会通过 checkpoint 生成新的 image 文件。\n当观察到 Master FE 节点上 `meta_dir/image`目录下的 `image.ckpt` 文件快和 `image.xxx` 文件一样大时，可以直接删除掉`image.ckpt` 文件。**\n\n### 查看 BDBJE 中的数据\n\nFE 的元数据日志以 Key-Value 的方式存储在 BDBJE 中。某些异常情况下，可能因为元数据错误而无法启动 FE。在这种情况下，Doris 提供一种方式可以帮助用户查询 BDBJE 中存储的数据，以方便进行问题排查。', '---\n{\n    ""title"": ""Insert Into"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# Insert Into\n\nInsert Into 语句的使用方式和 MySQL 等数据库中 Insert Into 语句的使用方式类似。但在 Doris 中，所有的数据写入都是一个独立的导入作业。所以这里将 Insert Into 也作为一种导入方式介绍。\n\n主要的 Insert Into 命令包含以下两种；\n\n* INSERT INTO tbl SELECT ...\n* INSERT INTO tbl (col1, col2, ...) VALUES (1, 2, ...), (1,3, ...);\n\n其中第二种命令仅用于 Demo，不要使用在测试或生产环境中。\n\n## 导入操作及返回结果\n\nInsert Into 命令需要通过 MySQL 协议提交，创建导入请求会同步返回导入结果。\n\n以下是两个Insert Into的使用示例：\n\n```sql\nINSERT INTO tbl2 WITH LABEL label1 SELECT * FROM tbl3;\nINSERT INTO tbl1 VALUES (""qweasdzxcqweasdzxc""), (""a"");\n```', '当该参数设置为 true 时，表示如果有一条数据错误，则导入失败。\n\n  默认为 false。可通过 `SET enable_insert_strict = true;` 来设置。 \n\n+ insert\\_timeout\n\n  Insert Into 本身也是一个 SQL 命令，Insert Into 语句受到 Session 变量 <version since=""dev"" type=""inline"">`insert_timeout`</version> 的限制。可以通过 `SET insert_timeout = xxx;` 来增加超时时间，单位是秒。\n\n## 最佳实践\n\n### 应用场景\n\n1. 用户希望仅导入几条假数据，验证一下 Doris 系统的功能。此时适合使用 [INSERT INTO VALUES](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/INSERT.md) 的语法，这里语法和MySql语法一样。\n2. 用户希望将已经在 Doris 表中的数据进行 ETL 转换并导入到一个新的 Doris 表中，此时适合使用 INSERT INTO SELECT 语法。\n3. 用户可以创建一种外部表，如 MySQL 外部表映射一张 MySQL 系统中的表。或者创建 [Broker](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/BROKER-LOAD.md) 外部表来映射 HDFS 上的数据文件。然后通过 INSERT INTO SELECT 语法将外部表中的数据导入到 Doris 表中存储。\n\n### 数据量\n\nInsert Into 对数据量没有限制，大数据量导入也可以支持。但 Insert Into 有默认的超时时间，用户预估的导入数据量过大，就需要修改系统的 Insert Into 导入超时时间。\n\n```\n导入数据量 = 36G 约≤ 3600s * 10M/s \n其中 10M/s 是最大导入限速，用户需要根据当前集群情况计算出平均的导入速度来替换公式中的 10M/s\n```\n\n### 完整例子', '```\n导入数据量 = 36G 约≤ 3600s * 10M/s \n其中 10M/s 是最大导入限速，用户需要根据当前集群情况计算出平均的导入速度来替换公式中的 10M/s\n```\n\n### 完整例子\n\n用户有一张表 store\\_sales 在数据库 sales 中，用户又创建了一张表叫 bj\\_store\\_sales 也在数据库 sales 中，用户希望将 store\\_sales 中销售记录在 bj 的数据导入到这张新建的表 bj\\_store\\_sales 中。导入的数据量约为：10G。\n\n```sql\nstore_sales schema：\n(id, total, user_id, sale_timestamp, region)\n\nbj_store_sales schema:\n(id, total, user_id, sale_timestamp)\n```\n\n集群情况：用户当前集群的平均导入速度约为 5M/s\n\n+ Step1: 判断是否要修改 Insert Into 的默认超时时间\n\n  ```\n  计算导入的大概时间\n  10G / 5M/s = 2000s\n  \n  修改 FE 配置\n  insert_load_default_timeout_second = 2000\n  ```\n\n+ Step2：创建导入任务\n\n  由于用户是希望将一张表中的数据做 ETL 并导入到目标表中，所以应该使用 Insert into query\\_stmt 方式导入。\n\n  ```sql\n  INSERT INTO bj_store_sales WITH LABEL `label` SELECT id, total, user_id, sale_timestamp FROM store_sales where region = ""bj"";\n  ```\n\n## 常见问题\n\n* 查看错误行\n\n  由于 Insert Into 无法控制错误率，只能通过 `enable_insert_strict` 设置为完全容忍错误数据或完全忽略错误数据。因此如果 `enable_insert_strict` 设为 true，则 Insert Into 可能会失败。而如果 `enable_insert_strict` 设为 false，则可能出现仅导入了部分合格数据的情况。', '然后通过 Stream Load，增加 Header `partial_columns:true`，并指定要导入的列名即可完成更新\n\n```\ncurl  --location-trusted -u root: -H ""partial_columns:true"" -H ""column_separator:,"" -H ""columns:id,balance,last_access_time"" -T /tmp/test.csv http://127.0.0.1:48037/api/db1/user_profile/_stream_load\n```\n\n# 更广泛的分析场景支持\n\n## 10 倍以上性价比的日志分析方案\n\n从过去的实时报表和 Ad-hoc 等典型 OLAP 场景到 ELT/ETL、日志检索与分析等更多业务场景，Apache Doris 正在不断拓展应用场景的边界，而日志数据的统一存储与分析正是我们在 2.0 版本的重要突破。\n\n过去业界典型的日志存储分析架构难以同时兼顾 高吞吐实时写入、低成本大规模存储与高性能文本检索分析，只能在某一方面或某几方面做权衡取舍。而在 Apache Doris 2.0 版本中，我们引入了全新倒排索引、以满足字符串类型的全文检索和普通数值/日期等类型的等值、范围检索，同时进一步优化倒排索引的查询性能、使其更加契合日志数据分析的场景需求，同时结合过去在大规模数据写入和低成本存储等方面的优势，实现了更高性价比的日志分析方案。\n\n在相同硬件配置和数据集的测试表现上，Apache Doris 相对于 ElasticSearch 实现了日志数据写入速度提升 4 倍、存储空间降低 80%、查询性能提升 2 倍，再结合 Apache Doris 2.0 版本引入的冷热数据分层特性，整体性价比提升 10 倍以上。\n\n![](/images/release-note-2.0beta-es-log.png)\n\n除了日志分析场景的优化以外，在复杂数据类型方面，我们增加了全新的数据类型 Map/Struct，包括支持以上类型的高效写入、存储、分析函数以及类型之间的相互嵌套，以更好满足多模态数据分析的支持。', '![](/images/release-note-2.0beta-ycsb-qps.png)\n\n参考文档：[https://doris.apache.org/zh-CN/docs/dev/query-acceleration/hight-concurrent-point-query](https://doris.apache.org/zh-CN/docs/dev/query-acceleration/hight-concurrent-point-query)\n\n详细介绍：[https://mp.weixin.qq.com/s/Ow77-kFMWXFxugFXjOPHhg](https://mp.weixin.qq.com/s/Ow77-kFMWXFxugFXjOPHhg)\n\n## 更全面、更高性能的数据湖分析能力\n\n在 Apache Doris 1.2 版本中，我们发布了 Multi-Catalog 功能，支持了多种异构数据源的元数据自动映射与同步，实现了数据湖的无缝对接。依赖 数据读取、执行引擎、查询优化器方面的诸多优化，在标准测试集场景下，Apache Doris 在湖上数据的查询性能，较 Presto/Trino 有 3-5 倍的提升。\n\n在 2.0 版本中，我们进一步对数据湖分析能力进行了加强，不但支持了更多的数据源，同时针对用户的实际生产环境做了诸多优化，相较于 1.2 版本，能够在真实工作负载情况下显著提升性能。\n\n**更多数据源支持**\n\n- 支持 Hudi Copy-on-Write 表的 Snapshot Query 以及 Merge-on-Read 表的 Read Optimized Query 和 Read Optimized Query，后续将支持 Incremental Query 和 Time Traval。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hudi](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hudi)', '在 Apache Doris 中每次 Commit 都会产生一个数据版本，在高并发写入场景下，容易出现因数据版本过多且 Compaction 不及时而导致的 -235 错误，同时查询性能也会随之下降。\n\n在 1.1 版本中我们引入了 QuickCompaction，增加了主动触发式的 Compaction 检查，在数据版本增加的时候主动触发 Compaction，同时通过提升分片元信息扫描的能力，快速发现数据版本过多的分片并触发 Compaction。通过主动式触发加被动式扫描的方式，彻底解决数据合并的实时性问题。\n\n同时，针对高频的小文件 Cumulative Compaction，实现了 Compaction 任务的调度隔离，防止重量级的 Base Compaction 对新增数据的合并造成影响。\n\n最后，针对小文件合并，优化了小文件合并的策略，采用梯度合并的方式，每次参与合并的文件都属于同一个数据量级，防止大小差别很大的版本进行合并，逐渐有层次的合并，减少单个文件参与合并的次数，能够大幅地节省系统的 CPU 消耗。\n\n![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a6d5c50f16a048f3ab27357bc97b7461~tplv-k3u1fbpfcp-zoom-1.image)\n\n在数据上游维持每秒 10w 的写入频率时（20 个并发写入任务、每个作业 5000 行、 Checkpoint 间隔 1s），1.1 版本表现如下：\n\n-   数据快速合并：Tablet 数据版本维持在 50 以下，Compaction Score 稳定。相较于之前版本高并发写入时频繁出现的 -235 问题，**Compaction 合并效率有 10 倍以上的提升**。\n\n<!---->\n\n-   CPU 资源消耗显著降低：针对小文件 Compaction 进行了策略优化，在上述高并发写入场景下，**CPU 资源消耗降低 25%** ；\n\n<!---->\n\n-   查询耗时稳定：提升了数据整体有序性，大幅降低查询耗时的波动性，**高并发写入时的查询耗时与仅查询时持平**，查询性能较之前版本**有 3-4 倍提升**。', '当需要使用 `CTE(Common Table Expressions)` 作为 insert 操作中的查询部分时，必须指定 `WITH LABEL` 和 `column` 部分。\n\n4. 过滤阈值\n\n   与其他导入方式不同，INSERT 操作不能指定过滤阈值（`max_filter_ratio`）。默认的过滤阈值为 1，即素有错误行都可以被忽略。\n\n   对于有要求数据不能够被过滤的业务场景，可以通过设置 [会话变量](../../../../advanced/variables.md) `enable_insert_strict` 为 `true` 来确保当有数据被过滤掉的时候，`INSERT` 不会被执行成功。\n\n5. 性能问题\n\n   不建议使用 `VALUES` 方式进行单行的插入。如果必须这样使用，请将多行数据合并到一个 INSERT 语句中进行批量提交。\n---\n{\n    ""title"": ""ANALYZE"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## ANALYZE\n\n### Name\n\n<version since=""2.0""></version>\n\nANALYZE\n\n### Description\n\n该语句用于收集各列的统计信息。\n\n```sql\nANALYZE < TABLE | DATABASE table_name | db_name > \n    [ (column_name [, ...]) ]\n    [ [ WITH SYNC ] [ WITH SAMPLE PERCENT | ROWS ] ];\n```\n\n- table_name: 指定的目标表。可以是 `db_name.table_name` 形式。\n- column_name: 指定的目标列。必须是 `table_name` 中存在的列，多个列名称用逗号分隔。\n- sync：同步收集统计信息。收集完后返回。若不指定则异步执行并返回JOB ID。\n- sample percent | rows：抽样收集统计信息。可以指定抽样比例或者抽样行数。\n\n### Example\n\n对一张表按照10%的比例采样收集统计数据：\n\n```sql\nANALYZE TABLE lineitem WITH SAMPLE PERCENT 10;\n```']","在Apache Doris中，使用`INSERT INTO VALUES`语句插入数据时，确实存在一种情况可能导致数据丢失。当一次性插入大量数据时，如果内存不足，Doris可能会尝试进行垃圾回收（GC）以释放内存。然而，在某些情况下，GC可能会意外删除尚未持久化的数据，从而导致数据丢失。

为了避免这种问题，建议采取以下措施：

1. 增加FE节点的内存分配，以确保有足够的内存处理大型INSERT语句。
2. 分批插入数据，而不是一次性插入大量数据。这有助于减轻内存压力，并降低数据丢失的风险。
3. 尝试启用`enable_insert_strict`会话变量。设置为`true`后，如果在插入过程中发生任何错误，整个INSERT语句将会失败。这可以帮助您更快地识别并解决问题。

请注意，这是一个潜在的问题，具体情况可能因您的集群配置和负载而异。如果您继续遇到此问题，建议检查Doris的其他相关日志，以便更准确地诊断问题。"
839ec443-c18a-11ee-9091-b026288862a5,"各位大佬，uniq key 或者那几个模型，对key的要求是不是不能用text之类的啊,我们要是主键包含text类型的应该怎么做啊,,","换成varchar,key列不能是string","['``` 更新后结果如下\n```sql +----------+--------------+--------------+ | order_id | order_amount | order_status | +----------+--------------+--------------+ | 1 | 100 | 待发货 | +----------+--------------+--------------+ 1 row in set (0.01 sec)\n``` ### Aggregate Key模型 #### 建表 将需要进行列更新的字段对应的聚合函数设置为`REPLACE_IF_NOT_NULL`\n``` CREATE TABLE `order_tbl` ( `order_id` int(11) NULL, `order_amount` int(11) REPLACE_IF_NOT_NULL NULL, `order_status` varchar(100) REPLACE_IF_NOT_NULL NULL ) ENGINE=OLAP AGGREGATE KEY(`order_id`) COMMENT \'OLAP\'\nDISTRIBUTED BY HASH(`order_id`) BUCKETS 1 PROPERTIES ( ""replication_allocation"" = ""tag.location.default: 1"" );\n``` #### 数据写入 无论是导入任务还是`INSERT INTO`, 直接写入要更新的字段的数据即可 #### 示例 与前面例子相同，对应的Stream Load命令为（不需要额外的header）：\n``` curl --location-trusted -u root: -H ""column_separator:,"" -H ""columns:order_id,order_status"" -T /tmp/update.csv http://127.0.0.1:48037/api/db1/order_tbl/_stream_load ``` 对应的`INSERT INTO`语句为（不需要额外设置session variable）：\n``` INSERT INTO order_tbl (order_id, order_status) values (1,\'待发货\');\n``` ## 使用限制 ### Unique Key模型Merge-on-Write实现 在2.0版本中，同一批次数据写入任务（无论是导入任务还是`INSERT INTO`）的所有行只能更新相同的列，如果需要更新不同列的数据，则需要分不同的批次进行写入 在2.1版本中，我们将支持灵活的列更新，用户可以在同一批导入中，每一行更新不同的列 ### Aggregate Key模型\n用户无法通过将某个字段由非NULL设置为NULL，写入的NULL值在`REPLACE_IF_NOT_NULL`聚合函数的处理中会自动忽略 --- { ""title"": ""Sequence 列"", ""language"": ""zh-CN"" } --- <!--split--> # sequence 列\nUniq模型主要针对需要唯一主键的场景，可以保证主键唯一性约束，但是由于使用REPLACE聚合方式，在同一批次中导入的数据，替换顺序不做保证，详细介绍可以参考[数据模型](../../data-table/data-model.md)。替换顺序无法保证则无法确定最终导入到表中的具体数据，存在了不确定性。', 'HLL是模糊去重，在数据量大的情况性能优于Count Distinct。 HLL的误差通常在1%左右，有时会达到2%。 ### example\nselect hour, HLL_UNION_AGG(pv) over(order by hour) uv from( select hour, HLL_RAW_AGG(device_id) as pv from metric_table -- 查询每小时的累计UV where datekey=20200622 group by hour order by 1 ) final; ### keywords\nHLL,HYPERLOGLOG --- { ""title"": ""ARRAY"", ""language"": ""zh-CN"" } --- <!--split--> ## ARRAY ### name <version since=""1.2.0""> ARRAY </version> ### description `ARRAY<T>` 由T类型元素组成的数组，不能作为key列使用。目前支持在Duplicate模型的表中使用。 <version since=""2.0"">\n2.0 版本之后支持在Unique模型的表中非key列使用。 </version> T支持的类型有：\n``` BOOLEAN, TINYINT, SMALLINT, INT, BIGINT, LARGEINT, FLOAT, DOUBLE, DECIMAL, DATE, DATEV2, DATETIME, DATETIMEV2, CHAR, VARCHAR, STRING ``` ### example 建表示例如下：\n``` mysql> CREATE TABLE `array_test` ( `id` int(11) NULL COMMENT """", `c_array` ARRAY<int(11)> NULL COMMENT """" ) ENGINE=OLAP DUPLICATE KEY(`id`) COMMENT ""OLAP"" DISTRIBUTED BY HASH(`id`) BUCKETS 1 PROPERTIES (\n""replication_allocation"" = ""tag.location.default: 1"", ""in_memory"" = ""false"", ""storage_format"" = ""V2"" );\n``` 插入数据示例： ``` mysql> INSERT INTO `array_test` VALUES (1, [1,2,3,4,5]); mysql> INSERT INTO `array_test` VALUES (2, [6,7,8]), (3, []), (4, null); ``` 查询数据示例：\n``` mysql> SELECT * FROM `array_test`; +------+-----------------+ | id | c_array | +------+-----------------+ | 1 | [1, 2, 3, 4, 5] | | 2 | [6, 7, 8] | | 3 | [] | | 4 | NULL | +------+-----------------+\n``` ### keywords ARRAY --- { ""title"": ""DATETIME"", ""language"": ""zh-CN"" } --- <!--split--> ## DATETIME <version since=""1.2.0""> DATETIMEV2 </version> ### description\nDATETIME([P]) 日期时间类型，可选参数P表示时间精度，取值范围是[0, 6]，即最多支持6位小数（微秒）。不设置时为0。 取值范围是[\'0000-01-01 00:00:00[.000000]\', \'9999-12-31 23:59:59[.999999]\']. 打印的形式是\'yyyy-MM-dd HH:mm:ss.SSSSSS\' ### note', '某些情况下，当用户使用相同的 SQL 查询一个 Unique Key 模型的表时，可能会出现多次查询结果不一致的现象。并且查询结果总在 2-3 种之间变化。 这可能是因为，在同一批导入数据中，出现了 key 相同但 value 不同的数据，这会导致，不同副本间，因数据覆盖的先后顺序不确定而产生的结果不一致的问题。 比如表定义为 k1, v1。一批次导入数据如下： ```text 1, ""abc"" 1, ""def""\n``` 那么可能副本1 的结果是 `1, ""abc""`，而副本2 的结果是 `1, ""def""`。从而导致查询结果不一致。 为了确保不同副本之间的数据先后顺序唯一，可以参考 [Sequence Column](../data-operate/update-delete/sequence-column-manual.md) 功能。 ### Q5. 查询 bitmap/hll 类型的数据返回 NULL 的问题\n在 1.1.x 版本中，在开启向量化的情况下，执行查询数据表中 bitmap 类型字段返回结果为 NULL 的情况下， 1. 首先你要 `set return_object_data_as_binary=true;` 2. 关闭向量化 `set enable_vectorized_engine=false;` 3. 关闭 SQL 缓存 `set [global] enable_sql_cache = false;`\n这里是因为 bitmap / hll 类型在向量化执行引擎中：输入均为NULL，则输出的结果也是NULL而不是0 ### Q6. 访问对象存储时报错：curl 77: Problem with the SSL CA cert 如果 be.INFO 日志中出现 `curl 77: Problem with the SSL CA cert` 错误。可以尝试通过以下方式解决：\n1. 在 [https://curl.se/docs/caextract.html](https://curl.se/docs/caextract.html) 下载证书：cacert.pem 2. 拷贝证书到指定位置：`sudo cp /tmp/cacert.pem /etc/ssl/certs/ca-certificates.crt` 3. 重启 BE 节点。\n### Q7. 导入报错：""Message"": ""[INTERNAL_ERROR]single replica load is disabled on BE."" 1. be.conf中增加 enable_single_replica_load = true 2. 重启 BE 节点。--- { ""title"": ""数据操作问题"", ""language"": ""zh-CN"" } --- <!--split--> # 数据操作问题\n本文档主要用于记录 Doris 使用过程中的数据操作常见问题。会不定期更新。 ### Q1. 使用 Stream Load 访问 FE 的公网地址导入数据，被重定向到内网 IP？\n当 stream load 的连接目标为FE的http端口时，FE仅会随机选择一台BE节点做http 307 redirect 操作，因此用户的请求实际是发送给FE指派的某一个BE的。而redirect返回的是BE的ip，也即内网IP。所以如果你是通过FE的公网IP发送的请求，很有可能因为redirect到内网地址而无法连接。\n通常的做法，一种是确保自己能够访问内网IP地址，或者是给所有BE上层架设一个负载均衡，然后直接将 stream load 请求发送到负载均衡器上，由负载均衡将请求透传到BE节点。 ### Q2. Doris 是否支持修改列名？ 在 1.2.0 版本之后, 开启 `""light_schema_change""=""true""` 选项时，可以支持修改列名。 在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：\nDoris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。 因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。 我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。 ### Q3. Unique Key模型的表是否支持创建物化视图？ 不支持。\nUnique Key模型的表是一个对业务比较友好的表，因为其特有的按照主键去重的功能，能够很方便的同步数据频繁变更的业务数据库。因此，很多用户在将数据接入到Doris时，会首先考虑使用Unique Key模型。']","在Doris中，对于主键的类型有一定的限制，确实不能直接使用`TEXT`类型作为主键的一部分。这是因为`TEXT`类型的字段通常用于存储较长的文本数据，而主键的设计通常考虑到性能和存储效率，因此推荐使用较短的数据类型来定义主键。

如果你的业务场景中确实需要使用到类似`TEXT`类型的数据作为主键，有以下几种解决方案：

1. **使用其他类型代替**: 考虑是否能够将`TEXT`字段改为`VARCHAR`类型，并限制一个合理的长度。`VARCHAR`类型在一定长度内可以作为主键或主键的一部分。

2. **使用散列函数**: 将`TEXT`类型字段通过散列函数（如MD5、SHA等）转换为固定长度的字符串，然后将这个字符串作为主键或主键的一部分。这种方法可以有效地将长文本转换为适合作为主键的格式，但需要注意的是，散列可能会导致极低概率的冲突。

3. **分割字段**: 如果`TEXT`字段中有部分数据是具有唯一性的，可以考虑将这部分数据分割出来作为主键的一部分，而将剩余的文本数据作为非主键字段存储。

需要注意的是，选择合适的方案应当基于对业务需求、数据特性和性能影响的综合考虑。在做出决策前，建议进行充分的测试，以确保所选方案能够满足业务需求且不会对系统性能产生负面影响。",49.74,,ERROR,"['### keywords\n\n    STRING\n---\n{\n    ""title"": ""HLL(HyperLogLog)"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## HLL(HyperLogLog)\n### description\nHLL\nHLL不能作为key列使用，支持在Aggregate模型、Duplicate模型和Unique模型的表中使用。在Aggregate模型表中使用时，建表时配合的聚合类型为HLL_UNION。\n用户不需要指定长度和默认值。长度根据数据的聚合程度系统内控制。\n并且HLL列只能通过配套的hll_union_agg、hll_raw_agg、hll_cardinality、hll_hash进行查询或使用。\n\nHLL是模糊去重，在数据量大的情况性能优于Count Distinct。\nHLL的误差通常在1%左右，有时会达到2%。\n\n### example\n\n    select hour, HLL_UNION_AGG(pv) over(order by hour) uv from(\n       select hour, HLL_RAW_AGG(device_id) as pv\n       from metric_table -- 查询每小时的累计UV\n       where datekey=20200622\n    group by hour order by 1\n    ) final;\n\n### keywords\n\n    HLL,HYPERLOGLOG\n---\n{\n    ""title"": ""ARRAY"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## ARRAY\n\n### name\n\n<version since=""1.2.0"">\n\nARRAY\n\n</version>\n\n### description\n\n`ARRAY<T>`\n\n由T类型元素组成的数组，不能作为key列使用。目前支持在Duplicate模型的表中使用。\n\n<version since=""2.0"">\n\n2.0 版本之后支持在Unique模型的表中非key列使用。\n\n</version>\n\nT支持的类型有：', '在 1.2.0 版本之前或未开启 `""light_schema_change""=""true""` 选项时，不支持修改列名, 原因如下：\n\nDoris支持修改数据库名、表名、分区名、物化视图（Rollup）名称，以及列的类型、注释、默认值等等。但遗憾的是，目前不支持修改列名。\n\n因为一些历史原因，目前列名称是直接写入到数据文件中的。Doris在查询时，也是通过列名查找到对应的列的。所以修改列名不仅是简单的元数据修改，还会涉及到数据的重写，是一个非常重的操作。\n\n我们不排除后续通过一些兼容手段来支持轻量化的列名修改操作。\n\n### Q3. Unique Key模型的表是否支持创建物化视图？\n\n不支持。\n\nUnique Key模型的表是一个对业务比较友好的表，因为其特有的按照主键去重的功能，能够很方便的同步数据频繁变更的业务数据库。因此，很多用户在将数据接入到Doris时，会首先考虑使用Unique Key模型。\n\n但遗憾的是，Unique Key模型的表是无法建立物化视图的。原因在于，物化视图的本质，是通过预计算来将数据“预先算好”，这样在查询时直接返回已经计算好的数据，来加速查询。在物化视图中，“预计算”的数据通常是一些聚合指标，比如求和、求count。这时，如果数据发生变更，如update或delete，因为预计算的数据已经丢失了明细信息，因此无法同步的进行更新。比如一个求和值5，可能是 1+4，也可能是2+3。因为明细信息的丢失，我们无法区分这个求和值是如何计算出来的，因此也就无法满足更新的需求。\n\n### Q4. tablet writer write failed, tablet_id=27306172, txn_id=28573520, err=-235 or -238\n\n这个错误通常发生在数据导入操作中。错误码为 -235。这个错误的含义是，对应tablet的数据版本超过了最大限制（默认500，由 BE 参数 `max_tablet_version_num` 控制），后续写入将被拒绝。比如问题中这个错误，即表示 27306172 这个tablet的数据版本超过了限制。', '详细介绍：[https://mp.weixin.qq.com/s/Ur4VpJtjByVL0qQNy_iQBw](https://mp.weixin.qq.com/s/Ur4VpJtjByVL0qQNy_iQBw)\n\n## 主键模型支持部分列更新\n\n在 Apache Doris 1.2 版本中我们引入了 Unique Key 模型的 Merg-on-Write 写时合并模式，在上游数据高频写入和更新的同时可以保证下游业务的高效稳定查询，实现了**实时写入和极速查询的统一。** 而 2.0 版本我们对 Unique Key 模型进行了全面增强。在功能上，支持了新的部分列更新能力，在上游多个源表同时写入时无需提前处理成宽表，直接通过部分列更新在写时完成 Join，大幅简化了宽表的写入流程。\n\n在性能上，2.0 版本大幅增强了 Unique Key 模型 Merge-on-Write 的大数据量写入性能和并发写入能力，大数据量导入较 1.2 版本有超过 50% 的性能提升，高并发导入有超过 10 倍的性能提升，并通过高效的并发处理机制来彻底解决了 publish timeout(Error -3115) 问题，同时由于 Doris 2.0 高效的 Compaction 机制，也不会出现 too many versions (Error-235) 问题。这使得 Merge-on-Write 能够在更广泛的场景下替代 Merge-on-Read 实现，同时我们还利用部分列更新能力来降低 UPDATE 语句和 DELETE 语句的计算成本，整体性能提升约 50%。\n\n### 部分列更新的使用示例（Stream Load）：\n\n例如有表结构如下', '- 对写入性能进行了大幅优化，高并发写入和混合负载写入场景的稳定性也显著提升。例如在单 Tablet 7GB 的重复导入测试中，数据导入的耗时从约 30 分钟缩短到了 90s，写入效率提升 20 倍；以某头部支付产品的场景压测为例，在 20 个并行写入任务下可以达到 30 万条每秒的写入吞吐，并且持续写入十几个小时后仍然表现非常稳定。\n\n- 支持部分列更新功能。在 2.0.0 版本之前 Apache Doris 仅支持通过 Aggregate Key 聚合模型的 Replace_if_not_null 进行部分列更新，在 2.0.0 版本中我们增加了 Unique Key 主键模型的部分列更新，在多张上游源表同时写入一张宽表时，无需由 Flink 进行多流 Join 打宽，直接写入宽表即可，减少了计算资源的消耗并大幅降低了数据处理链路的复杂性。同时在面对画像场景的实时标签列更新、订单场景的状态更新时，直接更新指定的列即可，较过去更为便捷。\n\n- 支持复杂条件更新和条件删除。在 2.0.0 版本之前 Unique Key 主键模型仅支持简单 Update 和 Delete 操作，在 2.0.0 版本中我们基于 Merge-on-Write 实现了复杂条件的数据更新和删除，并且执行效率更加高效。基于以上优化，Apache Doris 对于各类数据更新需求都有完备的能力支持！\n\n# 更加高效稳定的数据写入\n\n### 导入性能进一步提升\n\n聚焦于实时分析，我们在过去的几个版本中在不断增强实时分析能力，其中端到端的数据实时写入能力是优化的重要方向，在 Apache Doris 2.0 版本中，我们进一步强化了这一能力。通过 Memtable 不使用 Skiplist、并行下刷、单副本导入等优化，使得导入性能有了大幅提升：', '数据模型。\n\n`key_type(col1, col2, ...)`\n\n`key_type` 支持以下模型：\n\n* DUPLICATE KEY（默认）：其后指定的列为排序列。\n* AGGREGATE KEY：其后指定的列为维度列。\n* UNIQUE KEY：其后指定的列为主键列。\n\n<version since=""2.0"">\n注：当表属性`enable_duplicate_without_keys_by_default = true`时, 默认创建没有排序列的DUPLICATE表。\n</version>\n\n示例：\n\n```\nDUPLICATE KEY(col1, col2),\nAGGREGATE KEY(k1, k2, k3),\nUNIQUE KEY(k1, k2)\n```\n    \n#### table_comment\n\n表注释。示例：\n    \n    ```\n    COMMENT ""This is my first DORIS table""\n    ```\n\n#### partition_info\n\n分区信息，支持三种写法：\n\n1. LESS THAN：仅定义分区上界。下界由上一个分区的上界决定。\n\n    ```\n    PARTITION BY RANGE(col1[, col2, ...])\n    (\n        PARTITION partition_name1 VALUES LESS THAN MAXVALUE|(""value1"", ""value2"", ...),\n        PARTITION partition_name2 VALUES LESS THAN MAXVALUE|(""value1"", ""value2"", ...)\n    )\n    ```\n\n2. FIXED RANGE：定义分区的左闭右开区间。', '`""bloom_filter_columns"" = ""k1, k2, k3""`\n\n* `in_memory` \n\n    已弃用。只支持设置为\'false\'。\n\n* `compression`\n\n    Doris 表的默认压缩方式是 LZ4。1.1版本后，支持将压缩方式指定为ZSTD以获得更高的压缩比。\n\n    `""compression""=""zstd""`\n\n* `function_column.sequence_col`\n\n    当使用 UNIQUE KEY 模型时，可以指定一个sequence列，当KEY列相同时，将按照 sequence 列进行 REPLACE(较大值替换较小值，否则无法替换)\n\n    `function_column.sequence_col`用来指定sequence列到表中某一列的映射，该列可以为整型和时间类型（DATE、DATETIME），创建后不能更改该列的类型。如果设置了`function_column.sequence_col`, `function_column.sequence_type`将被忽略。\n\n    `""function_column.sequence_col"" = \'column_name\'`\n\n* `function_column.sequence_type`\n\n    当使用 UNIQUE KEY 模型时，可以指定一个sequence列，当KEY列相同时，将按照 sequence 列进行 REPLACE(较大值替换较小值，否则无法替换)\n\n    这里我们仅需指定顺序列的类型，支持时间类型或整型。Doris 会创建一个隐藏的顺序列。\n\n    `""function_column.sequence_type"" = \'Date\'`\n\n* `enable_unique_key_merge_on_write`\n\n    <version since=""1.2"" type=""inline""> unique表是否使用merge on write实现。</version>\n\n    该属性在 2.1 版本之前默认关闭，从 2.1 版本开始默认开启。\n\n* `light_schema_change`\n\n    <version since=""1.2"" type=""inline""> 是否使用light schema change优化。</version>', '## 数据模型的选择建议\n\n因为数据模型在建表时就已经确定，且**无法修改**。所以，选择一个合适的数据模型**非常重要**。\n\n1. Aggregate 模型可以通过预聚合，极大地降低聚合查询时所需扫描的数据量和查询的计算量，非常适合有固定模式的报表类查询场景。但是该模型对 count(*) 查询很不友好。同时因为固定了 Value 列上的聚合方式，在进行其他类型的聚合查询时，需要考虑语意正确性。\n2. Unique 模型针对需要唯一主键约束的场景，可以保证主键唯一性约束。但是无法利用 ROLLUP 等预聚合带来的查询优势。对于聚合查询有较高性能需求的用户，推荐使用自1.2版本加入的写时合并实现。\n3. Duplicate 适合任意维度的 Ad-hoc 查询。虽然同样无法利用预聚合的特性，但是不受聚合模型的约束，可以发挥列存模型的优势（只读取相关列，而不需要读取所有 Key 列）。\n4. 如果有部分列更新的需求，请查阅文档[部分列更新](../data-operate/update-delete/partial-update.md)获取相关使用建议\n---\n{\n    ""title"": ""Rollup 与查询"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# Rollup 与查询\n\nROLLUP 在多维分析中是“上卷”的意思，即将数据按某种指定的粒度进行进一步聚合。\n\n## 基本概念\n\n在 Doris 中，我们将用户通过建表语句创建出来的表称为 Base 表（Base Table）。Base 表中保存着按用户建表语句指定的方式存储的基础数据。\n\n在 Base 表之上，我们可以创建任意多个 ROLLUP 表。这些 ROLLUP 的数据是基于 Base 表产生的，并且在物理上是**独立存储**的。\n\nROLLUP 表的基本作用，在于在 Base 表的基础上，获得更粗粒度的聚合数据。\n\n下面我们用示例详细说明在不同数据模型中的 ROLLUP 表及其作用。\n\n###  Aggregate 和 Unique 模型中的 ROLLUP\n\n因为 Unique 只是 Aggregate 模型的一个特例，所以这里我们不加以区别。\n\n1. 示例1：获得每个用户的总消费']","在 Apache Doris 中，Unique Key 模型要求主键列必须是可比较的类型，这是因为 Unique Key 模型的核心特性是保证主键的唯一性，而这一特性依赖于比较操作。因此，Unique Key 列不能是文本（`STRING`）类型，因为文本类型的比较操作在数据库中通常是比较复杂的，且性能较差。

如果你的业务需求确实需要使用文本作为主键，你可以考虑以下几种替代方案：

1. **哈希主键**：你可以将文本转换为哈希值作为主键。这样，你实际上使用了一个整数（或长整数）类型的主键，而哈希值保证了唯一性。在 Doris 中，你可以使用 `HLL`（HyperLogLog）来作为主键，它是一种可以估算唯一值的算法，适用于文本类型的数据。

2. **组合主键**：将文本数据与其他可比较的数据（如时间戳、整数等）组合成复合主键。这样，你可以使用组合主键来保证唯一性，并且复合主键中的文本部分可以在查询时作为过滤条件使用。

3. **修改数据模型**：如果业务场景允许，你可以考虑使用 `DUPLICATE` 或 `AGGREGATE` 模型，这些模型对主键类型的限制不像 `UNIQUE` 模型那么严格。在这些模型中，你可以使用文本类型作为列，但不保证这些列的唯一性。

4. **处理后再导入**：在数据导入阶段，你可以将文本数据转换为适合作为主键的类型（比如通过哈希转换），然后再导入到 Doris 中。

选择哪种方案取决于你的具体业务需求和数据特点。如果你需要更详细的指导，建议参考 Doris 的官方文档或者社区资源，以获取最新和最准确的信息。"
839be244-c18a-11ee-a971-b026288862a5,"Doris目前可以热挂载磁盘吗？
",目前Doris还不支持热挂载磁盘，需要通过扩容节点来实现磁盘扩容。,"['对自动分区表导入数据时，为防止意外创建过多分区，我们使用了FE配置项`max_auto_partition_num`管控此类表自动创建时的最大分区数。如果确需创建更多分区，请修改FE master节点的该配置项。 --- { ""title"": ""运维常见问题"", ""language"": ""zh-CN"" } --- <!--split--> # 运维常见问题 本文档主要用于记录 Doris 使用过程中的运维常见问题。会不定期更新。\n**文中的出现的BE二进制文件名称 `doris_be`，在之前的版本中为 `palo_be`。** ### Q1. 通过 DECOMMISSION 下线BE节点时，为什么总会有部分tablet残留？ 在下线过程中，通过 show backends 查看下线节点的 tabletNum ，会观察到 tabletNum 数量在减少，说明数据分片正在从这个节点迁移走。当数量减到0时，系统会自动删除这个节点。但某些情况下，tabletNum 下降到一定数值后就不变化。这通常可能有以下两种原因：\n1. 这些 tablet 属于刚被删除的表、分区或物化视图。而刚被删除的对象会保留在回收站中。而下线逻辑不会处理这些分片。可以通过修改 FE 的配置参数 catalog_trash_expire_second 来修改对象在回收站中驻留的时间。当对象从回收站中被删除后，这些 tablet就会被处理了。 2. 这些 tablet 的迁移任务出现了问题。此时需要通过 `show proc ""/cluster_balance""` 来查看具体任务的错误了。\n对于以上情况，可以先通过 `show proc ""/cluster_health/tablet_health"";` 查看集群是否还有 unhealthy 的分片，如果为0，则可以直接通过 drop backend 语句删除这个 BE 。否则，还需要具体查看不健康分片的副本情况。 ### Q2. priorty_network 应该如何设置？\npriorty_network 是 FE、BE 都有的配置参数。这个参数主要用于帮助系统选择正确的网卡 IP 作为自己的 IP 。建议任何情况下，都显式的设置这个参数，以防止后续机器增加新网卡导致IP选择不正确的问题。\npriorty_network 的值是 CIDR 格式表示的。分为两部分，第一部分是点分十进制的 IP 地址，第二部分是一个前缀长度。比如 10.168.1.0/8 会匹配所有 10.xx.xx.xx 的IP地址，而 10.168.1.0/16 会匹配所有 10.168.xx.xx 的 IP 地址。\n之所以使用 CIDR 格式而不是直接指定一个具体 IP，是为了保证所有节点都可以使用统一的配置值。比如有两个节点：10.168.10.1 和 10.168.10.2，则我们可以使用 10.168.10.0/24 来作为 priorty_network 的值。 ### Q3. FE的Master、Follower、Observer都是什么？\n首先明确一点，FE 只有两种角色：Follower 和 Observer。而 Master 只是一组 Follower 节点中选择出来的一个 FE。Master 可以看成是一种特殊的 Follower。所以当我们被问及一个集群有多少 FE，都是什么角色时，正确的回答当时应该是所有 FE 节点的个数，以及 Follower 角色的个数和 Observer 角色的个数。\n所有 Follower 角色的 FE 节点会组成一个可选择组，类似 Paxos 一致性协议里的组概念。组内会选举出一个 Follower 作为 Master。当 Master 挂了，会自动选择新的 Follower 作为 Master。而 Observer 不会参与选举，因此 Observer 也不会成为 Master 。 一条元数据日志需要在多数 Follower 节点写入成功，才算成功。比如3个 FE ，2个写入成功才可以。这也是为什么 Follower 角色的个数需要是奇数的原因。\nObserver 角色和这个单词的含义一样，仅仅作为观察者来同步已经成功写入的元数据日志，并且提供元数据读服务。他不会参与多数写的逻辑。 通常情况下，可以部署 1 Follower + 2 Observer 或者 3 Follower + N Observer。前者运维简单，几乎不会出现 Follower 之间的一致性协议导致这种复杂错误情况（企业大多使用这种方式）。后者可以保证元数据写的高可用，如果是高并发查询场景，可以适当增加 Observer。\n### Q4. 节点新增加了新的磁盘，为什么数据没有均衡到新的磁盘上？ 当前Doris的均衡策略是以节点为单位的。也就是说，是按照节点整体的负载指标（分片数量和总磁盘利用率）来判断集群负载。并且将数据分片从高负载节点迁移到低负载节点。如果每个节点都增加了一块磁盘，则从节点整体角度看，负载并没有改变，所以无法触发均衡逻辑。 此外，Doris目前并不支持单个节点内部，各个磁盘间的均衡操作。所以新增磁盘后，不会将数据均衡到新的磁盘。', '```text I0916 23:21:22.038795 28087 task_worker_pool.cpp:1594] finish report TASK. master host: 10.10.10.10, port: 9222\n``` - `I0916 23:21:22.038795`：日志等级和日期时间。大写字母I表示INFO，W表示WARN，F表示FATAL。 - `28087`：线程id。通过线程id，就可以查看这个线程上下文信息，方面排查这个线程发生的事情。 - `task_worker_pool.cpp:1594`：代码文件和行号。 - `finish report TASK xxx`：日志内容。 通常情况下我们主要查看be.INFO日志。特殊情况下，如BE宕机，则需要查看be.out。\n### Q6. FE/BE 节点挂了应该如何排查原因? 1. BE BE进程是 C/C++ 进程，可能会因为一些程序Bug（内存越界，非法地址访问等）或 Out Of Memory（OOM）导致进程挂掉。此时我们可以通过以下几个步骤查看错误原因： 1. 查看be.out BE进程实现了在程序因异常情况退出时，会打印当前的错误堆栈到be.out里（注意是be.out，不是be.INFO或be.WARNING）。通过错误堆栈，通常能够大致获悉程序出错的位置。\n注意，如果be.out中出现错误堆栈，通常情况下是因为程序bug，普通用户可能无法自行解决，欢迎前往微信群、github discussion 或dev邮件组寻求帮助，并贴出对应的错误堆栈，以便快速排查问题。 2. dmesg\n如果be.out没有堆栈信息，则大概率是因为OOM被系统强制kill掉了。此时可以通过dmesg -T 这个命令查看linux系统日志，如果最后出现 Memory cgroup out of memory: Kill process 7187 (doris_be) score 1007 or sacrifice child 类似的日志，则说明是OOM导致的。\n内存问题可能有多方面原因，如大查询、导入、compaction等。Doris也在不断优化内存使用。欢迎前往微信群、github discussion 或dev邮件组寻求帮助。 3. 查看be.INFO中是否有F开头的日志。 F开头的日志是 Fatal 日志。如 F0916 ，表示9月16号的Fatal日志。Fatal日志通常表示程序断言错误，断言错误会直接导致进程退出（说明程序出现了Bug）。欢迎前往微信群、github discussion 或dev邮件组寻求帮助。\n2. FE FE 是 java 进程，健壮程度要优于 C/C++ 程序。通常FE 挂掉的原因可能是 OOM（Out-of-Memory）或者是元数据写入失败。这些错误通常在 fe.log 或者 fe.out 中有错误堆栈。需要根据错误堆栈信息进一步排查。 ### Q7. 关于数据目录SSD和HDD的配置, 建表有时候会遇到报错`Failed to find enough host with storage medium and tag`\nDoris支持一个BE节点配置多个存储路径。通常情况下，每块盘配置一个存储路径即可。同时，Doris支持指定路径的存储介质属性，如SSD或HDD。SSD代表高速存储设备，HDD代表低速存储设备。\n如果集群只有一种介质比如都是HDD或者都是SSD，最佳实践是不用在be.conf中显式指定介质属性。如果遇到上述报错```Failed to find enough host with storage medium and tag```，一般是因为be.conf中只配置了SSD的介质，而建表阶段中显式指定了```properties {""storage_medium"" = ""hdd""}```；同理如果be.conf只配置了HDD的介质，而而建表阶段中显式指定了```properties\n{""storage_medium"" = ""ssd""}```也会出现上述错误。解决方案可以修改建表的properties参数与配置匹配；或者将be.conf中SSD/HDD的显式配置去掉即可。\n通过指定路径的存储介质属性，我们可以利用Doris的冷热数据分区存储功能，在分区级别将热数据存储在SSD中，而冷数据会自动转移到HDD中。', '+----------------------------------------------------------------------+ | [0,13,0] | +----------------------------------------------------------------------+\nselect multi_search_all_positions(""Hello, World!"", [\'hello\', \'!\', \'world\', \'Hello\', \'World\']); +---------------------------------------------------------------------------------------------+\n| multi_search_all_positions(\'Hello, World!\', ARRAY(\'hello\', \'!\', \'world\', \'Hello\', \'World\')) | +---------------------------------------------------------------------------------------------+\n| [0, 13, 0, 1, 8] | +---------------------------------------------------------------------------------------------+\n``` ### keywords MULTI_SEARCH,SEARCH,POSITIONS --- { ""title"": ""MULTI_MATCH_ANY"", ""language"": ""zh-CN"" } --- <!--split--> ## multi_match_any ### Description #### Syntax `TINYINT multi_match_any(VARCHAR haystack, ARRAY<VARCHAR> patterns)`\n检查字符串 `haystack` 是否与 re2 语法中的正则表达式 `patterns` 相匹配。如果都没有匹配的正则表达式返回 0，否则返回 1。 ### example\n``` mysql> select multi_match_any(\'Hello, World!\', [\'hello\', \'!\', \'world\']); +-----------------------------------------------------------+ | multi_match_any(\'Hello, World!\', [\'hello\', \'!\', \'world\']) |\n+-----------------------------------------------------------+ | 1 | +-----------------------------------------------------------+\nmysql> select multi_match_any(\'abc\', [\'A\', \'bcd\']); +--------------------------------------+ | multi_match_any(\'abc\', [\'A\', \'bcd\']) | +--------------------------------------+ | 0 |\n+--------------------------------------+\n``` ### keywords MULTI_MATCH,MATCH,ANY --- { ""title"": ""磁盘空间管理"", ""language"": ""zh-CN"" } --- <!--split--> # 磁盘空间管理 本文档主要介绍和磁盘存储空间有关的系统参数和处理策略。 Doris 的数据磁盘空间如果不加以控制，会因磁盘写满而导致进程挂掉。因此我们监测磁盘的使用率和剩余空间，通过设置不同的警戒水位，来控制 Doris 系统中的各项操作，尽量避免发生磁盘被写满的情况。']",信息缺失关于Doris是否支持热挂载磁盘的具体信息。,7.5,,ERROR,"['Doris 的数据磁盘空间如果不加以控制，会因磁盘写满而导致进程挂掉。因此我们监测磁盘的使用率和剩余空间，通过设置不同的警戒水位，来控制 Doris 系统中的各项操作，尽量避免发生磁盘被写满的情况。\n\n## 名词解释\n\n- Data Dir：数据目录，在 BE 配置文件 `be.conf` 的 `storage_root_path` 中指定的各个数据目录。通常一个数据目录对应一个磁盘、因此下文中 **磁盘** 也指代一个数据目录。\n\n## 基本原理\n\nBE 定期（每隔一分钟）会向 FE 汇报一次磁盘使用情况。FE 记录这些统计值，并根据这些统计值，限制不同的操作请求。\n\n在 FE 中分别设置了 **高水位（High Watermark）** 和 **危险水位（Flood Stage）** 两级阈值。危险水位高于高水位。当磁盘使用率高于高水位时，Doris 会限制某些操作的执行（如副本均衡等）。而如果高于危险水位，则会禁止某些操作的执行（如导入）。\n\n同时，在 BE 上也设置了 **危险水位（Flood Stage）**。考虑到 FE 并不能完全及时的检测到 BE 上的磁盘使用情况，以及无法控制某些 BE 自身运行的操作（如 Compaction）。因此 BE 上的危险水位用于 BE 主动拒绝和停止某些操作，达到自我保护的目的。\n\n## FE 参数\n\n**高水位：**\n\n```text\nstorage_high_watermark_usage_percent 默认 85 (85%)。\nstorage_min_left_capacity_bytes 默认 2GB。\n```\n\n当磁盘空间使用率**大于** `storage_high_watermark_usage_percent`，**或者** 磁盘空间剩余大小**小于** `storage_min_left_capacity_bytes` 时，该磁盘不会再被作为以下操作的目的路径：\n\n- Tablet 均衡操作（Balance）\n- Colocation 表数据分片的重分布（Relocation）\n- Decommission\n\n**危险水位：**\n\n```text\nstorage_flood_stage_usage_percent 默认 95 (95%)。\nstorage_flood_stage_left_capacity_bytes 默认 1GB。\n```', '建议以挂载本地 Doris 源码目录的方式运行镜像，这样编译的产出二进制文件会存储在宿主机中，不会因为镜像退出而消失。\n\n   同时，建议同时将镜像中 maven 的 `.m2` 目录挂载到宿主机目录，以防止每次启动镜像编译时，重复下载 maven 的依赖库。\n   \n   此外，运行镜像编译时需要 download 部分文件，可以采用 host 模式启动镜像。 host 模式不需要加 -p 进行端口映射，因为和宿主机共享网络IP和端口。\n   \n   docker run 部分参数说明如下：\n   \n    | 参数 | 注释 |\n    |---|---|\n    | -v | 给容器挂载存储卷，挂载到容器的某个目录 |\n    | --name | 指定容器名字，后续可以通过名字进行容器管理 |\n    | --network | 容器网络设置: bridge 使用 docker daemon 指定的网桥，host 容器使用主机的网络， container:NAME_or_ID 使用其他容器的网路，共享IP和PORT等网络资源， none 容器使用自己的网络（类似--net=bridge），但是不进行配置 |\n    \n    如下示例，是指将容器的 /root/doris-DORIS-x.x.x-release 挂载至宿主机 /your/local/doris-DORIS-x.x.x-release 目录，且命名 mydocker 后用 host 模式 启动镜像：\n\n    ```\n    $ docker run -it --network=host --name mydocker -v /your/local/.m2:/root/.m2 -v /your/local/doris-DORIS-x.x.x-release/:/root/doris-DORIS-x.x.x-release/ apache/doris:build-env-ldb-toolchain-latest\n    ```\n\n3. 下载源码\n\n   启动镜像后，你应该已经处于容器内。可以通过以下命令下载 Doris 源码（已挂载本地源码目录则不用）：\n\n    ```\n    $ git clone https://github.com/apache/doris.git\n    ```\n\n4. 编译 Doris', '### Q7. 关于数据目录SSD和HDD的配置, 建表有时候会遇到报错`Failed to find enough host with storage medium and tag`\n\nDoris支持一个BE节点配置多个存储路径。通常情况下，每块盘配置一个存储路径即可。同时，Doris支持指定路径的存储介质属性，如SSD或HDD。SSD代表高速存储设备，HDD代表低速存储设备。\n\n如果集群只有一种介质比如都是HDD或者都是SSD，最佳实践是不用在be.conf中显式指定介质属性。如果遇到上述报错```Failed to find enough host with storage medium and tag```，一般是因为be.conf中只配置了SSD的介质，而建表阶段中显式指定了```properties {""storage_medium"" = ""hdd""}```；同理如果be.conf只配置了HDD的介质，而而建表阶段中显式指定了```properties {""storage_medium"" = ""ssd""}```也会出现上述错误。解决方案可以修改建表的properties参数与配置匹配；或者将be.conf中SSD/HDD的显式配置去掉即可。\n\n通过指定路径的存储介质属性，我们可以利用Doris的冷热数据分区存储功能，在分区级别将热数据存储在SSD中，而冷数据会自动转移到HDD中。\n\n需要注意的是，Doris并不会自动感知存储路径所在磁盘的实际存储介质类型。这个类型需要用户在路径配置中显式的表示。比如路径 ""/path/to/data1.SSD"" 即表示这个路径是SSD存储介质。而 ""data1.SSD"" 就是实际的目录名称。Doris是根据目录名称后面的 "".SSD"" 后缀来确定存储介质类型的，而不是实际的存储介质类型。也就是说，用户可以指定任意路径为SSD存储介质，而Doris仅识别目录后缀，不会去判断存储介质是否匹配。如果不写后缀，则默认为HDD。\n\n换句话说，"".HDD"" 和 "".SSD"" 只是用于标识存储目录“相对”的“低速”和“高速”之分，而并不是标识实际的存储介质类型。所以如果BE节点上的存储路径没有介质区别，则无需填写后缀。\n\n### Q8. 多个FE，在使用Nginx实现web UI负载均衡时，无法登录', '![](/images/release-note-2.0beta-ssb-parquet.png)\n![](/images/release-note-2.0beta-ssb-orc.png)\n\n- 支持湖上数据的本地文件缓存。可以利用本地磁盘缓存 HDFS 或对象存储等远端存储系统上的数据，通过缓存加速访问相同数据的查询。在命中本地文件缓存的情况下，通过 Apache Doris 查询湖上数据的性能可与 Apache Doris 内部表持平，该功能可以极大提升湖上热数据的查询性能。参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache](https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache)\n\n- 支持外表的统计信息收集。和 Apache Doris 内表一样，用户可以通过 Analyze 语句分析并收集指定外表的统计信息，结合 Nereids 全新查询优化器，能够更准确更智能地对复杂 SQL 进行查询计划的调优。以 TPC-H 标准测试数据集为例，无需手动改写 SQL 即可获得最优的查询计划并获得更好的性能表现。 参考文档：[https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/](https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/)\n\n- 优化了 JDBC Catalog 的数据写回性能。通过 PrepareStmt 和批量方式，用户通过 INSERT INTO 命令、通过 JDBC Catalog 将数据写回到 MySQL、Oracle 等关系型数据库的性能提升数十倍。\n\n![](/images/release-note-2.0beta-jdbc.png)\n\n# 支持 Kubernetes 容器化部署\n\n在过去 Apache Doris 是基于 IP 通信的，在 K8s 环境部署时由于宿主机故障发生 Pod IP 漂移将导致集群不可用，在 2.0 版本中我们支持了 FQDN，使得 Apache Doris 可以在无需人工干预的情况下实现节点自愈，因此可以更好应对 K8s 环境部署以及灵活扩缩容。']","Doris 本身并不直接支持热挂载磁盘的概念。在 Doris 中，如果需要添加或更换磁盘，通常需要通过修改配置文件 `be.conf` 中的 `storage_root_path` 参数来指定新的数据目录，然后重启 Backend (BE) 服务来使配置生效。

但是，Doris 提供了磁盘空间监控机制，可以在一定程度上“模拟”热挂载的效果。具体来说：

1. **高水位和危险水位设置**：通过设置不同级别的阈值（高水位和危险水位），Doris 可以在磁盘使用率达到一定比例时限制写入操作，从而避免磁盘被写满。
   
2. **在线添加磁盘**：虽然不能热挂载，但是在不重启 BE 的情况下，你可以添加新的磁盘到系统，并设置相应的阈值，使得新的磁盘可以开始接收数据。

   - 首先在 `be.conf` 中添加新的存储路径。
   - 然后在 FE 中更新对应的磁盘配置，设置合适的警戒水位。
   - 通知 Doris 集群新的存储路径可用。

   在此过程中，旧的磁盘仍然可以继续服务，而新的磁盘会在下一次平衡操作时被考虑。

3. **平滑迁移**：对于数据的迁移，Doris 支持在线的 Tablet 均衡操作，可以在不中断服务的情况下，将数据从旧的磁盘迁移到新的磁盘。

然而，如果是在运行中的系统中物理地添加或替换磁盘，实际上还是需要运维人员手动进行操作，并且可能需要重启服务来识别新的硬件。

总之，虽然 Doris 没有热挂载磁盘的直接功能，但它的设计允许一定程度的灵活性来管理和扩展存储空间，同时保持服务的连续性。"
8f6f000b-c195-11ee-ac35-b026288862a5,"routine load 作业配置了max_filter_ratio属性，暂停后能将这个属性给删除吗
",可以改成0,"['Doris 自身能够保证不丢不重的订阅 Kafka 中的消息，即 `Exactly-Once` 消费语义。 ## 订阅 Kafka 消息 订阅 Kafka 消息使用了 Doris 中的例行导入（Routine Load）功能。 用户首先需要创建一个**例行导入作业**。作业会通过例行调度，不断地发送一系列的**任务**，每个任务会消费一定数量 Kafka 中的消息。 请注意以下使用限制：\n1. 支持无认证的 Kafka 访问，以及通过 SSL 方式认证的 Kafka 集群。 2. 支持的消息格式如下： - csv 文本格式。每一个 message 为一行，且行尾**不包含**换行符。 - Json 格式，详见 [导入 Json 格式数据](../import-way/load-json-format.md)。 3. 仅支持 Kafka 0.10.0.0(含) 以上版本。 ### 访问 SSL 认证的 Kafka 集群\n例行导入功能支持无认证的 Kafka 集群，以及通过 SSL 认证的 Kafka 集群。\n访问 SSL 认证的 Kafka 集群需要用户提供用于认证 Kafka Broker 公钥的证书文件（ca.pem）。如果 Kafka 集群同时开启了客户端认证，则还需提供客户端的公钥（client.pem）、密钥文件（client.key），以及密钥密码。这里所需的文件需要先通过 `CREAE FILE` 命令上传到 Doris 中，并且 catalog 名称为 `kafka`。`CREATE FILE` 命令的具体帮助可以参见 [CREATE\nFILE](../../../sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-FILE.md) 命令手册。这里给出示例：\n- 上传文件\n```sql CREATE FILE ""ca.pem"" PROPERTIES(""url"" = ""https://example_url/kafka-key/ca.pem"", ""catalog"" = ""kafka""); CREATE FILE ""client.key"" PROPERTIES(""url"" = ""https://example_urlkafka-key/client.key"", ""catalog"" = ""kafka"");\nCREATE FILE ""client.pem"" PROPERTIES(""url"" = ""https://example_url/kafka-key/client.pem"", ""catalog"" = ""kafka"");\n``` 上传完成后，可以通过 [SHOW FILES](../../../sql-manual/sql-reference/Show-Statements/SHOW-FILE.md) 命令查看已上传的文件。 ### 创建例行导入作业\n创建例行导入任务的具体命令，请参阅 [ROUTINE LOAD](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/CREATE-ROUTINE-LOAD.md) 命令手册。这里给出示例： 1. 访问无认证的 Kafka 集群\n```sql CREATE ROUTINE LOAD demo.my_first_routine_load_job ON test_1 COLUMNS TERMINATED BY "","" PROPERTIES ( ""max_batch_interval"" = ""20"", ""max_batch_rows"" = ""300000"", ""max_batch_size"" = ""209715200"", ) FROM KAFKA', '``` 上传完成后，可以通过 [SHOW FILES](../../../sql-manual/sql-reference/Show-Statements/SHOW-FILE.md) 命令查看已上传的文件。 ### 创建例行导入作业\n创建例行导入任务的具体命令，请参阅 [ROUTINE LOAD](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/CREATE-ROUTINE-LOAD.md) 命令手册。这里给出示例： 1. 访问无认证的 Kafka 集群\n```sql CREATE ROUTINE LOAD demo.my_first_routine_load_job ON test_1 COLUMNS TERMINATED BY "","" PROPERTIES ( ""max_batch_interval"" = ""20"", ""max_batch_rows"" = ""300000"", ""max_batch_size"" = ""209715200"", ) FROM KAFKA\n( ""kafka_broker_list"" = ""broker1:9092,broker2:9092,broker3:9092"", ""kafka_topic"" = ""my_topic"", ""property.group.id"" = ""xxx"", ""property.client.id"" = ""xxx"", ""property.kafka_default_offsets"" = ""OFFSET_BEGINNING"" );\n``` - `max_batch_interval/max_batch_rows/max_batch_size` 用于控制一个子任务的运行周期。一个子任务的运行周期由最长运行时间、最多消费行数和最大消费数据量共同决定。 2. 访问 SSL 认证的 Kafka 集群\n```sql CREATE ROUTINE LOAD demo.my_first_routine_load_job ON test_1 COLUMNS TERMINATED BY "","", PROPERTIES ( ""max_batch_interval"" = ""20"", ""max_batch_rows"" = ""300000"", ""max_batch_size"" = ""209715200"", ) FROM KAFKA\n( ""kafka_broker_list""= ""broker1:9091,broker2:9091"", ""kafka_topic"" = ""my_topic"", ""property.security.protocol"" = ""ssl"", ""property.ssl.ca.location"" = ""FILE:ca.pem"", ""property.ssl.certificate.location"" = ""FILE:client.pem"",\n""property.ssl.key.location"" = ""FILE:client.key"", ""property.ssl.key.password"" = ""abcdefg"" );\n``` ### 查看导入作业状态 查看**作业**状态的具体命令和示例请参阅 [SHOW ROUTINE LOAD](../../../sql-manual/sql-reference/Show-Statements/SHOW-ROUTINE-LOAD.md) 命令文档。\n查看某个作业的**任务**运行状态的具体命令和示例请参阅 [SHOW ROUTINE LOAD TASK](../../../sql-manual/sql-reference/Show-Statements/SHOW-ROUTINE-LOAD-TASK.md) 命令文档。 只能查看当前正在运行中的任务，已结束和未开始的任务无法查看。 ### 修改作业属性\n用户可以修改已经创建的作业的部分属性。具体说明请参阅 [ALTER ROUTINE LOAD](../../../sql-manual/sql-reference/Data-Manipulation-Statements/Load/ALTER-ROUTINE-LOAD.md) 命令手册。 ### 作业控制 用户可以通过 `STOP/PAUSE/RESUME` 三个命令来控制作业的停止，暂停和重启。', '| `doris_be_network_send_packets` | | Num | 各个网卡的发送包个数累计。采集自 `/proc/net/dev` | | |\n| `doris_be_proc` | `{mode=""ctxt_switch""}` | Num | CPU 上下文切换的累计值。采集自 `/proc/stat` | 观测是否有异常的上下文切换 | P0 |\n| `doris_be_proc ` | `{mode=""interrupt""}` | Num | CPU 中断次数的累计值。采集自 `/proc/stat` | | |\n| `doris_be_proc` | `{mode=""procs_blocked""}` | Num | 系统当前被阻塞的进程数（如等待IO）。采集自 `/proc/stat` | | |\n| `doris_be_proc` | `{mode=""procs_running""}` | Num | 系统当前正在执行的进程数。采集自 `/proc/stat` | | |\n| `doris_be_snmp_tcp_in_errs` | | Num | tcp包接收错误的次数。采集自 `/proc/net/snmp` | 可观测网络错误如重传、丢包等。需和其他 snmp 指标配合使用 | P0 |\n| `doris_be_snmp_tcp_in_segs` | | Num | tcp包发送的个数。 采集自 `/proc/net/snmp` | | |\n| `doris_be_snmp_tcp_out_segs` | | Num | tcp包发送的个数。采集自 `/proc/net/snmp` | | |\n| `doris_be_snmp_tcp_retrans_segs` | | Num | tcp包重传的个数。采集自 `/proc/net/snmp` | | | --- {\n""title"": ""PAUSE-ROUTINE-LOAD"", ""language"": ""zh-CN"" } ---\n<!--split--> ## PAUSE-ROUTINE-LOAD ### Name PAUSE ROUTINE LOAD ### Description 用于暂停一个 Routine Load 作业。被暂停的作业可以通过 RESUME 命令重新运行。 ```sql PAUSE [ALL] ROUTINE LOAD FOR job_name']","是的，可以通过 `ALTER ROUTINE LOAD` 命令修改已经创建的例行导入作业的属性，包括删除已配置的属性。如果你想删除 `max_filter_ratio` 属性，可以通过指定新的属性集合（不包含 `max_filter_ratio`）来实现。具体的命令格式如下：

```sql
ALTER ROUTINE LOAD job_name ON database.table PROPERTIES ( /* 新的属性集合 */ );
```

注意，这里的 /* 新的属性集合 */ 应该是你想保留或新增的属性列表，如果不想保留 `max_filter_ratio`，则不应包含此属性。执行此命令后，原有的 `max_filter_ratio` 属性将被删除。

information is missing on 是否需要在作业暂停状态下执行此操作。",14.01,,ERROR,"['7. STOP和PAUSE的区别\n\n   FE会自动定期清理STOP状态的ROUTINE LOAD，而PAUSE状态的则可以再次被恢复启用。\n\n## 相关参数\n\n一些系统配置参数会影响例行导入的使用。\n\n1. max_routine_load_task_concurrent_num\n\n   FE 配置项，默认为 5，可以运行时修改。该参数限制了一个例行导入作业最大的子任务并发数。建议维持默认值。设置过大，可能导致同时并发的任务数过多，占用集群资源。\n\n2. max_routine_load_task_num_per_be\n\n   FE 配置项，默认为5，可以运行时修改。该参数限制了每个 BE 节点最多并发执行的子任务个数。建议维持默认值。如果设置过大，可能导致并发任务数过多，占用集群资源。\n\n3. max_routine_load_job_num\n\n   FE 配置项，默认为100，可以运行时修改。该参数限制的例行导入作业的总数，包括 NEED_SCHEDULED, RUNNING, PAUSE 这些状态。超过后，不能在提交新的作业。\n\n4. max_consumer_num_per_group\n\n   BE 配置项，默认为 3。该参数表示一个子任务中最多生成几个 consumer 进行数据消费。对于 Kafka 数据源，一个 consumer 可能消费一个或多个 kafka partition。假设一个任务需要消费 6 个 kafka partition，则会生成 3 个 consumer，每个 consumer 消费 2 个 partition。如果只有 2 个 partition，则只会生成 2 个 consumer，每个 consumer 消费 1 个 partition。\n\n5. max_tolerable_backend_down_num FE 配置项，默认值是0。在满足某些条件下，Doris可PAUSED的任务重新调度，即变成RUNNING。该参数为0代表只有所有BE节点是alive状态才允许重新调度。\n\n6. period_of_auto_resume_min FE 配置项，默认是5分钟。Doris重新调度，只会在5分钟这个周期内，最多尝试3次. 如果3次都失败则锁定当前任务，后续不在进行调度。但可通过人为干预，进行手动恢复。\n\n## 更多帮助', '<!--split-->\n\n## PAUSE-ROUTINE-LOAD\n\n### Name\n\nPAUSE ROUTINE LOAD \n\n### Description\n\n用于暂停一个 Routine Load 作业。被暂停的作业可以通过 RESUME 命令重新运行。\n\n```sql\nPAUSE [ALL] ROUTINE LOAD FOR job_name\n```\n\n### Example\n\n1. 暂停名称为 test1 的例行导入作业。\n\n   ```sql\n   PAUSE ROUTINE LOAD FOR test1;\n   ```\n\n2. 暂停所有例行导入作业。\n\n   ```sql\n   PAUSE ALL ROUTINE LOAD;\n   ```\n\n### Keywords\n\n    PAUSE, ROUTINE, LOAD\n\n### Best Practice\n\n---\n{\n    ""title"": ""MULTI-LOAD"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## MULTI-LOAD\n\n### Name\n\nMULTI LOAD\n\n### Description\n\n用户通过 HTTP 协议提交多个导入作业。Multi Load 可以保证多个导入作业的原子生效', '采样窗口为 `max_batch_rows * 10`。即如果在采样窗口内，错误行数/总行数大于 `max_filter_ratio`，则会导致例行作业被暂停，需要人工介入检查数据质量问题。\n\n      被 where 条件过滤掉的行不算错误行。\n\n  14. `enclose`\n      When the csv data field contains row delimiters or column delimiters, to prevent accidental truncation, single-byte characters can be specified as brackets for protection. For example, the column separator is "","", the bracket is ""\'"", and the data is ""a,\'b,c\'"", then ""b,c"" will be parsed as a field.\n\n  15. `escape`\n      转义符。用于转义在csv字段中出现的与包围符相同的字符。例如数据为""a,\'b,\'c\'""，包围符为""\'""，希望""b,\'c被作为一个字段解析，则需要指定单字节转义符，例如""\\""，然后将数据修改为""a,\'b,\\\'c\'""。\n\n- `FROM data_source [data_source_properties]`\n\n  数据源的类型。当前支持：\n\n  ```text\n  FROM KAFKA\n  (\n      ""key1"" = ""val1"",\n      ""key2"" = ""val2""\n  )\n  ```\n\n  `data_source_properties` 支持如下数据源属性：\n\n  1. `kafka_broker_list`\n\n     Kafka 的 broker 连接信息。格式为 ip:host。多个broker之间以逗号分隔。\n\n     `""kafka_broker_list"" = ""broker1:9092,broker2:9092""`\n\n  2. `kafka_topic`\n\n     指定要订阅的 Kafka 的 topic。\n\n     `""kafka_topic"" = ""my_topic""`']",当您暂停了一个例行加载（Routine Load）作业并想要删除 max_filter_ratio 属性时，通常情况下这是可能的。然而，具体的实现可能会因数据库管理系统或工具的不同而有所差异。您应该查阅您的数据库管理系统或工具的文档，以了解如何正确地编辑或删除例行加载作业的属性。通常，您可以使用一个更新或编辑作业配置的命令来进行这样的操作。如果您不确定如何操作，您也可以考虑联系相关的技术支持人员寻求帮助。
80191615-c187-11ee-81a9-b026288862a5,使用delete删除数据会有什么影响吗？ ,删除数据不会立即释放磁盘空间，但Doris会自动进行compaction来清除垃圾数据，无需手动处理。,"['``` ### example ```sql select x, y, row_number() over(partition by x order by y) as rank from int_t;\n| x | y | rank | |---|------|----------| | 1 | 1 | 1 | | 1 | 2 | 2 | | 1 | 2 | 3 | | 2 | 1 | 1 | | 2 | 2 | 2 | | 2 | 3 | 3 | | 3 | 1 | 1 | | 3 | 1 | 2 |\n| 3 | 2 | 3 |\n``` ### keywords WINDOW,FUNCTION,ROW_NUMBER --- { ""title"": ""WINDOW_FUNCTION_NTILE"", ""language"": ""zh-CN"" } --- <!--split--> ## WINDOW FUNCTION NTILE ### description\n对于NTILE(n), 该函数会将排序分区中的所有行按顺序分配到n个桶中(编号较小的桶满了之后才能分配编号较大的桶)。对于每一行, NTILE()函数会返回该行数据所在的桶的编号(从1到n)。对于不能平均分配的情况, 优先分配到编号较小的桶中。所有桶中的行数相差不能超过1。目前n只能是正整数。 ```sql NTILE(n) OVER(partition_by_clause order_by_clause)\n``` ### example ```sql select x, y, ntile(2) over(partition by x order by y) as rank from int_t;\n| x | y | rank | |---|------|----------| | 1 | 1 | 1 | | 1 | 2 | 1 | | 1 | 2 | 2 | | 2 | 1 | 1 | | 2 | 2 | 1 | | 2 | 3 | 2 | | 3 | 1 | 1 | | 3 | 1 | 1 |\n| 3 | 2 | 2 |\n``` ### keywords WINDOW,FUNCTION,NTILE --- { ""title"": ""批量删除"", ""language"": ""zh-CN"" } --- <!--split--> # 批量删除\n目前Doris 支持 [Broker Load](../import/import-way/broker-load-manual.md)，[Routine Load](../import/import-way/routine-load-manual)， [Stream Load](../import/import-way/stream-load-manual) 等多种导入方式，对于数据的删除目前只能通过delete语句进行删除，使用delete 语句的方式删除时，每执行一次delete\n都会生成一个新的数据版本，如果频繁删除会严重影响查询性能，并且在使用delete方式删除时，是通过生成一个空的rowset来记录删除条件实现，每次读取都要对删除条件进行过滤，同样在条件较多时会对性能造成影响。对比其他的系统，greenplum 的实现方式更像是传统数据库产品，snowflake 通过merge 语法实现。', '| | k7 | DOUBLE | Yes | false | N/A | NONE | +----------------+-------+----------+------+-------+---------+-------+\n``` 2. 删除表 all_type_table 中一个不存在的物化视图 ```sql drop materialized view k1_k2 on all_type_table; ERROR 1064 (HY000): errCode = 2, detailMessage = Materialized view [k1_k2] does not exist in table [all_type_table]\n``` 删除请求直接报错 3. 删除表 all_type_table 中的物化视图 k1_k2，不存在不报错。 ```sql drop materialized view if exists k1_k2 on all_type_table; Query OK, 0 rows affected (0.00 sec)\n``` 存在则删除，不存在则不报错。 ### Keywords DROP, MATERIALIZED, VIEW ### Best Practice --- { ""title"": ""DROP-POLICY"", ""language"": ""zh-CN"" } --- <!--split--> ## DROP-POLICY ### Name DROP POLICY ### Description 删除安全策略 #### 行安全策略 语法：\n1. 删除行安全策略 ```sql DROP ROW POLICY test_row_policy_1 on table1 [FOR user| ROLE role];\n``` 2. 删除存储策略 ```sql DROP STORAGE POLICY policy_name1 ``` ### Example 1. 删除 table1 的 test_row_policy_1 ```sql DROP ROW POLICY test_row_policy_1 on table1\n``` 2. 删除 table1 作用于 test 的 test_row_policy_1 行安全策略 ```sql DROP ROW POLICY test_row_policy_1 on table1 for test ``` 3. 删除 table1 作用于 role1 的 test_row_policy_1 行安全策略 ```sql DROP ROW POLICY test_row_policy_1 on table1 for role role1\n``` 4. 删除名字为policy_name1的存储策略 ```sql DROP STORAGE POLICY policy_name1\n``` ### Keywords DROP, POLICY ### Best Practice --- { ""title"": ""TRUNCATE-TABLE"", ""language"": ""zh-CN"" } --- <!--split--> ## TRUNCATE-TABLE ### Name TRUNCATE TABLE ### Description 该语句用于清空指定表和分区的数据 语法：\n```sql TRUNCATE TABLE [db.]tbl[ PARTITION(p1, p2, ...)];\n``` 说明： - 该语句清空数据，但保留表或分区。 - 不同于 DELETE，该语句只能整体清空指定的表或分区，不能添加过滤条件。 - 不同于 DELETE，使用该方式清空数据不会对查询性能造成影响。 - 该操作删除的数据不可恢复。 - 使用该命令时，表状态需为 NORMAL，即不允许正在进行 SCHEMA CHANGE 等操作。 - 该命令可能会导致正在进行的导入失败。 ### Example 1. 清空 example_db 下的表 tbl', ""delete操作的语法详见官网 [DELETE](../../sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/DELETE.md) 语法。 ## 返回结果 Delete命令是一个SQL命令，返回结果是同步的，分为以下几种： 1. 执行成功 如果Delete顺利执行完成并可见，将返回下列结果，`Query OK`表示成功\n```sql mysql> delete from test_tbl PARTITION p1 where k1 = 1; Query OK, 0 rows affected (0.04 sec) {'label':'delete_e7830c72-eb14-4cb9-bbb6-eebd4511d251', 'status':'VISIBLE', 'txnId':'4005'}\n``` 2. 提交成功，但未可见 Doris的事务提交分为两步：提交和发布版本，只有完成了发布版本步骤，结果才对用户是可见的。若已经提交成功了，那么就可以认为最终一定会发布成功，Doris会尝试在提交完后等待发布一段时间，如果超时后即使发布版本还未完成也会优先返回给用户，提示用户提交已经完成。若如果Delete已经提交并执行，但是仍未发布版本和可见，将返回下列结果\n```sql mysql> delete from test_tbl PARTITION p1 where k1 = 1; Query OK, 0 rows affected (0.04 sec)\n{'label':'delete_e7830c72-eb14-4cb9-bbb6-eebd4511d251', 'status':'COMMITTED', 'txnId':'4005', 'err':'delete job is committed but may be taking effect later' }\n``` 结果会同时返回一个json字符串： `affected rows`：表示此次删除影响的行，由于Doris的删除目前是逻辑删除，因此对于这个值是恒为0； `label`：自动生成的 label，是该导入作业的标识。每个导入作业，都有一个在单 database 内部唯一的 Label； `status`：表示数据删除是否可见，如果可见则显示`VISIBLE`，如果不可见则显示`COMMITTED`； `txnId`：这个Delete job对应的事务id；\n`err`：字段会显示一些本次删除的详细信息。 3. 提交失败，事务取消 如果Delete语句没有提交成功，将会被Doris自动中止，返回下列结果 ```sql mysql> delete from test_tbl partition p1 where k1 > 80; ERROR 1064 (HY000): errCode = 2, detailMessage = {错误原因}\n``` 示例： 比如说一个超时的删除，将会返回timeout时间和未完成的`(tablet=replica)`\n```sql mysql> delete from test_tbl partition p1 where k1 > 80; ERROR 1064 (HY000): errCode = 2, detailMessage = failed to delete replicas from job: 4005, Unfinished replicas:10000=60000, 10001=60000, 10002=60000\n``` **综上，对于Delete操作返回结果的正确处理逻辑为：** 1. 如果返回结果为`ERROR 1064 (HY000)`，则表示删除失败； 2. 如果返回结果为`Query OK`，则表示删除执行成功； - 如果`status`为`COMMITTED`，表示数据仍不可见，用户可以稍等一段时间再用`show delete`命令查看结果； - 如果`status`为`VISIBLE`，表示数据删除成功。 ## Delete操作相关FE配置\n**TIMEOUT配置** 总体来说，Doris的删除作业的超时时间限制在30秒到5分钟时间内，具体时间可通过下面配置项调整 - `tablet_delete_timeout_second` delete自身的超时时间是可受指定分区下tablet的数量弹性改变的，此项配置为平均一个tablet所贡献的timeout时间，默认值为2。 假设此次删除所指定分区下有5个tablet，那么可提供给delete的timeout时间为10秒，由于低于最低超时时间30秒，因此最终超时时间为30秒。""]","使用delete语句删除数据在Doris系统中会有以下影响：

1. **生成新的数据版本**：每执行一次delete操作，都会生成一个新的数据版本。如果频繁进行delete操作，会导致数据版本数量增加，这可能会影响查询性能，因为系统需要处理更多的数据版本。

2. **影响查询性能**：在使用delete方式删除数据时，是通过生成一个空的rowset来记录删除条件实现的。每次读取数据时都需要对这些删除条件进行过滤，如果删除条件较多，这会对查询性能造成影响。

3. **逻辑删除**：Doris中的删除操作实际上是逻辑删除，不会立即从物理存储中移除数据。这意味着，虽然数据对用户查询不可见，但它们仍然占用存储空间，直到进行压缩或合并操作时才可能被物理删除。

4. **超时和失败处理**：Doris的删除作业有超时时间限制（默认30秒到5分钟），如果删除操作超时或因其他原因失败，系统会自动中止该操作。这需要用户根据错误信息进行相应的处理。

5. **事务处理**：Doris的删除操作支持事务，这意味着删除操作的结果要么全部应用，要么完全不应用。Doris会在删除操作提交后尝试等待一段时间直到数据版本发布，确保数据的一致性。如果提交成功但数据版本未立即发布，操作状态会标记为`COMMITTED`，提示用户删除操作已提交但可能稍后才生效。

因此，在使用delete删除数据时，需要考虑到这些可能的影响，并根据实际情况进行优化，比如减少频繁的删除操作，合理安排删除操作的执行时间，以减少对查询性能的影响。",34.11,,ERROR,"['### example\n\n```sql\nselect x, y, ntile(2) over(partition by x order by y) as rank from int_t;\n\n| x | y    | rank     |\n|---|------|----------|\n| 1 | 1    | 1        |\n| 1 | 2    | 1        |\n| 1 | 2    | 2        |\n| 2 | 1    | 1        |\n| 2 | 2    | 1        |\n| 2 | 3    | 2        |\n| 3 | 1    | 1        |\n| 3 | 1    | 1        |\n| 3 | 2    | 2        |\n```\n\n### keywords\n\n    WINDOW,FUNCTION,NTILE\n---\n{\n    ""title"": ""批量删除"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 批量删除\n\n目前Doris 支持 [Broker Load](../import/import-way/broker-load-manual.md)，[Routine Load](../import/import-way/routine-load-manual)， [Stream Load](../import/import-way/stream-load-manual) 等多种导入方式，对于数据的删除目前只能通过delete语句进行删除，使用delete 语句的方式删除时，每执行一次delete 都会生成一个新的数据版本，如果频繁删除会严重影响查询性能，并且在使用delete方式删除时，是通过生成一个空的rowset来记录删除条件实现，每次读取都要对删除条件进行过滤，同样在条件较多时会对性能造成影响。对比其他的系统，greenplum 的实现方式更像是传统数据库产品，snowflake 通过merge 语法实现。', '2. 删除 table1 作用于 test 的 test_row_policy_1 行安全策略\n\n   ```sql\n   DROP ROW POLICY test_row_policy_1 on table1 for test\n   ```\n\n3. 删除 table1 作用于 role1 的 test_row_policy_1 行安全策略\n\n   ```sql\n   DROP ROW POLICY test_row_policy_1 on table1 for role role1\n   ```\n\n4. 删除名字为policy_name1的存储策略\n```sql\nDROP STORAGE POLICY policy_name1\n```\n\n### Keywords\n\n    DROP, POLICY\n\n### Best Practice\n\n---\n{\n    ""title"": ""TRUNCATE-TABLE"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n## TRUNCATE-TABLE\n\n### Name\n\nTRUNCATE TABLE\n\n### Description\n\n该语句用于清空指定表和分区的数据\n语法：\n\n```sql\nTRUNCATE TABLE [db.]tbl[ PARTITION(p1, p2, ...)];\n```\n\n说明：\n\n- 该语句清空数据，但保留表或分区。\n- 不同于 DELETE，该语句只能整体清空指定的表或分区，不能添加过滤条件。\n- 不同于 DELETE，使用该方式清空数据不会对查询性能造成影响。\n- 该操作删除的数据不可恢复。\n- 使用该命令时，表状态需为 NORMAL，即不允许正在进行 SCHEMA CHANGE 等操作。\n- 该命令可能会导致正在进行的导入失败。\n\n### Example\n\n1. 清空 example_db 下的表 tbl\n\n    ```sql\n    TRUNCATE TABLE example_db.tbl;\n    ```\n\n2. 清空表 tbl 的 p1 和 p2 分区\n\n    ```sql\n    TRUNCATE TABLE tbl PARTITION(p1, p2);\n    ```\n\n### Keywords\n\n    TRUNCATE, TABLE']",使用DELETE语句删除数据时，每执行一次DELETE都会生成一个新的数据版本。如果频繁删除，可能会严重影响查询性能。此外，在使用DELETE方式删除时，是通过生成一个空的rowset来记录删除条件实现的，每次读取都需要对删除条件进行过滤。因此，当条件较多时，也可能会对性能造成影响。
865d8c3c-c178-11ee-a777-b026288862a5,"Doris的BE挂的时候如果有数据通过streamload方式入库，这些数据丢失了，Doris有没有机制还原的？
",1台BE挂了不会丢失数据，会自动同步成多副本。如果数据确实丢失了，可以通过人为捕获异常处理待恢复重试。另外，也可以考虑使用Doris的tablet恢复功能来支持这个场景。,"['1. 通过`show roles;`查看预期的角色在doris中是否存在，如果不存在，需要通过` CREATE ROLE rol_name;`创建角色。 2. 检查预期的group是否在`ldap_group_basedn`对应的组织结构下。 3. 检查预期group是否包含member属性。 4. 检查预期group的member属性是否包含当前用户。 --- { ""title"": ""数据删除恢复"", ""language"": ""zh-CN"" } ---\n<!--split--> # 数据删除恢复 数据删除恢复包含两种情况： 1. 用户执行命令`drop database/table/partition`之后，再使用命令`recover`来恢复整个数据库/表/分区的所有数据。这种修复将会把FE上的数据库/表/分区的结构，从catalog回收站里恢复过来，把它们从不可见状态，重新变回可见，并且原来的数据也恢复可见； 2. 用户因为某些误操作或者线上bug，导致BE上部分tablet被删除，通过运维工具把这些tablet从BE回收站中抢救回来。\n上面两个，前者针对的是数据库/表/分区在FE上已经不可见，且数据库/表/分区的元数据尚保留在FE的catalog回收站里。而后者针对的是数据库/表/分区在FE上可见，但部分BE tablet数据被删除。 下面分别阐述这两种恢复。 ## Drop 恢复\nDoris为了避免误操作造成的灾难，支持对误删除的数据库/表/分区进行数据恢复，在drop table或者 drop database 或者 drop partition之后，Doris不会立刻对数据进行物理删除，而是在FE的catalog回收站中保留一段时间（默认1天，可通过fe.conf中`catalog_trash_expire_second`参数配置），管理员可以通过RECOVER命令对误删除的数据进行恢复。\n**注意，如果是使用`drop force`进行删除的，则是直接删除，无法再恢复。** ### 查看可恢复数据 查看FE上哪些数据可恢复 ```sql SHOW CATALOG RECYCLE BIN [ WHERE NAME [ = ""name"" | LIKE ""name_matcher""] ]\n``` 这里name可以是数据库/表/分区名。 关于该命令使用的更多详细语法及最佳实践，请参阅 [SHOW-CATALOG-RECYCLE-BIN](../../sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN.md) 命令手册，你也可以在 MySql 客户端命令行下输入 `help SHOW CATALOG RECYCLE BIN ` 获取更多帮助信息。 ### 开始数据恢复\n1.恢复名为 example_db 的 database ```sql RECOVER DATABASE example_db;\n``` 2.恢复名为 example_tbl 的 table ```sql RECOVER TABLE example_db.example_tbl; ``` 3.恢复表 example_tbl 中名为 p1 的 partition ```sql RECOVER PARTITION p1 FROM example_tbl;\n``` 执行`RECOVER`命令之后，原来的数据将恢复可见。 关于 RECOVER 使用的更多详细语法及最佳实践，请参阅 [RECOVER](../../sql-manual/sql-reference/Database-Administration-Statements/RECOVER.md) 命令手册，你也可以在 MySql 客户端命令行下输入 `help RECOVER` 获取更多帮助信息。 ## BE tablet 数据恢复 ### 从 BE 回收站中恢复数据\n用户在使用Doris的过程中，可能会发生因为一些误操作或者线上bug，导致一些有效的tablet被删除（包括元数据和数据）。 为了防止在这些异常情况出现数据丢失，Doris提供了回收站机制，来保护用户数据。\n用户删除的tablet数据在BE端不会被直接删除，会被放在回收站中存储一段时间，在一段时间之后会有定时清理机制将过期的数据删除。默认情况下，在磁盘空间占用不超过81%（BE配置`config.storage_flood_stage_usage_percent` * 0.9 * 100%）时，BE回收站中的数据最长保留3天（见BE配置`config.trash_file_expire_time_sec`）。', '```sql CREATE TABLE bitmap_sink ( dt int, page string, user_id int ) WITH ( \'connector\' = \'doris\', \'fenodes\' = \'127.0.0.1:8030\', \'table.identifier\' = \'test.bitmap_test\', \'username\' = \'root\', \'password\' = \'\',\n\'sink.label-prefix\' = \'doris_label\', \'sink.properties.columns\' = \'dt,page,user_id,user_id=to_bitmap(user_id)\' )\n``` 4. **errCode = 2, detailMessage = Label [label_0_1] has already been used, relate to txn [19650]** Exactly-Once场景下，Flink Job重启时必须从最新的Checkpoint/Savepoint启动，否则会报如上错误。 不要求Exactly-Once时，也可通过关闭2PC提交（sink.enable-2pc=false） 或更换不同的sink.label-prefix解决。\n5. **errCode = 2, detailMessage = transaction [19650] not found** 发生在Commit阶段，checkpoint里面记录的事务ID，在FE侧已经过期，此时再次commit就会出现上述错误。 此时无法从checkpoint启动，后续可通过修改fe.conf的streaming_label_keep_max_second配置来延长过期时间，默认12小时。\n6. **errCode = 2, detailMessage = current running txns on db 10006 is 100, larger than limit 100**\n这是因为同一个库并发导入超过了100，可通过调整 fe.conf的参数 `max_running_txn_num_per_db` 来解决，具体可参考 [max_running_txn_num_per_db](https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/fe-config/#max_running_txn_num_per_db)。\n同时，一个任务频繁修改label重启，也可能会导致这个错误。2pc场景下(Duplicate/Aggregate模型)，每个任务的label需要唯一，并且从checkpoint重启时，flink任务才会主动abort掉之前已经precommit成功，没有commit的txn，频繁修改label重启，会导致大量precommit成功的txn无法被abort，占用事务。在Unique模型下也可关闭2pc，可以实现幂等写入。 7. **Flink写入Uniq模型时，如何保证一批数据的有序性？**\n可以添加sequence列配置来保证，具体可参考 [sequence](https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual) 8. **Flink任务没报错，但是无法同步数据？** Connector1.1.0版本以前，是攒批写入的，写入均是由数据驱动，需要判断上游是否有数据写入。1.1.0之后，依赖Checkpoint，必须开启Checkpoint才能写入。\n9. **tablet writer write failed, tablet_id=190958, txn_id=3505530, err=-235** 通常发生在Connector1.1.0之前，是由于写入频率过快，导致版本过多。可以通过设置sink.batch.size 和 sink.batch.interval参数来降低Streamload的频率。 10. **Flink导入有脏数据，如何跳过？**\nFlink在数据导入时，如果有脏数据，比如字段格式、长度等问题，会导致StreamLoad报错，此时Flink会不断的重试。如果需要跳过，可以通过禁用StreamLoad的严格模式(strict_mode=false,max_filter_ratio=1)或者在Sink算子之前对数据做过滤。\n11. **源表和Doris表应如何对应？** 使用Flink Connector导入数据时，要注意两个方面，第一是源表的列和类型跟flink sql中的列和类型要对应上；第二个是flink sql中的列和类型要跟doris表的列和类型对应上，具体可以参考上面的""Doris 和 Flink 列类型映射关系"" 12. **TApplicationException: get_next failed: out of sequence response: expected 4 but got 3**', ""|\n| doris.sink.properties.format | csv | Stream Load 的数据格式。<br/>共支持3种格式：csv，json，arrow（1.4.0版本开始支持）<br/> [更多参数详情](https://doris.apache.org/zh-CN/docs/dev/data-operate/import/import-way/stream-load-manual) |\n| doris.sink.properties.* | -- | Stream Load 的导入参数。<br/>例如:<br/>指定列分隔符:`'doris.sink.properties.column_separator' = ','`等<br/> [更多参数详情](https://doris.apache.org/zh-CN/docs/dev/data-operate/import/import-way/stream-load-manual) |\n| doris.sink.task.partition.size | -- | Doris写入任务对应的 Partition 个数。Spark RDD 经过过滤等操作，最后写入的 Partition 数可能会比较大，但每个 Partition 对应的记录数比较少，导致写入频率增加和计算资源浪费。<br/>此数值设置越小，可以降低 Doris 写入频率，减少 Doris 合并压力。该参数配合 doris.sink.task.use.repartition 使用。\n|\n| doris.sink.task.use.repartition | false | 是否采用 repartition 方式控制 Doris写入 Partition数。默认值为 false，采用 coalesce 方式控制（注意: 如果在写入之前没有 Spark action 算子，可能会导致整个计算并行度降低）。<br/>如果设置为 true，则采用 repartition 方式（注意: 可设置最后 Partition 数，但会额外增加 shuffle 开销）。\n|\n| doris.sink.batch.interval.ms | 50 | 每个批次sink的间隔时间，单位 ms。\n|\n| doris.sink.enable-2pc | false | 是否开启两阶段提交。开启后将会在作业结束时提交事务，而部分任务失败时会将所有预提交状态的事务会滚。 |\n| doris.sink.auto-redirect | false | 是否重定向 StreamLoad 请求。开启后 StreamLoad 将通过 FE 写入, 不再显式获取 BE 信息。""]","Doris在BE（Backend）挂掉的情况下，对于通过StreamLoad方式入库的数据，如果数据已经被成功处理并持久化到磁盘上，那么这部分数据不会丢失，因为Doris的架构设计中包括了数据的副本机制。即使一个BE节点挂掉，只要数据有副本存在于其他BE节点上，Doris通过副本恢复机制可以保证数据的可用性和一致性。

如果在数据入库过程中，BE挂掉导致数据还未成功持久化到磁盘（即数据处于传输或处理过程中），这部分数据可能会丢失。对于这种情况，Doris本身没有直接的机制来还原这部分丢失的数据。数据恢复主要依赖于外部数据源或者重试机制来重新发送数据。

在实际操作中，可以通过以下几种方式减少数据丢失的风险：

1. **确保高可用配置：** 配置足够的BE副本数，确保即使个别BE节点出现故障，数据仍然可以从其他副本中恢复。
2. **使用FE的事务机制：** Doris的FE（Frontend）负责元数据管理和事务控制，确保数据的一致性。在BE故障的情况下，FE可以协调副本之间的数据恢复。
3. **外部重试机制：** 在数据源侧实现重试逻辑，如果检测到StreamLoad入库失败（例如，因为BE挂掉），可以重新发送数据。

综上所述，虽然Doris在BE挂掉时对于正在处理的数据没有直接的还原机制，但通过系统的高可用设计和外部的数据重试策略，可以大大减少数据丢失的风险。",24.57,,ERROR,"['3. 取消导入任务\n\n   已提交切尚未结束的导入任务可以通过 CANCEL LOAD 命令取消。取消后，已写入的数据也会回滚，不会生效。\n\n4. Label、导入事务、多表原子性\n\n   Doris 中所有导入任务都是原子生效的。并且在同一个导入任务中对多张表的导入也能够保证原子性。同时，Doris 还可以通过 Label 的机制来保证数据导入的不丢不重。具体说明可以参阅 [导入事务和原子性](../../../../data-operate/import/import-scenes/load-atomicity.md) 文档。\n\n5. 列映射、衍生列和过滤\n\n   Doris 可以在导入语句中支持非常丰富的列转换和过滤操作。支持绝大多数内置函数和 UDF。关于如何正确的使用这个功能，可参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。\n\n6. 错误数据过滤\n\n   Doris 的导入任务可以容忍一部分格式错误的数据。容忍率通过 `max_filter_ratio` 设置。默认为0，即表示当有一条错误数据时，整个导入任务将会失败。如果用户希望忽略部分有问题的数据行，可以将次参数设置为 0~1 之间的数值，Doris 会自动跳过哪些数据格式不正确的行。\n\n   关于容忍率的一些计算方式，可以参阅 [列的映射，转换与过滤](../../../../data-operate/import/import-scenes/load-data-convert.md) 文档。\n\n7. 严格模式\n\n   `strict_mode` 属性用于设置导入任务是否运行在严格模式下。该属性会对列映射、转换和过滤的结果产生影响，它同时也将控制部分列更新的行为。关于严格模式的具体说明，可参阅 [严格模式](../../../../data-operate/import/import-scenes/load-strict-mode.md) 文档。\n\n8. 超时时间\n\n   Stream Load 的默认超时时间为 10 分钟。从任务提交开始算起。如果在超时时间内没有完成，则任务会失败。\n\n9. 数据量和任务数限制', '- 修复了函数 from_date_format_str 中微秒数丢失的问题。 [#13446](https://github.com/apache/doris/pull/13446)\n\n- 修复了排序 exprs 的 nullability 属性在使用子 smap 信息进行替换后可能不正确的问题。 [#13328](https://github.com/apache/doris/pull/13328)\n\n- 修复了 case when 有 1000 个条件时出现 Core 的问题。 [#13315](https://github.com/apache/doris/pull/13315)\n\n- 修复了 Stream Load 导入数据时最后一行数据丢失的问题。 [#13066](https://github.com/apache/doris/pull/13066)\n\n- 恢复表或分区的副本数与备份前相同。 [#11942](https://github.com/apache/doris/pull/11942)\n---\n{\n    ""title"": ""Release 1.2.3"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n在 1.2.3 版本中，Doris 团队已经修复了自 1.2.2 版本发布以来超过 200 个问题或性能改进项。同时，1.2.3 版本也作为 1.2.2 的迭代版本，具备更高的稳定性，建议所有用户升级到这个版本。\n\n\n# Improvement\n\n### JDBC Catalog \n\n- 支持通过 JDBC Catalog 连接到另一个 Doris 数据库。\n\n目前 JDBC Catalog 连接 Doris 只支持用 5.x 版本的 JDBC jar 包。如果使用 8.x JDBC jar 包可能会出现列类型无法匹配问题。\n\n参考文档：[https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc/#doris](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc/#doris)\n\n- 支持通过参数 `only_specified_database` 来同步指定的数据库。', '3.恢复表 example_tbl 中名为 p1 的 partition\n\n```sql\nRECOVER PARTITION p1 FROM example_tbl;\n```\n\n执行`RECOVER`命令之后，原来的数据将恢复可见。\n\n关于 RECOVER 使用的更多详细语法及最佳实践，请参阅 [RECOVER](../../sql-manual/sql-reference/Database-Administration-Statements/RECOVER.md) 命令手册，你也可以在 MySql 客户端命令行下输入 `help RECOVER` 获取更多帮助信息。\n\n## BE tablet 数据恢复\n\n### 从 BE 回收站中恢复数据\n\n用户在使用Doris的过程中，可能会发生因为一些误操作或者线上bug，导致一些有效的tablet被删除（包括元数据和数据）。\n\n为了防止在这些异常情况出现数据丢失，Doris提供了回收站机制，来保护用户数据。\n\n用户删除的tablet数据在BE端不会被直接删除，会被放在回收站中存储一段时间，在一段时间之后会有定时清理机制将过期的数据删除。默认情况下，在磁盘空间占用不超过81%（BE配置`config.storage_flood_stage_usage_percent` * 0.9 * 100%）时，BE回收站中的数据最长保留3天（见BE配置`config.trash_file_expire_time_sec`）。\n\nBE回收站中的数据包括：tablet的data文件(.dat)，tablet的索引文件(.idx)和tablet的元数据文件(.hdr)。数据将会存放在如下格式的路径：\n\n```\n/root_path/trash/time_label/tablet_id/schema_hash/\n```\n\n* `root_path`：对应BE节点的某个数据根目录。\n* `trash`：回收站的目录。\n* `time_label`：时间标签，为了回收站中数据目录的唯一性，同时记录数据时间，使用时间标签作为子目录。\n\n当用户发现线上的数据被误删除，需要从回收站中恢复被删除的tablet，需要用到这个tablet数据恢复功能。', '* 注：可以先通过 `ADMIN SHOW FRONTEND CONFIG;` 命令查看当前版本是否支持该参数。\n\n3. 设置完成几分钟后，应该会在 Master FE 日志 `fe.log` 中看到如下日志：\n\n    ```\n    tablet 20000 has only one replica 20001 on backend 10001 and it is lost. create an empty replica to recover it.\n    ```\n\n    该日志表示系统已经创建了一个空白 Tablet 用于填补缺失副本。\n    \n4. 通过查询来判断是否已经修复成功。\n\n5. 全部修复成功后，通过以下命令关闭 `recover_with_empty_tablet` 参数：\n\n    ```\n    ADMIN SET FRONTEND CONFIG (""recover_with_empty_tablet"" = ""false"");\n    ```\n\n---\n{\n    ""title"": ""数据备份恢复"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 数据备份恢复\n\nDoris 支持将当前数据以文件的形式，通过 broker 备份到远端存储系统中。之后可以通过 恢复 命令，从远端存储系统中将数据恢复到任意 Doris 集群。通过这个功能，Doris 可以支持将数据定期的进行快照备份。也可以通过这个功能，在不同集群间进行数据迁移。\n\n该功能需要 Doris 版本 0.8.2+\n\n使用该功能，需要部署对应远端存储的 broker。如 BOS、HDFS 等。可以通过 `SHOW BROKER;` 查看当前部署的 broker。\n\n## 简要原理说明\n\n恢复操作需要指定一个远端仓库中已存在的备份，然后将这个备份的内容恢复到本地集群中。当用户提交 Restore 请求后，系统内部会做如下操作：\n\n1. 在本地创建对应的元数据\n\n   这一步首先会在本地集群中，创建恢复对应的表分区等结构。创建完成后，该表可见，但是不可访问。\n\n2. 本地snapshot\n\n   这一步是将上一步创建的表做一个快照。这其实是一个空快照（因为刚创建的表是没有数据的），其目的主要是在 Backend 上产生对应的快照目录，用于之后接收从远端仓库下载的快照文件。', '## 更多帮助\n\n关于 RESTORE 使用的更多详细语法及最佳实践，请参阅 [RESTORE](../../sql-manual/sql-reference/Data-Definition-Statements/Backup-and-Restore/RESTORE.md) 命令手册，你也可以在 MySql 客户端命令行下输入 `HELP RESTORE` 获取更多帮助信息。\n\n---\n{\n    ""title"": ""数据备份"",\n    ""language"": ""zh-CN""\n}\n---\n\n<!--split-->\n\n# 数据备份\n\nDoris 支持将当前数据以文件的形式，通过 broker 备份到远端存储系统中。之后可以通过 恢复 命令，从远端存储系统中将数据恢复到任意 Doris 集群。通过这个功能，Doris 可以支持将数据定期的进行快照备份。也可以通过这个功能，在不同集群间进行数据迁移。\n\n该功能需要 Doris 版本 0.8.2+\n\n使用该功能，需要部署对应远端存储的 broker。如 BOS、HDFS 等。可以通过 `SHOW BROKER;` 查看当前部署的 broker。\n\n## 简要原理说明\n\n备份操作是将指定表或分区的数据，直接以 Doris 存储的文件的形式，上传到远端仓库中进行存储。当用户提交 Backup 请求后，系统内部会做如下操作：\n\n1. 快照及快照上传\n\n   快照阶段会对指定的表或分区数据文件进行快照。之后，备份都是对快照进行操作。在快照之后，对表进行的更改、导入等操作都不再影响备份的结果。快照只是对当前数据文件产生一个硬链，耗时很少。快照完成后，会开始对这些快照文件进行逐一上传。快照上传由各个 Backend 并发完成。\n\n2. 元数据准备及上传\n\n   数据文件快照上传完成后，Frontend 会首先将对应元数据写成本地文件，然后通过 broker 将本地元数据文件上传到远端仓库。完成最终备份作业\n\n3. 动态分区表说明\n\n   如果该表是动态分区表，备份之后会自动禁用动态分区属性，在做恢复的时候需要手动将该表的动态分区属性启用，命令如下:\n\n   ```sql\n   ALTER TABLE tbl1 SET (""dynamic_partition.enable""=""true"")\n   ```']","Doris有一个回收站机制来保护用户数据。当用户在使用Doris过程中因一些误操作或线上bug导致有效tablet被删除时，这些被删除的tablet数据在BE端不会立即被删除，而是会被放入回收站中存储一段时间。在一段时间之后，会有定时清理机制将过期的数据删除。

回收站中的数据包括tablet的.data文件、.idx文件和.hdr文件，并按照如下格式存放：
```
/root_path/trash/time_label/tablet_id/schema_hash/
```
其中：

- `root_path`: 对应BE节点的某个数据根目录。
- `trash`: 回收站的目录。
- `time_label`: 时间标签，为了回收站中数据目录的唯一性，同时记录数据时间，使用时间标签作为子目录。
- `tablet_id`: 删除的tablet的ID。
- `schema_hash`: 表格的schema哈希值。

如果发生了数据丢失的情况，用户可以从回收站中恢复被删除的tablet。在FE层面执行`RECOVER`命令即可使原来的数据恢复可见。具体的SQL命令示例如下：
```sql
RECOVER TABLE example_tbl;
```

此外，如果仅想恢复特定分区的数据，可以在命令中指定分区名称：
```sql
RECOVER PARTITION p1 FROM example_tbl;
```
请注意，如果在执行`RECOVER`命令之前，您已经使用`DROP DATABASE`或`DROP TABLE`命令删除了相关表，那么这部分数据将无法恢复。所以建议在遇到此类问题时先尝试`RECOVER`命令，然后再考虑是否需要使用其他方式进行数据恢复。"
0e635249-c17a-11ee-be97-b026288862a5,"Doris报错""tcp listen failed, errno=98, errmsg=Address already in use""是什么原因？",这个错误通常是由于端口被占用导致的，可以通过查看端口占用情况并释放端口来解决。,"['启动后会监听两个端口， 默认为6032和6033。6032端口是ProxySQL的管理端口，6033是ProxySQL对外提供服务的端口 (即连接到转发后端的真正数据库的转发端口)。 # netstat -tunlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name\ntcp 0 0 0.0.0.0:6032 0.0.0.0:* LISTEN 23940/proxysql tcp 0 0 0.0.0.0:6033 0.0.0.0:* LISTEN\n``` ### ProxySQL 配置\nProxySQL 有配置文件 `/etc/proxysql.cnf` 和配置数据库文件`/var/lib/proxysql/proxysql.db`。**这里需要特别注意**：如果存在`""proxysql.db""`文件(在`/var/lib/proxysql`目录下)，则 ProxySQL 服务只有在第一次启动时才会去读取`proxysql.cnf文件`并解析；后面启动会就不会读取`proxysql.cnf`文件了！如果想要让proxysql.cnf 文件里的配置在重启 proxysql\n服务后生效(即想要让 proxysql 重启时读取并解析 proxysql.cnf配置文件)，则需要先删除 `/var/lib/proxysql/proxysql.db`数据库文件，然后再重启 proxysql 服务。这样就相当于初始化启动 proxysql 服务了，会再次生产一个纯净的 proxysql.db 数据库文件(如果之前配置了 proxysql 相关路由规则等，则就会被抹掉)', '| `doris_be_memory_pswpin` | | 字节 | 系统从磁盘换入到内存的数量 | 通常情况下，swap应该关闭，因此这个数值应该是0 | |\n| `doris_be_memory_pswpout` | | 字节 | 系统从内存换入到磁盘的数量 | 通常情况下，swap应该关闭，因此这个数值应该是0 | |\n| `doris_be_network_receive_bytes` | | 字节 | 各个网卡的接收字节累计。采集自 `/proc/net/dev` | | |\n| `doris_be_network_receive_packets` | | Num | 各个网卡的接收包个数累计。采集自 `/proc/net/dev` | | |\n| `doris_be_network_send_bytes` | | 字节 | 各个网卡的发送字节累计。采集自 `/proc/net/dev` | | |\n| `doris_be_network_send_packets` | | Num | 各个网卡的发送包个数累计。采集自 `/proc/net/dev` | | |\n| `doris_be_proc` | `{mode=""ctxt_switch""}` | Num | CPU 上下文切换的累计值。采集自 `/proc/stat` | 观测是否有异常的上下文切换 | P0 |\n| `doris_be_proc ` | `{mode=""interrupt""}` | Num | CPU 中断次数的累计值。采集自 `/proc/stat` | | |\n| `doris_be_proc` | `{mode=""procs_blocked""}` | Num | 系统当前被阻塞的进程数（如等待IO）。采集自 `/proc/stat` | | |\n| `doris_be_proc` | `{mode=""procs_running""}` | Num | 系统当前正在执行的进程数。采集自 `/proc/stat` | | |\n| `doris_be_snmp_tcp_in_errs` | | Num | tcp包接收错误的次数。采集自 `/proc/net/snmp` | 可观测网络错误如重传、丢包等。需和其他 snmp 指标配合使用 | P0 |', '| `doris_be_memory_pswpout` | | 字节 | 系统从内存换入到磁盘的数量 | 通常情况下，swap应该关闭，因此这个数值应该是0 | |\n| `doris_be_network_receive_bytes` | | 字节 | 各个网卡的接收字节累计。采集自 `/proc/net/dev` | | |\n| `doris_be_network_receive_packets` | | Num | 各个网卡的接收包个数累计。采集自 `/proc/net/dev` | | |\n| `doris_be_network_send_bytes` | | 字节 | 各个网卡的发送字节累计。采集自 `/proc/net/dev` | | |\n| `doris_be_network_send_packets` | | Num | 各个网卡的发送包个数累计。采集自 `/proc/net/dev` | | |\n| `doris_be_proc` | `{mode=""ctxt_switch""}` | Num | CPU 上下文切换的累计值。采集自 `/proc/stat` | 观测是否有异常的上下文切换 | P0 |\n| `doris_be_proc ` | `{mode=""interrupt""}` | Num | CPU 中断次数的累计值。采集自 `/proc/stat` | | |\n| `doris_be_proc` | `{mode=""procs_blocked""}` | Num | 系统当前被阻塞的进程数（如等待IO）。采集自 `/proc/stat` | | |\n| `doris_be_proc` | `{mode=""procs_running""}` | Num | 系统当前正在执行的进程数。采集自 `/proc/stat` | | |\n| `doris_be_snmp_tcp_in_errs` | | Num | tcp包接收错误的次数。采集自 `/proc/net/snmp` | 可观测网络错误如重传、丢包等。需和其他 snmp 指标配合使用 | P0 |\n| `doris_be_snmp_tcp_in_segs` | | Num | tcp包发送的个数。 采集自 `/proc/net/snmp` | | |']","错误信息 ""tcp listen failed, errno=98, errmsg=Address already in use"" 表示 Doris 尝试监听的 TCP 端口已经被其他进程占用。错误码 98 对应于 Linux 系统的 EADDRINUSE 错误，这通常发生在一个应用程序尝试绑定到一个已经被另一个应用程序使用的端口时。解决这个问题的方法通常包括：

1. **检查端口使用情况**：使用如 `netstat -tunlp` 或 `lsof -i:端口号` 命令来检查哪个进程正在使用该端口。
2. **更改 Doris 配置**：如果该端口确实被其他进程使用，可以考虑更改 Doris 的配置文件，使用其他未被占用的端口。
3. **停止占用端口的进程**：如果该端口被不应当使用它的进程占用，可以考虑停止那个进程，再次尝试启动 Doris。

务必确保选择的新端口不会与系统中运行的其他服务冲突，以避免未来发生相同的错误。",27.45,,ERROR,"['2. 是否 Stream load 同一个作业被重复提交了\n\n     由于 Stream load 是 HTTP 协议提交创建导入任务，一般各个语言的 HTTP Client 均会自带请求重试逻辑。Doris 系统在接受到第一个请求后，已经开始操作 Stream load，但是由于没有及时返回给 Client 端结果， Client 端会发生再次重试创建请求的情况。这时候 Doris 系统由于已经在操作第一个请求，所以第二个请求已经就会被报 Label Already Exists 的情况。\n\n     排查上述可能的方法：使用 Label 搜索 FE Master 的日志，看是否存在同一个 Label 出现了两次 `redirect load action to destination=` 的情况。如果有就说明，请求被 Client 端重复提交了。\n\n     建议用户根据当前请求的数据量，计算出大致导入的时间，并根据导入超时时间，将Client 端的请求超时间改成大于导入超时时间的值，避免请求被 Client 端多次提交。\n\n  3. Connection reset 异常\n\n     在社区版 0.14.0 及之前的版本在启用Http V2之后出现connection reset异常，因为Web 容器内置的是tomcat，Tomcat 在 307 (Temporary Redirect) 是有坑的，对这个协议实现是有问题的，所有在使用Stream load 导入大数据量的情况下会出现connect reset异常，这个是因为tomcat在做307跳转之前就开始了数据传输，这样就造成了BE收到的数据请求的时候缺少了认证信息，之后将内置容器改成了Jetty解决了这个问题，如果你遇到这个问题，请升级你的Doris或者禁用Http V2（`enable_http_server_v2=false`）。\n\n     升级以后同时升级你程序的http client 版本到 `4.5.13`，在你的pom.xml文件中引入下面的依赖', '| `doris_be_proc`                           | `{mode=""procs_blocked""}` | Num  | 系统当前被阻塞的进程数（如等待IO）。采集自 `/proc/stat`                                                           |                                          |    |\n| `doris_be_proc`                           | `{mode=""procs_running""}` | Num  | 系统当前正在执行的进程数。采集自 `/proc/stat`                                                                 |                                          |    |\n| `doris_be_snmp_tcp_in_errs`               |                          | Num  | tcp包接收错误的次数。采集自 `/proc/net/snmp`                                                              | 可观测网络错误如重传、丢包等。需和其他 snmp 指标配合使用          | P0 |\n| `doris_be_snmp_tcp_in_segs`               |                          | Num  | tcp包发送的个数。 采集自 `/proc/net/snmp`                                                               |                                          |    |', '## 导入说明\n\nApache Doris 的数据导入实现有以下共性特征，这里分别介绍，以帮助大家更好的使用数据导入功能\n\n## 导入的原子性保证\n\nDoris 的每一个导入作业，不论是使用 Broker Load 进行批量导入，还是使用 INSERT 语句进行单条导入，都是一个完整的事务操作。导入事务可以保证一批次内的数据原子生效，不会出现部分数据写入的情况。\n\n同时，一个导入作业都会有一个 Label。这个 Label 是在一个数据库（Database）下唯一的，用于唯一标识一个导入作业。Label 可以由用户指定，部分导入功能也会由系统自动生成。\n\nLabel 是用于保证对应的导入作业，仅能成功导入一次。一个被成功导入的 Label，再次使用时，会被拒绝并报错 `Label already used`。通过这个机制，可以在 Doris 侧做到 `At-Most-Once` 语义。如果结合上游系统的 `At-Least-Once` 语义，则可以实现导入数据的 `Exactly-Once` 语义。\n\n关于原子性保证的最佳实践，可以参阅 导入事务和原子性。\n\n## 同步及异步导入\n\n导入方式分为同步和异步。对于同步导入方式，返回结果即表示导入成功还是失败。而对于异步导入方式，返回成功仅代表作业提交成功，不代表数据导入成功，需要使用对应的命令查看导入作业的运行状态。\n\n## 导入array类型\n\n向量化场景才能支持array函数，非向量化场景不支持。\n\n如果想要应用array函数导入数据，则应先启用向量化功能；然后需要根据array函数的参数类型将输入参数列转换为array类型；最后，就可以继续使用array函数了。\n\n例如以下导入，需要先将列b14和列a13先cast成`array<string>`类型，再运用`array_union`函数。']","错误信息 ""tcp listen failed, errno=98, errmsg=Address already in use"" 表示在 Apache Doris 中尝试监听一个网络端口时遇到了问题。`errno=98` 是一个标准的错误码，通常对应于 `EADDRINUSE` 错误，意味着指定的端口已经被系统中的另一个进程或服务使用。

这种情况通常发生在以下几种场景：

1. **端口冲突**：尝试启动的 Doris 服务与系统上已运行的另一个应用程序（可能是另一个 Doris 实例，或者是其他任何使用相同端口的服务）发生了端口冲突。
   
2. **快速重启**：如果刚刚关闭了一个 Doris 服务实例并且立即尝试重启它，有时操作系统可能还没有完全释放该端口，导致出现这个错误。

3. **错误的配置**：在配置文件中可能指定了一个已经被其他服务占用的端口。

解决这个问题通常需要以下步骤：

1. **查找占用端口的进程**：使用操作系统的命令（如 `lsof -i :<端口>` 或 `netstat -tulpn | grep :<端口>`）来找出哪个进程正在监听冲突的端口。

2. **停止或重新配置冲突的服务**：如果找到占用的进程，根据情况可以停止该进程或者重新配置它使用其他端口。

3. **检查 Doris 配置**：确认 Doris 的配置文件中的端口设置是否正确，没有与系统其他服务发生冲突。

4. **等待端口释放**：如果刚刚关闭了服务，尝试等待一段时间再重启 Doris 服务。

通过上述步骤，应该能够解决 ""Address already in use"" 的问题。"
